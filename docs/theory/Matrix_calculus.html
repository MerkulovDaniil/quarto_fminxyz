<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Matrix calculus</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script defer="" type="text/javascript" src="../../assets/toggle_button.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<meta property="og:title" content="Matrix calculus">
<meta property="og:image" content="https://new.fmin.xyz/docs/theory/vector_im.svg">
<meta name="twitter:title" content="Matrix calculus">
<meta name="twitter:image" content="https://new.fmin.xyz/docs/theory/vector_im.svg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.svg" alt="fmin.xyz" class="navbar-logo">
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/MerkulovDaniil/optim" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/@fmin" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/fminxyz" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../docs/theory/index.html">Theory</a></li><li class="breadcrumb-item"><a href="../../docs/theory/Matrix_calculus.html">Matrix calculus</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/theory/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Matrix_calculus.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Matrix calculus</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/theory/convex sets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex sets</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Affine_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Affine set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Convex_set.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Conic_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conic set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/convex sets/Projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projection</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Convex_function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Conjugate_set.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate set</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Conjugate function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Dual norm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dual norm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Subgradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient and subdifferential</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Optimality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimality conditions. KKT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Convex_optimization_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex optimization problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Duality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Duality</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/theory/Rates_of_convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rates of convergence</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/Simplex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LP and simplex algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/Autograd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Adaptive Metrics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Newton.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Newton method</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Quasi_newton.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quasi Newton methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/adaptive_metric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Adaptive metric methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/CG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/KFAC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KFAC</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/adaptive_metrics/Natural_gradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Natural gradient descent</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Fom</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/fom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">First order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/GD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Subgradient descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Projected_subgradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projected subgradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Mirror_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mirror descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/SGD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic gradient descent</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/SAG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic average gradient</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/ADAM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ADAM: A Method for Stochastic Optimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Lookahead.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lookahead Optimizer: <span class="math inline">k</span> steps forward, <span class="math inline">1</span> step back</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/fom/Shampoo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shampoo: Preconditioned Stochastic Tensor Optimization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Line Search</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/line_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Line search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/binary_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Binary search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/golden_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Golden search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/inexact.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inexact line search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/line_search/parabola.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Successive parabolic interpolation</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">
 <span class="menu-text">Zom</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/zom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Zero order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/bee_algorithm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bee algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/nelder-mead.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nelder–Mead</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/methods/zom/simulated-annealing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulated annealing</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/exercises/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/matrix_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matrix calculus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convex_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex sets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/projection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/separation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Separation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/conjugate_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate sets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convex_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convex functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/subgradient.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subgradient and subdifferential</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/conjugate_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conjugate functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/gop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General optimization problems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/duality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Duality</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rates of convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/line_search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Line search</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/cvxpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CVXPY library</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/automatic_differentiation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/zom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Zero order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/fom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">First order methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/exercises/uncategorized.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uncategorized</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/applications/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/A-Star.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="math inline">A^*</span> algorithm for path finding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/deep_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/Eigenfaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Eigenfaces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/ellipsoid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Minimum volume ellipsoid</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/knapsack_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knapsack problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear least squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/MLE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Maximum likelihood estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/Neural_Lipschitz_constant.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural network Lipschitz constant</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/NN_Loss_Surface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Network Loss Surface Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principal component analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/rendezvous.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rendezvous problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/salesman_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Travelling salesman problem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/total_variation_inpainting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Total variation in-painting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/applications/two_way_partitioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Two way partitioning problem</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/benchmarks/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/benchmarks/CNN_on_Fashion_MNIST.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNN on FashionMNIST</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/benchmarks/linear_least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Least Squares</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../docs/materials/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/materials/tutorials/Colab tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick start to the Colab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../docs/materials/tutorials/Tutorials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tutorials</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#basic-linear-algebra-background" id="toc-basic-linear-algebra-background" class="nav-link active" data-scroll-target="#basic-linear-algebra-background"><span class="header-section-number">1</span> Basic linear algebra background</a>
  <ul class="collapse">
  <li><a href="#vectors-and-matrices" id="toc-vectors-and-matrices" class="nav-link" data-scroll-target="#vectors-and-matrices"><span class="header-section-number">1.1</span> Vectors and matrices</a></li>
  <li><a href="#matrix-and-vector-product" id="toc-matrix-and-vector-product" class="nav-link" data-scroll-target="#matrix-and-vector-product"><span class="header-section-number">1.2</span> Matrix and vector product</a></li>
  <li><a href="#norms-and-scalar-products" id="toc-norms-and-scalar-products" class="nav-link" data-scroll-target="#norms-and-scalar-products"><span class="header-section-number">1.3</span> Norms and scalar products</a></li>
  <li><a href="#eigenvalues-eigenvectors-and-the-singular-value-decomposition" id="toc-eigenvalues-eigenvectors-and-the-singular-value-decomposition" class="nav-link" data-scroll-target="#eigenvalues-eigenvectors-and-the-singular-value-decomposition"><span class="header-section-number">1.4</span> Eigenvalues, eigenvectors, and the singular-value decomposition</a>
  <ul class="collapse">
  <li><a href="#eigenvalues" id="toc-eigenvalues" class="nav-link" data-scroll-target="#eigenvalues"><span class="header-section-number">1.4.1</span> Eigenvalues</a></li>
  <li><a href="#singular-value-decomposition" id="toc-singular-value-decomposition" class="nav-link" data-scroll-target="#singular-value-decomposition"><span class="header-section-number">1.4.2</span> Singular value decomposition</a></li>
  <li><a href="#skeleton-decomposition" id="toc-skeleton-decomposition" class="nav-link" data-scroll-target="#skeleton-decomposition"><span class="header-section-number">1.4.3</span> Skeleton decomposition</a></li>
  </ul></li>
  <li><a href="#canonical-tensor-decomposition" id="toc-canonical-tensor-decomposition" class="nav-link" data-scroll-target="#canonical-tensor-decomposition"><span class="header-section-number">1.5</span> Canonical tensor decomposition</a></li>
  <li><a href="#determinant-and-trace" id="toc-determinant-and-trace" class="nav-link" data-scroll-target="#determinant-and-trace"><span class="header-section-number">1.6</span> Determinant and trace</a></li>
  </ul></li>
  <li><a href="#optimization-bingo" id="toc-optimization-bingo" class="nav-link" data-scroll-target="#optimization-bingo"><span class="header-section-number">2</span> Optimization bingo</a>
  <ul class="collapse">
  <li><a href="#gradient" id="toc-gradient" class="nav-link" data-scroll-target="#gradient"><span class="header-section-number">2.1</span> Gradient</a></li>
  <li><a href="#hessian" id="toc-hessian" class="nav-link" data-scroll-target="#hessian"><span class="header-section-number">2.2</span> Hessian</a></li>
  <li><a href="#jacobian" id="toc-jacobian" class="nav-link" data-scroll-target="#jacobian"><span class="header-section-number">2.3</span> Jacobian</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.4</span> Summary</a></li>
  <li><a href="#taylor-approximations" id="toc-taylor-approximations" class="nav-link" data-scroll-target="#taylor-approximations"><span class="header-section-number">2.5</span> Taylor approximations</a>
  <ul class="collapse">
  <li><a href="#first-order-taylor-approximation" id="toc-first-order-taylor-approximation" class="nav-link" data-scroll-target="#first-order-taylor-approximation"><span class="header-section-number">2.5.1</span> First-order Taylor approximation</a></li>
  <li><a href="#second-order-taylor-approximation" id="toc-second-order-taylor-approximation" class="nav-link" data-scroll-target="#second-order-taylor-approximation"><span class="header-section-number">2.5.2</span> Second-order Taylor approximation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#derivatives" id="toc-derivatives" class="nav-link" data-scroll-target="#derivatives"><span class="header-section-number">3</span> Derivatives</a>
  <ul class="collapse">
  <li><a href="#naive-approach" id="toc-naive-approach" class="nav-link" data-scroll-target="#naive-approach"><span class="header-section-number">3.1</span> Naive approach</a></li>
  <li><a href="#differential-approach" id="toc-differential-approach" class="nav-link" data-scroll-target="#differential-approach"><span class="header-section-number">3.2</span> Differential approach</a>
  <ul class="collapse">
  <li><a href="#differentials" id="toc-differentials" class="nav-link" data-scroll-target="#differentials"><span class="header-section-number">3.2.1</span> Differentials</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">3.2.2</span> Properties</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">4</span> References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/MerkulovDaniil/optim/edit/master/docs/theory/Matrix_calculus.md" class="toc-action">Edit this page</a></p></div></div></nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button type="button" name="button" class="btn" id="toggleSpoilers">
🔽
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
</header>

<section id="basic-linear-algebra-background" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Basic linear algebra background</h1>
<section id="vectors-and-matrices" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="vectors-and-matrices"><span class="header-section-number">1.1</span> Vectors and matrices</h2>
<p>We will treat all vectors as column vectors by default. The space of real vectors of length <span class="math inline">n</span> is denoted by <span class="math inline">\mathbb{R}^n</span>, while the space of real-valued <span class="math inline">m \times n</span> matrices is denoted by <span class="math inline">\mathbb{R}^{m \times n}</span>. That’s it: <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span id="eq-vector"><span class="math display">
x = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} \quad x^T = \begin{bmatrix}
x_1 &amp; x_2 &amp; \dots &amp; x_n
\end{bmatrix} \quad x \in \mathbb{R}^n, x_i \in \mathbb{R}
\tag{1}</span></span> Similarly, if <span class="math inline">A \in \mathbb{R}^{m \times n}</span> we denote transposition as <span class="math inline">A^T \in \mathbb{R}^{n \times m}</span>: <span class="math display">
A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn}
\end{bmatrix} \quad A^T = \begin{bmatrix}
a_{11} &amp; a_{21} &amp; \dots &amp; a_{m1} \\
a_{12} &amp; a_{22} &amp; \dots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1n} &amp; a_{2n} &amp; \dots &amp; a_{mn}
\end{bmatrix} \quad A \in \mathbb{R}^{m \times n}, a_{ij} \in \mathbb{R}
</span> We will write <span class="math inline">x \geq 0</span> and <span class="math inline">x \neq 0</span> to indicate componentwise relationships</p>
<div id="fig-vector" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="vector_im.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Equivivalent representations of a vector</figcaption>
</figure>
</div>
<p>A matrix is symmetric if <span class="math inline">A = A^T</span>. It is denoted as <span class="math inline">A \in \mathbb{S}^n</span> (set of square symmetric matrices of dimension <span class="math inline">n</span>). Note, that only a square matrix could be symmetric by definition.</p>
<p>A matrix <span class="math inline">A \in \mathbb{S}^n</span> is called <strong>positive (negative) definite</strong> if for all <span class="math inline">x \neq 0 : x^T Ax &gt; (&lt;) 0</span>. We denote this as <span class="math inline">A \succ (\prec) 0</span>. The set of such matrices is denoted as <span class="math inline">\mathbb{S}^n_{++} (\mathbb{S}^n_{- -})</span></p>
<p>A matrix <span class="math inline">A \in \mathbb{S}^n</span> is called <strong>positive (negative) semidefinite</strong> if for all <span class="math inline">x : x^T Ax \geq (\leq) 0</span>. We denote this as <span class="math inline">A \succeq (\preceq) 0</span>. The set of such matrices is denoted as <span class="math inline">\mathbb{S}^n_{+} (\mathbb{S}^n_{-})</span></p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Is it correct, that a positive definite matrix has all positive entries?</p>
</div>
</div>
</div>
</div>
</section>
<section id="matrix-and-vector-product" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="matrix-and-vector-product"><span class="header-section-number">1.2</span> Matrix and vector product</h2>
<p>Let <span class="math inline">A</span> be a matrix of size <span class="math inline">m \times n</span>, and <span class="math inline">B</span> be a matrix of size <span class="math inline">n \times p</span>, and let the product <span class="math inline">AB</span> be: <span class="math display">
C = AB
</span> then <span class="math inline">C</span> is a <span class="math inline">m \times p</span> matrix, with element <span class="math inline">(i, j)</span> given by: <span class="math display">
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.
</span></p>
<p>This operation in a naive form requires <span class="math inline">\mathcal{O}(n^3)</span> arithmetical operations, where <span class="math inline">n</span> is usually assumed as the largest dimension of matrices.</p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Is it possible to multiply two matrices faster, than <span class="math inline">\mathcal{O}(n^3)</span>? How about <span class="math inline">\mathcal{O}(n^2)</span>, <span class="math inline">\mathcal{O}(n)</span>?</p>
</div>
</div>
</div>
</div>
<p>Let <span class="math inline">A</span> be a matrix of shape <span class="math inline">m \times n</span>, and <span class="math inline">x</span> be <span class="math inline">n \times 1</span> vector, then the <span class="math inline">i</span>-th component of the product: <span class="math display">
z = Ax
</span> is given by: <span class="math display">
z_i = \sum_{k=1}^n a_{ik}x_k
</span></p>
<p>Remember, that:</p>
<ul>
<li><span class="math inline">C = AB \quad C^T = B^T A^T</span></li>
<li><span class="math inline">AB \neq BA</span></li>
<li><span class="math inline">e^{A} =\sum\limits_{k=0}^{\infty }{1 \over k!}A^{k}</span></li>
<li><span class="math inline">e^{A+B} \neq e^{A} e^{B}</span> (but if <span class="math inline">A</span> and <span class="math inline">B</span> are commuting matrices, which means that <span class="math inline">AB = BA</span>, <span class="math inline">e^{A+B} = e^{A} e^{B}</span>)</li>
<li><span class="math inline">\langle x, Ay\rangle = \langle A^T x, y\rangle</span></li>
</ul>
</section>
<section id="norms-and-scalar-products" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="norms-and-scalar-products"><span class="header-section-number">1.3</span> Norms and scalar products</h2>
<p>Norm is a <strong>qualitative measure of the smallness of a vector</strong> and is typically denoted as <span class="math inline">\Vert x \Vert</span>.</p>
<p>The norm should satisfy certain properties:</p>
<ol type="1">
<li><span class="math inline">\Vert \alpha x \Vert = \vert \alpha\vert \Vert x \Vert</span>, <span class="math inline">\alpha \in \mathbb{R}</span></li>
<li><span class="math inline">\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert</span> (triangle inequality)</li>
<li>If <span class="math inline">\Vert x \Vert = 0</span> then <span class="math inline">x = 0</span></li>
</ol>
<p>The distance between two vectors is then defined as <span class="math display">
d(x, y) = \Vert x - y \Vert.
</span> The most well-known and widely used norm is <strong>Euclidean norm</strong>: <span class="math display">
\Vert x \Vert_2 = \sqrt{\sum_{i=1}^n |x_i|^2},
</span> which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.</p>
<p>Euclidean norm, or <span class="math inline">2</span>-norm, is a subclass of an important class of <span class="math inline">p</span>-norms:</p>
<p><span class="math display">
\Vert x \Vert_p = \Big(\sum_{i=1}^n |x_i|^p\Big)^{1/p}.
</span> There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: <span class="math display">
\Vert x \Vert_{\infty} = \max_i | x_i|
</span> <span class="math inline">L_1</span> norm (or <strong>Manhattan distance</strong>) which is defined as the sum of modules of the elements of <span class="math inline">x</span>:</p>
<p><span class="math display">
\Vert x \Vert_1 = \sum_i |x_i|
</span></p>
<p><span class="math inline">L_1</span> norm plays a very important role: it all relates to the <strong>compressed sensing</strong> methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Balls_p_norm.ipynb">👨‍💻</a></p>
<div id="fig-p_balls" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./p_balls.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Balls in different norms on a plane</figcaption>
</figure>
</div>
<p>In some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm <strong>Frobenius</strong> norm: <span class="math display">
\Vert A \Vert_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
</span> Spectral norm, <span class="math inline">\Vert A \Vert_2</span> is one of the most used matrix norms (along with the Frobenius norm).</p>
<p><span class="math display">
\Vert A \Vert_2 = \sup_{x \ne 0} \frac{\Vert A x \Vert_2}{\Vert x \Vert_{2}},
</span> It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the <strong>singular value decomposition</strong> (SVD) of the matrix. It holds</p>
<p><span class="math display">
\Vert A \Vert_2 = \sigma_1(A) = \sqrt{\lambda_{\max}(A^TA)}
</span></p>
<p>where <span class="math inline">\sigma_1(A)</span> is the largest singular value of the matrix <span class="math inline">A</span>.</p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Is it true, that all matrix norms satisfy the submultiplicativity property: <span class="math inline">\Vert AB \Vert \leq \Vert A \Vert \Vert B \Vert</span>? Hint: consider Chebyshev matrix norm <span class="math inline">\Vert A \Vert_C = \max\limits_{i,j} \vert a_{ij}\vert</span>.</p>
</div>
</div>
</div>
</div>
<p>The standard <strong>scalar (inner) product</strong> between vectors <span class="math inline">x</span> and <span class="math inline">y</span> from <span class="math inline">\mathbb{R}^n</span> is given by <span class="math display">
\langle x, y \rangle = x^T y = \sum\limits_{i=1}^n x_i y_i = y^T x =  \langle y, x \rangle
</span></p>
<p>Here <span class="math inline">x_i</span> and <span class="math inline">y_i</span> are the scalar <span class="math inline">i</span>-th components of corresponding vectors.</p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Is there any connection between the norm <span class="math inline">\Vert \cdot \Vert</span> and scalar product <span class="math inline">\langle \cdot, \cdot \rangle</span>?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Prove, that you can switch the position of a matrix inside a scalar product with transposition: <span class="math inline">\langle x, Ay\rangle = \langle A^Tx, y\rangle</span> and <span class="math inline">\langle x, yB\rangle = \langle xB^T, y\rangle</span></p>
</div>
</div>
</div>
</div>
<p>The standard <strong>scalar (inner) product</strong> between matrices <span class="math inline">X</span> and <span class="math inline">Y</span> from <span class="math inline">\mathbb{R}^{m \times n}</span> is given by</p>
<p><span class="math display">
\langle X, Y \rangle = \text{tr}(X^T Y) = \sum\limits_{i=1}^m\sum\limits_{j=1}^n X_{ij} Y_{ij} =  \text{tr}(Y^T X) =  \langle Y, X \rangle
</span></p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Is there any connection between the Frobenious norm <span class="math inline">\Vert \cdot \Vert_F</span> and scalar product between matrices <span class="math inline">\langle \cdot, \cdot \rangle</span>?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Simplify the following expression: <span class="math display">
\sum\limits_{i=1}^n \langle S^{-1} a_i, a_i \rangle,
</span> where <span class="math inline">S = \sum\limits_{i=1}^n a_ia_i^T, a_i \in \mathbb{R}^n, \det(S) \neq 0</span></p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<ol type="1">
<li><p>Let <span class="math inline">A</span> be the matrix of columns vector <span class="math inline">a_i</span>, therefore matrix <span class="math inline">A^T</span> contains rows <span class="math inline">a_i^T</span></p></li>
<li><p>Note, that, <span class="math inline">S = A A^T</span> - it is the skeleton decomposition from vectors <span class="math inline">a_i</span>. Also note, that <span class="math inline">A</span> is not symmetric, while <span class="math inline">S</span>, clearly, is.</p></li>
<li><p>The target sum is <span class="math inline">\sum\limits_{i=1}^n a_i^T S^{-1} a_i</span>.</p></li>
<li><p>The most important part of this exercise lies here: we’ll present this sum as the trace of some matrix <span class="math inline">M</span> to use trace cyclic property. <span class="math display">
\sum\limits_{i=1}^n a_i^T S^{-1} a_i = \sum\limits_{i=1}^n m_{ii},
</span> where <span class="math inline">m_{ii}</span> - i-th diagonal element of some matrix <span class="math inline">M</span>.</p></li>
<li><p>Note, that <span class="math inline">M = A^T \left( S^{-1} A \right)</span> is the product of 2 matrices, because <span class="math inline">i</span>-th diagonal element of <span class="math inline">M</span> is the scalar product of <span class="math inline">i</span>-th row of the first matrix <span class="math inline">A^T</span> and <span class="math inline">i</span>-th column of the second matrix <span class="math inline">S^{-1} A</span>. <span class="math inline">i</span>-th row of matrix <span class="math inline">A^T</span>, by definition, is <span class="math inline">a_i^T</span>, while <span class="math inline">i</span>-th column of the matrix <span class="math inline">S^{-1} A</span> is clearly <span class="math inline">S^&gt;{-1} a_i</span>.</p>
<p>Indeed, <span class="math inline">m_{ii} = a_i^T S^{-1} a_i</span>, then we can finish the exercise: <span class="math display">
\begin{split}
\sum\limits_{i=1}^n a_i^T S^{-1} a_i &amp;= \sum\limits_{i=1}^n m_{ii} = \text{tr} M \\
&amp;= \text{tr} \left( A^T S^{-1} A\right) =  \text{tr} \left( AA^T S^{-1} \right) \\
&amp;=  \text{tr } \left( SS^{-1} \right) =  \text{tr} \left( I\right) = n
\end{split}
</span></p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="eigenvalues-eigenvectors-and-the-singular-value-decomposition" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="eigenvalues-eigenvectors-and-the-singular-value-decomposition"><span class="header-section-number">1.4</span> Eigenvalues, eigenvectors, and the singular-value decomposition</h2>
<section id="eigenvalues" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="eigenvalues"><span class="header-section-number">1.4.1</span> Eigenvalues</h3>
<p>A scalar value <span class="math inline">\lambda</span> is an eigenvalue of the <span class="math inline">n \times n</span> matrix <span class="math inline">A</span> if there is a nonzero vector <span class="math inline">q</span> such that <span class="math display">
Aq = \lambda q.
</span></p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Consider a 2x2 matrix: <span class="math display">
A = \begin{bmatrix}
2 &amp; 1 \\
1 &amp; 3 \\
\end{bmatrix}
</span> The eigenvalues of this matrix can be found by solving the characteristic equation: <span class="math display">
\text{det}(A - \lambda I) = 0
</span> For this matrix, the eigenvalues are <span class="math inline">\lambda_1 = 1</span> and <span class="math inline">\lambda_2 = 4</span>. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.</p>
</div>
</div>
</div>
</div>
<p>The vector <span class="math inline">q</span> is called an eigenvector of <span class="math inline">A</span>. The matrix <span class="math inline">A</span> is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.</p>
<div class="callout callout-style-default callout-theorem no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-theorem">
<p><span class="math display">
A \succeq 0 \Leftrightarrow \text{all eigenvalues of } A \text{ are } \geq 0
</span> <span class="math display">
A \succ 0 \Leftrightarrow \text{all eigenvalues of } A \text{ are } &gt; 0
</span></p>
<div class="callout callout-style-default callout-proof no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-proof" data-collapse="true">
<p>We will just prove the first point here. The second one can be proved analogously.</p>
<ol type="1">
<li><span class="math inline">\rightarrow</span> Suppose some eigenvalue <span class="math inline">\lambda</span> is negative and let <span class="math inline">x</span> denote its corresponding eigenvector. Then <span class="math display">
Ax = \lambda x \rightarrow x^T Ax = \lambda x^T x &lt; 0
</span> which contradicts the condition of <span class="math inline">A \succeq 0</span>.</li>
<li><span class="math inline">\leftarrow</span> For any symmetric matrix, we can pick a set of eigenvectors <span class="math inline">v_1, \dots, v_n</span> that form an orthogonal basis of <span class="math inline">\mathbb{R}^n</span>. Pick any <span class="math inline">x \in \mathbb{R}^n</span>. <span class="math display">
\begin{split}
x^T A x &amp;= (\alpha_1 v_1 + \ldots + \alpha_n v_n)^T A (\alpha_1 v_1 + \ldots + \alpha_n v_n)\\
&amp;= \sum \alpha_i^2 v_i^T A v_i = \sum \alpha_i^2 \lambda_i v_i^T v_i \geq 0
\end{split}
</span> here we have used the fact that <span class="math inline">v_i^T v_j = 0</span>, for <span class="math inline">i \neq j</span>.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>If a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?</p>
</div>
</div>
</div>
</div>
<p>Suppose <span class="math inline">A \in S_n</span>, i.e., <span class="math inline">A</span> is a real symmetric <span class="math inline">n \times n</span> matrix. Then <span class="math inline">A</span> can be factorized as</p>
<p><span class="math display">
A = Q\Lambda Q^T,
</span></p>
<p>where <span class="math inline">Q \in \mathbb{R}^{n \times n}</span> is orthogonal, i.e., satisfies <span class="math inline">Q^T Q = I</span>, and <span class="math inline">\Lambda = \text{diag}(\lambda_1, \ldots , \lambda_n)</span>. The (real) numbers <span class="math inline">\lambda_i</span> are the eigenvalues of <span class="math inline">A</span> and are the roots of the characteristic polynomial <span class="math inline">\text{det}(A - \lambda I)</span>. The columns of <span class="math inline">Q</span> form an orthonormal set of eigenvectors of <span class="math inline">A</span>. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of <span class="math inline">A</span>. <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>We usually order the eigenvalues as <span class="math inline">\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n</span>. We use the notation <span class="math inline">\lambda_i(A)</span> to refer to the <span class="math inline">i</span>-th largest eigenvalue of <span class="math inline">A \in S</span>. We usually write the largest or maximum eigenvalue as <span class="math inline">\lambda_1(A) = \lambda_{\text{max}}(A)</span>, and the least or minimum eigenvalue as <span class="math inline">\lambda_n(A) = \lambda_{\text{min}}(A)</span>.</p>
<p>The largest and smallest eigenvalues satisfy</p>
<p><span class="math display">
\lambda_{\text{min}} (A) = \inf_{x \neq 0} \dfrac{x^T Ax}{x^T x}, \qquad \lambda_{\text{max}} (A) = \sup_{x \neq 0} \dfrac{x^T Ax}{x^T x}
</span></p>
<p>and consequently <span class="math inline">\forall x \in \mathbb{R}^n</span> (Rayleigh quotient):</p>
<p><span class="math display">
\lambda_{\text{min}} (A) x^T x \leq x^T Ax \leq \lambda_{\text{max}} (A) x^T x
</span></p>
<p>The <strong>condition number</strong> of a nonsingular matrix is defined as</p>
<p><span class="math display">
\kappa(A) = \|A\|\|A^{-1}\|
</span></p>
<p>Suppose <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with rank <span class="math inline">A = r</span>. Then <span class="math inline">A</span> can be factored as</p>
<p><span class="math display">
A = U \Sigma V^T , \quad (A.12)
</span></p>
<p>where <span class="math inline">U \in \mathbb{R}^{m \times r}</span> satisfies <span class="math inline">U^T U = I</span>, <span class="math inline">V \in \mathbb{R}^{n \times r}</span> satisfies <span class="math inline">V^T V = I</span>, and <span class="math inline">\Sigma</span> is a diagonal matrix with <span class="math inline">\Sigma = \text{diag}(\sigma_1, ..., \sigma_r)</span>, such that</p>
<p><span class="math display">
\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r &gt; 0.
</span></p>
</section>
<section id="singular-value-decomposition" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="singular-value-decomposition"><span class="header-section-number">1.4.2</span> Singular value decomposition</h3>
<p>This factorization is called the <strong>singular value decomposition (SVD)</strong> of <span class="math inline">A</span>. The columns of <span class="math inline">U</span> are called left singular vectors of <span class="math inline">A</span>, the columns of <span class="math inline">V</span> are right singular vectors, and the numbers <span class="math inline">\sigma_i</span> are the singular values. The singular value decomposition can be written as</p>
<p><span class="math display">
A = \sum_{i=1}^{r} \sigma_i u_i v_i^T,
</span></p>
<p>where <span class="math inline">u_i \in \mathbb{R}^m</span> are the left singular vectors, and <span class="math inline">v_i \in \mathbb{R}^n</span> are the right singular vectors.</p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Consider a 2x2 matrix: <span class="math display">
B = \begin{bmatrix}
4 &amp; 0 \\
0 &amp; 2 \\
\end{bmatrix}
</span> The singular value decomposition of this matrix can be represented as: <span class="math display">
B = U \Sigma V^T.
</span> Where <span class="math inline">U</span> and <span class="math inline">V</span> are orthogonal matrices and <span class="math inline">\Sigma</span> is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Let <span class="math inline">A \in \mathbb{R}^{m \times n}</span>, and let <span class="math inline">q := \min\{m, n\}</span>. Show that <span class="math display">
\|A\|_F^2 = \sum_{i=1}^{q} \sigma_i^2(A) ,
</span> where <span class="math inline">\sigma_1(A) \geq \ldots \geq \sigma_q(A) \geq 0</span> are the singular values of matrix <span class="math inline">A</span>. Hint: use the connection between Frobenius norm and scalar product and SVD.</p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="&quot;true">
<p><br> <br> <br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Suppose, matrix <span class="math inline">A \in \mathbb{S}^n_{++}</span>. What can we say about the connection between its eigenvalues and singular values?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?</p>
</div>
</div>
</div>
</div>
</section>
<section id="skeleton-decomposition" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="skeleton-decomposition"><span class="header-section-number">1.4.3</span> Skeleton decomposition</h3>
<p>Simple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:</p>
<p><span class="math display">
A = U V^T \quad A = \hat{C}\hat{A}^{-1}\hat{R}
</span></p>
<p>The latter expression refers to the fun fact: you can randomly choose <span class="math inline">r</span> linearly independent columns of a matrix and any <span class="math inline">r</span> linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.</p>
<div id="fig-skeleton" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./skeleton.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Illustration of Skeleton decomposition</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?</p>
</div>
</div>
</div>
</div>
<p>Use cases for Skeleton decomposition are:</p>
<ul>
<li>Model reduction, data compression, and speedup of computations in numerical analysis: given rank-<span class="math inline">r</span> matrix with <span class="math inline">r \ll n, m</span> one needs to store <span class="math inline">\mathcal{O}((n + m)r) \ll nm</span> elements.</li>
<li>Feature extraction in machine learning, where it is also known as matrix factorization</li>
<li>All applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.</li>
</ul>
</section>
</section>
<section id="canonical-tensor-decomposition" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="canonical-tensor-decomposition"><span class="header-section-number">1.5</span> Canonical tensor decomposition</h2>
<p>One can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of <span class="math inline">r</span> primitive tensors.</p>
<div id="fig-cp_decomposition" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./cp.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Illustration of Canonical Polyadic decomposition</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Note, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of <em>rank</em> for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?</p>
</div>
</div>
</div>
</div>
</section>
<section id="determinant-and-trace" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="determinant-and-trace"><span class="header-section-number">1.6</span> Determinant and trace</h2>
<p>The determinant and trace can be expressed in terms of the eigenvalues</p>
<p><span class="math display">
\text{det} A = \prod\limits_{i=1}^n \lambda_i, \qquad \text{tr} A = \sum\limits_{i=1}^n \lambda_i
</span></p>
<p>The determinant has several appealing (and revealing) properties. For instance,</p>
<ul>
<li><span class="math inline">\text{det} A = 0</span> if and only if <span class="math inline">A</span> is singular;</li>
<li><span class="math inline">\text{det} AB = (\text{det} A)(\text{det} B)</span>;</li>
<li><span class="math inline">\text{det} A^{-1} = \frac{1}{\text{det} \ A}</span>.</li>
</ul>
<p>Don’t forget about the cyclic property of a trace for arbitrary matrices <span class="math inline">A, B, C, D</span> (assuming, that all dimensions are consistent):</p>
<p><span class="math display">
\text{tr} (ABCD) = \text{tr} (DABC) = \text{tr} (CDAB) = \text{tr} (BCDA)
</span></p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>For the matrix:<br>
<span class="math display">
C = \begin{bmatrix}
2 &amp; 1 \\
1 &amp; 3 \\
\end{bmatrix}
</span> The determinant is <span class="math inline">\text{det}(C) = 6 - 1 = 5</span>, and the trace is <span class="math inline">\text{tr}(C) = 2 + 3 = 5</span>. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How does the determinant of a matrix relate to its invertibility?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>What can you say about the determinant value of a positive definite matrix?</p>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="optimization-bingo" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Optimization bingo</h1>
<section id="gradient" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="gradient"><span class="header-section-number">2.1</span> Gradient</h2>
<p>Let <span class="math inline">f(x):\mathbb{R}^n→\mathbb{R}</span>, then vector, which contains all first-order partial derivatives:</p>
<p><span class="math display">
\nabla f(x) = \dfrac{df}{dx} = \begin{pmatrix}
    \frac{\partial f}{\partial x_1} \\
    \frac{\partial f}{\partial x_2} \\
    \vdots \\
    \frac{\partial f}{\partial x_n}
\end{pmatrix}
</span></p>
<p>named gradient of <span class="math inline">f(x)</span>. This vector indicates the direction of the steepest ascent. Thus, vector <span class="math inline">−\nabla f(x)</span> means the direction of the steepest descent of the function in the point. Moreover, the gradient vector is always orthogonal to the contour line in the point.</p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>For the function <span class="math inline">f(x, y) = x^2 + y^2</span>, the gradient is: <span class="math display">
\nabla f(x, y) =
\begin{bmatrix}
2x \\
2y \\
\end{bmatrix}
</span> This gradient points in the direction of the steepest ascent of the function.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How does the magnitude of the gradient relate to the steepness of the function?</p>
</div>
</div>
</div>
</div>
</section>
<section id="hessian" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="hessian"><span class="header-section-number">2.2</span> Hessian</h2>
<p>Let <span class="math inline">f(x):\mathbb{R}^n→\mathbb{R}</span>, then matrix, containing all the second order partial derivatives:</p>
<p><span class="math display">
f''(x) = \dfrac{\partial^2 f}{\partial x_i \partial x_j} = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_1\partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2 \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \dots  &amp; \frac{\partial^2 f}{\partial x_n \partial x_n}
\end{pmatrix}
</span></p>
<p>In fact, Hessian could be a tensor in such a way: <span class="math inline">\left(f(x): \mathbb{R}^n \to \mathbb{R}^m \right)</span> is just 3d tensor, every slice is just hessian of corresponding scalar function <span class="math inline">\left( H\left(f_1(x)\right), H\left(f_2(x)\right), \ldots, H\left(f_m(x)\right)\right)</span>.</p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>For the function <span class="math inline">f(x, y) = x^2 + y^2</span>, the Hessian is:</p>
<p><span class="math display">
H_f(x, y) = \begin{bmatrix} 2 &amp; 0 \\
0 &amp; 2 \\
\end{bmatrix}
</span></p>
</div>
</div>
</div>
</div>
<p>This matrix provides information about the curvature of the function in different directions.</p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How can the Hessian matrix be used to determine the concavity or convexity of a function?</p>
</div>
</div>
</div>
</div>
</section>
<section id="jacobian" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="jacobian"><span class="header-section-number">2.3</span> Jacobian</h2>
<p>The extension of the gradient of multidimensional <span class="math inline">f(x):\mathbb{R}^n\to\mathbb{R}^m</span> is the following matrix:</p>
<p><span class="math display">
J_f = f'(x) = \dfrac{df}{dx^T} = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \dots  &amp; \frac{\partial f_m}{\partial x_n} \\
    \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \dots  &amp; \frac{\partial f_m}{\partial x_n} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \dots  &amp; \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
</span></p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>For the function<br>
<span class="math display">
f(x, y) = \begin{bmatrix}
x + y \\
x - y \\
\end{bmatrix},
</span> the Jacobian is: <span class="math display">
J_f(x, y) = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{bmatrix}
</span></p>
</div>
</div>
</div>
</div>
<p>This matrix provides information about the rate of change of the function with respect to its inputs.</p>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>How does the Jacobian matrix relate to the gradient for scalar-valued functions?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Can we somehow connect those three definitions above (gradient, jacobian, and hessian) using a single correct statement?</p>
</div>
</div>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.4</span> Summary</h2>
<p><span class="math display">
f(x) : X \to Y; \qquad \frac{\partial f(x)}{\partial x} \in G
</span></p>
<table class="table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">X</th>
<th style="text-align: center;">Y</th>
<th style="text-align: center;">G</th>
<th style="text-align: center;">Name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\mathbb{R}</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}</span></td>
<td style="text-align: center;"><span class="math inline">f'(x)</span> (derivative)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\mathbb{R}^n</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R^n}</span></td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial f}{\partial x_i}</span> (gradient)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\mathbb{R}^n</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}^m</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}^{m \times n}</span></td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial f_i}{\partial x_j}</span> (jacobian)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\mathbb{R}^{m \times n}</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}</span></td>
<td style="text-align: center;"><span class="math inline">\mathbb{R}^{n \times m}</span></td>
<td style="text-align: center;"><span class="math inline">\dfrac{\partial f}{\partial x_{ij}}</span></td>
</tr>
</tbody>
</table>
</section>
<section id="taylor-approximations" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="taylor-approximations"><span class="header-section-number">2.5</span> Taylor approximations</h2>
<p>Taylor approximations provide a way to approximate functions locally by polynomials. The idea is that for a smooth function, we can approximate it by its tangent (for the first order) or by its parabola (for the second order) at a point.</p>
<section id="first-order-taylor-approximation" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="first-order-taylor-approximation"><span class="header-section-number">2.5.1</span> First-order Taylor approximation</h3>
<p>The first-order Taylor approximation, also known as the linear approximation, is centered around some point <span class="math inline">x_0</span>. If <span class="math inline">f: \mathbb{R}^n \rightarrow \mathbb{R}</span> is a differentiable function, then its first-order Taylor approximation is given by:</p>
<p><span class="math display">
f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)
</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">f(x_0)</span> is the value of the function at the point <span class="math inline">x_0</span>.</li>
<li><span class="math inline">\nabla f(x_0)</span> is the gradient of the function at the point <span class="math inline">x_0</span>.</li>
</ul>
<p>It is very usual to replace the <span class="math inline">f(x)</span> with <span class="math inline">f_{x_0}^I(x)</span> near the point <span class="math inline">x_0</span> for simple analysis of some approaches.</p>
<div id="fig-first_order_taylor" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./first_order_taylor.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: First order Taylor approximation near the point <span class="math inline">x_0</span></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>For the function <span class="math inline">f(x) = e^x</span> around the point <span class="math inline">x_0 = 0</span>, the first order Taylor approximation is: <span class="math display">
f_{x_0}^I(x) = 1 + x
</span> The second-order Taylor approximation is: <span class="math display">
f_{x_0}^{II}(x) = 1 + x + \frac{x^2}{2}
</span> These approximations provide polynomial representations of the function near the point <span class="math inline">x_0</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="second-order-taylor-approximation" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="second-order-taylor-approximation"><span class="header-section-number">2.5.2</span> Second-order Taylor approximation</h3>
<p>The second-order Taylor approximation, also known as the quadratic approximation, includes the curvature of the function. For a twice-differentiable function <span class="math inline">f: \mathbb{R}^n \rightarrow \mathbb{R}</span>, its second-order Taylor approximation centered at some point <span class="math inline">x_0</span> is:</p>
<p><span class="math display">
f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\nabla^2 f(x_0)</span> is the Hessian matrix of <span class="math inline">f</span> at the point <span class="math inline">x_0</span>.</li>
</ul>
<div id="fig-second_order_taylor" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./second_order_taylor.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Second order Taylor approximation near the point <span class="math inline">x_0</span></figcaption>
</figure>
</div>
<p>When using the linear approximation of the function is not sufficient one can consider replacing the <span class="math inline">f(x)</span> with <span class="math inline">f_{x_0}^{II}(x)</span> near the point <span class="math inline">x_0</span>. In general, Taylor approximations give us a way to locally approximate functions. The first-order approximation is a plane tangent to the function at the point <span class="math inline">x_0</span>, while the second-order approximation includes the curvature and is represented by a parabola. These approximations are especially useful in optimization and numerical methods because they provide a tractable way to work with complex functions.</p>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Calculate first and second order Taylor approximation of the function <span class="math inline">f(x) = \dfrac{1}{2}x^T A x - b^T x + c</span></p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<p><br><br> <br><br> <br><br> <br><br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-question">
<p>Why might one choose to use a Taylor approximation instead of the original function in certain applications?</p>
</div>
</div>
</div>
</div>
<p>Note, that even the second-order approximation could become inaccurate very quickly. The code for the picture below is available here: <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/taylor_inaccuracy.ipynb">👨‍💻</a></p>
<div class="responsive-video"><video autoplay="" loop="" class="video"><source src="inaccurate_taylor.mp4" type="video/mp4">Your browser does not support the video tag.</video></div>
</section>
</section>
</section>
<section id="derivatives" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Derivatives</h1>
<section id="naive-approach" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="naive-approach"><span class="header-section-number">3.1</span> Naive approach</h2>
<p>The basic idea of the naive approach is to reduce matrix/vector derivatives to the well-known scalar derivatives. <img src="./matrix_calculus.svg" id="fig-naive_derivative" class="img-fluid" alt="Naive approach of taking derivatives"> One of the most important practical tricks here is to separate indices of sum (<span class="math inline">i</span>) and partial derivatives (<span class="math inline">k</span>). Ignoring this simple rule tends to produce mistakes.</p>
</section>
<section id="differential-approach" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="differential-approach"><span class="header-section-number">3.2</span> Differential approach</h2>
<p>The guru approach implies formulating a set of simple rules, which allows you to calculate derivatives just like in a scalar case. It might be convenient to use the differential notation here. <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="callout callout-style-default callout-theorem no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-theorem">
<p>Let <span class="math inline">x \in S</span> be an interior point of the set <span class="math inline">S</span>, and let <span class="math inline">D : U \rightarrow V</span> be a linear operator. We say that the function <span class="math inline">f</span> is differentiable at the point <span class="math inline">x</span> with derivative <span class="math inline">D</span> if for all sufficiently small <span class="math inline">h \in U</span> the following decomposition holds: <span class="math display">
f(x + h) = f(x) + D[h] + o(\|h\|)
</span> If for any linear operator <span class="math inline">D : U \rightarrow V</span> the function <span class="math inline">f</span> is not differentiable at the point <span class="math inline">x</span> with derivative <span class="math inline">D</span>, then we say that <span class="math inline">f</span> is not differentiable at the point <span class="math inline">x</span>.</p>
</div>
</div>
</div>
</div>
<section id="differentials" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="differentials"><span class="header-section-number">3.2.1</span> Differentials</h3>
<p>After obtaining the differential notation of <span class="math inline">df</span> we can retrieve the gradient using the following formula:</p>
<p><span class="math display">
df(x) = \langle \nabla f(x), dx\rangle
</span></p>
<p>Then, if we have a differential of the above form and we need to calculate the second derivative of the matrix/vector function, we treat “old” <span class="math inline">dx</span> as the constant <span class="math inline">dx_1</span>, then calculate <span class="math inline">d(df) = d^2f(x)</span></p>
<p><span class="math display">
d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
</span></p>
</section>
<section id="properties" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="properties"><span class="header-section-number">3.2.2</span> Properties</h3>
<p>Let <span class="math inline">A</span> and <span class="math inline">B</span> be the constant matrices, while <span class="math inline">X</span> and <span class="math inline">Y</span> are the variables (or matrix functions).</p>
<ul>
<li><span class="math inline">dA = 0</span></li>
<li><span class="math inline">d(\alpha X) = \alpha (dX)</span></li>
<li><span class="math inline">d(AXB) = A(dX )B</span></li>
<li><span class="math inline">d(X+Y) = dX + dY</span></li>
<li><span class="math inline">d(X^T) = (dX)^T</span></li>
<li><span class="math inline">d(XY) = (dX)Y + X(dY)</span></li>
<li><span class="math inline">d\langle X, Y\rangle = \langle dX, Y\rangle+ \langle X, dY\rangle</span></li>
<li><span class="math inline">d\left( \dfrac{X}{\phi}\right) = \dfrac{\phi dX - (d\phi) X}{\phi^2}</span></li>
<li><span class="math inline">d\left( \det X \right) = \det X \langle X^{-T}, dX \rangle</span></li>
<li><span class="math inline">d\left(\text{tr } X \right) = \langle I, dX\rangle</span></li>
<li><span class="math inline">df(g(x)) = \dfrac{df}{dg} \cdot dg(x)</span></li>
<li><span class="math inline">H = (J(\nabla f))^T</span></li>
<li><span class="math inline">d(X^{-1})=-X^{-1}(dX)X^{-1}</span></li>
</ul>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Find <span class="math inline">\nabla^2 f(x)</span>, if <span class="math inline">f(x) = \dfrac12 \langle Ax, x\rangle - \langle b, x\rangle + c</span>.</p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-36-contents" aria-controls="callout-36" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-36" class="callout-36-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<p><br><br> <br><br> <br><br> <br><br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Find <span class="math inline">df, \nabla f(x)</span>, if <span class="math inline">f(x) = \ln \langle x, Ax\rangle</span>.</p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-38-contents" aria-controls="callout-38" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-38" class="callout-38-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<ol type="1">
<li>It is essential for <span class="math inline">A</span> to be positive definite, because it is a logarithm argument. So, <span class="math inline">A \in \mathbb{S}^n_{++}</span>Let’s find the differential first: <span class="math display">
\begin{split}
df &amp;= d \left( \ln \langle x, Ax\rangle \right) = \dfrac{d \left( \langle x, Ax\rangle \right)}{ \langle x, Ax\rangle} = \dfrac{\langle dx, Ax\rangle +  \langle x, d(Ax)\rangle}{ \langle x, Ax\rangle} = \\
&amp;= \dfrac{\langle Ax, dx\rangle + \langle x, Adx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle Ax, dx\rangle + \langle A^T x, dx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle (A + A^T) x, dx\rangle}{ \langle x, Ax\rangle}
\end{split}
</span></li>
<li>Note, that our main goal is to derive the form <span class="math inline">df = \langle \cdot, dx\rangle</span> <span class="math display">
df = \left\langle  \dfrac{2 A x}{ \langle x, Ax\rangle} , dx\right\rangle
</span> Hence, the gradient is <span class="math inline">\nabla f(x) = \dfrac{2 A x}{ \langle x, Ax\rangle}</span></li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Find <span class="math inline">df, \nabla f(X)</span>, if <span class="math inline">f(X) = \Vert AX - B\Vert_F</span>.</p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-40-contents" aria-controls="callout-40" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-40" class="callout-40-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<p><br><br><br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Find <span class="math inline">df, \nabla f(X)</span>, if <span class="math inline">f(X) = \langle S, X\rangle - \log \det X</span>.</p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-42-contents" aria-controls="callout-42" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-42" class="callout-42-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<p><br><br><br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-example no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<div>
<div class="callout-example">
<p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">\nabla^2f(x)</span>, if <span class="math inline">f(x) = \ln \left( 1 + \exp\langle a,x\rangle\right)</span></p>
<div class="callout callout-style-default callout-solution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-44-contents" aria-controls="callout-44" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-44" class="callout-44-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div>
<div class="callout-solution" data-collapse="true">
<p><br><br><br></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="references" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> References</h1>
<ul>
<li><a href="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</a> book by S. Boyd and L. Vandenberghe - Appendix A. Mathematical background.</li>
<li><a href="assets/files/NumericalOptimization.pdf">Numerical Optimization</a> by J. Nocedal and S. J. Wright. - Background Material.</li>
<li><a href="https://nla.skoltech.ru/lectures/files/decompositions.pdf">Matrix decompositions Cheat Sheet</a>.</li>
<li><a href="https://atmos.washington.edu/~dennis/MatrixCalculus.pdf">Good introduction</a></li>
<li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a></li>
<li><a href="http://www.machinelearning.ru/wiki/images/a/ab/MOMO18_Seminar1.pdf">MSU seminars</a> (Rus.)</li>
<li><a href="http://www.matrixcalculus.org/">Online tool</a> for analytic expression of a derivative.</li>
<li><a href="https://charlesfrye.github.io/math/2019/01/25/frechet-determinant.html">Determinant derivative</a></li>
<li><a href="https://web.stanford.edu/~boyd/vmls/">Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares</a> - book by Stephen Boyd &amp; Lieven Vandenberghe.</li>
<li><a href="https://nla.skoltech.ru">Numerical Linear Algebra</a> course at Skoltech</li>
</ul>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>A full introduction to applied linear algebra can be found in <a href="https://web.stanford.edu/~boyd/vmls/">Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares</a> - book by Stephen Boyd &amp; Lieven Vandenberghe, which is indicated in the source. Also, a useful refresher for linear algebra is in Appendix A of the book Numerical Optimization by Jorge Nocedal Stephen J. Wright.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A good cheat sheet with matrix decomposition is available at the NLA course <a href="https://nla.skoltech.ru/_files/decompositions.pdf">website</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The most comprehensive and intuitive guide about the theory of taking matrix derivatives is presented in <a href="http://www.machinelearning.ru/wiki/images/a/ab/MOMO18_Seminar1.pdf">these notes</a> by Dmitry Kropotov team.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/new\.fmin\.xyz");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>