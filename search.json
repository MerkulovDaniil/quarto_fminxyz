[
  {
    "objectID": "assets/Notebooks/Balls_p_norm.html",
    "href": "assets/Notebooks/Balls_p_norm.html",
    "title": "",
    "section": "",
    "text": "%matplotlib inline\nimport jax\nfrom jax import numpy as jnp, vmap\nimport matplotlib.pyplot as plt\n\np_values = [0.5, 0.8, 1,2,3,100500]\nM = 10000 # Number of sampling points\n\ndef compute_points(p, key):\n    a = jax.random.normal(key, (1, 2))\n    norm_value = jnp.linalg.norm(a[0, :], p)\n    return jnp.where(norm_value &lt;= 1, a[0, :], jnp.array([float('nan'), float('nan')]))\n\nfig, axes = plt.subplots(1, len(p_values), figsize=(2.5*len(p_values), 3))\naxes = axes.ravel()\n\nfor i, p in enumerate(p_values):\n    keys = jax.random.split(jax.random.PRNGKey(i), M)  # Note: Modified key generation to make samples unique per `p`\n    results = vmap(lambda k: compute_points(p, k))(keys)\n    valid_points = results[~jnp.isnan(results[:, 0])]\n    axes[i].plot(valid_points[:, 0], valid_points[:, 1], '.')\n    axes[i].axis('equal')\n    axes[i].set_title(f'p = {p}')\n\nplt.suptitle('Unit disk in the p-th norm')\nplt.tight_layout()  # Adjust layout to ensure proper visualization\n# plt.savefig(\"p_balls.svg\")\nplt.show()\n\nWARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\n%matplotlib inline\nimport jax\nfrom jax import numpy as jnp, vmap\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\np_values = jnp.logspace(-0.99, 0.99, 180)\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.set_xlim([-1.1, 1.1])\nax.set_ylim([-1.1, 1.1])\nax.grid(linestyle=\":\")\n\ndef init():\n    ax.clear()\n    ax.grid(linestyle=\":\")\n    return ax,\n\ndef update(p):\n    x = jnp.linspace(-1, 1, 401)\n    y_positive = jnp.power(1 - jnp.power(jnp.abs(x), p), 1/p)\n    y_negative = -y_positive\n\n    ax.clear()\n    ax.set_xlim([-1.1, 1.1])\n    ax.set_ylim([-1.1, 1.1])\n    ax.fill_between(x, y_positive, y_negative, color='blue', alpha=0.6)\n    ax.grid(linestyle=\":\")\n    ax.set_title(f'p-norm unit ball. p = {p:.1f}')\n    return ax,\n\nframes_sequence = jnp.concatenate([p_values, p_values[::-1]])\n\nani = FuncAnimation(fig, update, frames=frames_sequence, init_func=init,\n                    interval=1000/60, repeat=True)  # Set repeat to True\n\nax.axis('equal')\nplt.close()\n# To display the animation in the notebook\nfrom IPython import display\nhtml = display.HTML(ani.to_html5_video())\ndisplay.display(html)\n\n# Uncomment the following line if you want to save the animation as a video\nani.save(\"p_balls_animation.mp4\", writer='ffmpeg', fps=60, dpi=300)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "docs/applications/salesman_problem.html",
    "href": "docs/applications/salesman_problem.html",
    "title": "1 Problem",
    "section": "",
    "text": "Suppose, we have N points in \\mathbb{R}^d Euclidian space (for simplicity, we’ll consider and plot case with d=2). Let’s imagine, that these points are nothing else but houses in some 2d village. Salesman should find the shortest way to go through the all houses only once.\n\nThat is, very simple formulation, however, implies NP - hard problem with the factorial growth of possible combinations. The goal is to minimize the following cumulative distance:\n\nd = \\sum_{i=1}^{N-1} \\| x_{y(i+1)}  - x_{y(i)}\\|_2 \\to \\min_{y},\n\nwhere x_k is the k-th point from N and y stands for the N- dimensional vector of indicies, which describes the order of path. Actually, the problem could be formulated as an LP problem, which is easier to solve."
  },
  {
    "objectID": "docs/applications/salesman_problem.html#crossing-procedure",
    "href": "docs/applications/salesman_problem.html#crossing-procedure",
    "title": "1 Problem",
    "section": "2.1 Crossing procedure",
    "text": "2.1 Crossing procedure\nEach iteration of the algorithm starts with the crossing (breed) procedure. Formally speaking, we should formulate the mapping, that takes two creature vectors as an input and returns its offspring, which inherits parents properties, while remaining consistent. We will use ordered crossover as such procedure."
  },
  {
    "objectID": "docs/applications/salesman_problem.html#mutation",
    "href": "docs/applications/salesman_problem.html#mutation",
    "title": "1 Problem",
    "section": "2.2 Mutation",
    "text": "2.2 Mutation\nIn order to give our algorithm some ability to escape local minima, we provide it with mutation procedure. We simply swap some houses in an individual vector. To be more accurate, we define mutation rate (say, 0.05). On the one hand, the higher the rate, the less stable the population is, on the other, the smaller the rate, the more often algorithm gets stuck in the local minima. We choose \\text{mutation_rate} \\cdot n individuals and in each case swap random \\text{mutation_rate} \\cdot N digits."
  },
  {
    "objectID": "docs/applications/salesman_problem.html#selection",
    "href": "docs/applications/salesman_problem.html#selection",
    "title": "1 Problem",
    "section": "2.3 Selection",
    "text": "2.3 Selection\nAt the end of the iteration we have increased population (due to crossing results), then we just calculate total path distance to each individual and select top n of them.  \nIn general, for any c &gt; 0, where d is the number of dimensions in the Euclidean space, there is a polynomial-time algorithm that finds a tour of length at most (1 + \\frac{1}{c}) times the optimal for geometric instances of TSP in\n\n\\mathcal{O}\\left(N(\\log N)^{(\\mathcal{O}(c{\\sqrt {d}}))^{d-1}}\\right)"
  },
  {
    "objectID": "docs/applications/least_squares.html",
    "href": "docs/applications/least_squares.html",
    "title": "1 Problem",
    "section": "",
    "text": "In a least-squares, or linear regression, problem, we have measurements  X \\in \\mathbb{R}^{m \\times n}  and  y \\in \\mathbb{R}^{m}  and seek a vector  \\theta \\in \\mathbb{R}^{n}  such that  X \\theta  is close to  y . Closeness is defined as the sum of the squared differences:\n\n\\sum\\limits_{i=1}^m (x_i^\\top \\theta - y_i)^2\n\nalso known as the  l_2 -norm squared,  \\|X \\theta - y\\|^2_2 \nFor example, we might have a dataset of m users, each represented by n features. Each row x_i^\\top of X is the features for user  i , while the corresponding entry y_i of y is the measurement we want to predict from x_i^\\top, such as ad spending. The prediction is given by  x_i^\\top \\theta .\nWe find the optimal \\theta by solving the optimization problem\n\n\\|X \\theta - y\\|^2_2 \\to \\min_{\\theta \\in \\mathbb{R}^{n}}\n\nLet \\theta^* denote the optimal  \\theta . The quantity  r=X \\theta^* - y  is known as the residual. If  \\|r\\|_2 = 0 , we have a perfect fit.\nNote, that the function needn’t be linear in the argument x but only in the parameters \\theta that are to be determined in the best fit."
  },
  {
    "objectID": "docs/applications/least_squares.html#moorepenrose-inverse",
    "href": "docs/applications/least_squares.html#moorepenrose-inverse",
    "title": "1 Problem",
    "section": "2.1 Moore–Penrose inverse",
    "text": "2.1 Moore–Penrose inverse\nIf the matrix X is relatively small, we can write down and calculate exact solution:\n\n\\theta^* = (X^\\top X)^{-1} X^\\top y = X^\\dagger y,\n\nwhere X^\\dagger is called pseudo-inverse matrix. However, this approach squares the condition number of the problem, which could be an obstacle in case of ill-conditioned huge scale problem."
  },
  {
    "objectID": "docs/applications/least_squares.html#qr-decomposition",
    "href": "docs/applications/least_squares.html#qr-decomposition",
    "title": "1 Problem",
    "section": "2.2 QR decomposition",
    "text": "2.2 QR decomposition\nFor any matrix X \\in \\mathbb{R}^{m \\times n} there is exists QR decomposition:\n\nX = Q \\cdot R,\n\nwhere Q is an orthogonal matrix (its columns are orthogonal unit vectors meaning Q^\\top Q=QQ^\\top=I and R is an upper triangular matrix. It is important to notice, that since Q^{-1} = Q^\\top, we have:\n\nQR\\theta = y \\quad \\longrightarrow \\quad R \\theta = Q^\\top y\n\nNow, process of finding theta consists of two steps: 1. Find the QR decomposition of X. 1. Solve triangular system R \\theta = Q^\\top y, which is triangular and, therefore, easy to solve."
  },
  {
    "objectID": "docs/applications/least_squares.html#cholesky-decomposition",
    "href": "docs/applications/least_squares.html#cholesky-decomposition",
    "title": "1 Problem",
    "section": "2.3 Cholesky decomposition",
    "text": "2.3 Cholesky decomposition\nFor any positive definite matrix A \\in \\mathbb{R}^{n \\times n} there is exists Cholesky decomposition:\n\nX^\\top X = A = L^\\top \\cdot L,\n\nwhere L is an lower triangular matrix. We have:\n\nL^\\top L\\theta = y \\quad \\longrightarrow \\quad L^\\top z_\\theta = y\n\nNow, process of finding theta consists of two steps: 1. Find the Cholesky decomposition of X^\\top X. 1. Find the z_\\theta = L\\theta by solving triangular system L^\\top z_\\theta = y 1. Find the \\theta by solving triangular system L\\theta = z_\\theta\nNote, that in this case the error stil proportional to the squared condition number."
  },
  {
    "objectID": "docs/applications/A-Star.html",
    "href": "docs/applications/A-Star.html",
    "title": "1 Problem",
    "section": "",
    "text": "The graph is one of the most significant structures in the algorithms, because this structure can represent many real life cases, from streets to networks.\nAnd one is the most popular problem is: Find the least sum of graph edges for given start and end points\nGenerally, we need determine input and output data:\n- Input data: graph map and end or start point/node (or both for certain path) - Output data: paths (or intermediate points/nodes) with the least sum of graph edges as result"
  },
  {
    "objectID": "docs/applications/A-Star.html#breadth-first-search",
    "href": "docs/applications/A-Star.html#breadth-first-search",
    "title": "1 Problem",
    "section": "2.1 Breadth First Search",
    "text": "2.1 Breadth First Search\nThis is the simplest algorithm for graph traversing. It starts at the tree root (it may be start/end node) and explores all the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n\n\n\n\n\n\n\n\nOrigin Graph\nResult Tree\nAnimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously this algorithm has low performance: O(\\vert V \\vert + \\vert E\\vert) = O(b^d), where b is branch factor (average quantity of children nodes in tree, e.g. for binary tree b=2) and d is depth/distance from root."
  },
  {
    "objectID": "docs/applications/A-Star.html#dijkstras-algorithm",
    "href": "docs/applications/A-Star.html#dijkstras-algorithm",
    "title": "1 Problem",
    "section": "2.2 Dijkstra’s algorithm",
    "text": "2.2 Dijkstra’s algorithm\n\n\n\nDescription\nAnimation\n\n\n\n\nDijkstra’s Algorithm (also called Uniform Cost Search) lets us prioritize which paths to explore. Instead of exploring all possible paths equally (like in Breadth First Search), it favors lower cost paths.\n_________________________"
  },
  {
    "objectID": "docs/applications/A-Star.html#greedy-best-first-search",
    "href": "docs/applications/A-Star.html#greedy-best-first-search",
    "title": "1 Problem",
    "section": "2.3 Greedy Best-First-Search",
    "text": "2.3 Greedy Best-First-Search\nWith Breadth First Search and Dijkstra’s Algorithm, the frontier expands in all directions. This is a reasonable choice if you’re trying to find a path to all locations or to many locations. However, a common case is to find a path to only one location. Let’s make the frontier expand towards the goal more than it expands in other directions. First, we’ll define a heuristic function that tells us how close we are to the goal. E.g. on flat map we can use function like H(A, B) = |A.x - B.x| + |A.y - B.y| , where A and B are nodes with coordinates {x, y}. Let’s consider not only shortest edges, but also use the estimated distance to the goal for the priority queue ordering. The location closest to the goal will be explored first.\n\n\n\n\n\n\n\nResult of Heuristic function\nAnimation\n\n\n\n\nWe can see that firstly nodes, that are closer to target are considered at first. But when algorithm finds a barrier, then it tries to find the path to walk around, but this path is best from the corner, not from the start position, so the result path is not the shortest. This is a result of the heuristic function. To solve this problem let’s consider next algorithm\n_________________________"
  },
  {
    "objectID": "docs/applications/A-Star.html#a-star-algorithm",
    "href": "docs/applications/A-Star.html#a-star-algorithm",
    "title": "1 Problem",
    "section": "2.4 A-Star Algorithm",
    "text": "2.4 A-Star Algorithm\nDijkstra’s Algorithm works well to find the shortest path, but it wastes time exploring in directions that aren’t promising. Greedy Best First Search explores in promising directions but it may not find the shortest path. The A* algorithm uses both the actual distance from the start and the estimated distance to the goal.\n\n\n\n\n\n\n\nResult of Cost and Heuristic function\nAnimation\n\n\n\n\nBecause of considering both cost and result of heuristic functuion as result metric for Dijkstra’s algorithm, we can find the shortest path faster, than raw Dijkstra’s algorithm, and precisely, than Greedy Best-First-Search\n_________________________"
  },
  {
    "objectID": "docs/applications/A-Star.html#a-star-implementation",
    "href": "docs/applications/A-Star.html#a-star-implementation",
    "title": "1 Problem",
    "section": "2.5 A-Star Implementation",
    "text": "2.5 A-Star Implementation\nLet’s take a closer look at this algorithm and analyze it with code example. First of all you need to create a Priority Queue because you should consider points, which are closer to destination from start position. Priority does not equal cost. This Queue contains possible points, that are to be considered as possible shortest way to destination.\n# Only main methods\nclass PriorityQueue:\n    # Puts item in collection, sorted by priority.\n    def put(self, item, priority):\n    # Returns the most priority item.\n    def get(self):\nAlso you need a class, that describes your Graph Model with 2 methods. First finds neighbors of current node, and second returns cost between current node and next. This methods allows to implement any structure, be neither grid, hexagonal map or graph.\n# Only main methods\nclass SquareGrid:\n    # Returns neigbours of 'id' cell\n    # according to map and 'walls'.\n    def neighbors(self, id):\n    # Returns cost (or distance) between 2 cells.\n    # Applicable for neighbors only.\n    def cost(self, current, next):\nAlso you need to add your heuristic function too. Because, e.i. on a grid, a cost is always equals to 1 (if you don’t use diagonals), so it would be like a Breadth First Search, but you know a destination point, so you can use direction.\ndef heuristic(a, b):\n    (x1, y1) = a\n    (x2, y2) = b\n    return ((x1 - x2)**2 + (y1 - y2)**2)**.5\nNow we can implement our A-Star algorithm. First of all we need to init our algorithm: frontier stores points according to priority. We will store information in dictionaries: - came_from like pair &lt;TO point : FROM point&gt; - cost_so_far like pair &lt;Point : Distance from start&gt;\nFirstly add our start point to them. Than, for each point (temporary as origin we find its neighbors and for each calculate the cost as: cost from origin to neighbor. If there is no information about this node in Queue or the cost is less than before, that add this point to queue with priority = cost + heuristic. Last step allows to consider more closer point to destination at first.\ndef  a_star_search(graph, start, goal):\n    ## Create a queue and add start point\n    frontier = PriorityQueue()\n    frontier.put(start,  0)\n    # Dictionaries with init for start point\n    came_from = {}\n    cost_so_far = {}\n    came_from[start] = None\n    cost_so_far[start] = 0\n    # Not all neighbors are visited\n    while  not frontier.empty():\n        # Get next node (firstly it is start one) \n        current = frontier.get()\n        if current == goal:\n            break\n        # Find all neighbor nodes\n        for  next  in graph.neighbors(current):\n            new_cost = cost_so_far[current] + graph.cost(current,  next)\n            # Not visited or cost to it is less\n            if  next  not  in cost_so_far or new_cost &lt; cost_so_far[next]:\n                cost_so_far[next] = new_cost\n                priority = new_cost + heuristic(goal,  next)\n                frontier.put(next, priority)\n                came_from[next] = current\n    return came_from, cost_so_far"
  },
  {
    "objectID": "docs/applications/index.html",
    "href": "docs/applications/index.html",
    "title": "",
    "section": "",
    "text": "This section contains self-sufficient examples of numerical applications with Python code. Most of the examples are covered in the Boyd book. There are also some very useful link about applications with code: * CVXOPT examples * CVXPY examples * Alexandr Katrutsa demos"
  },
  {
    "objectID": "docs/applications/MLE.html",
    "href": "docs/applications/MLE.html",
    "title": "1 Problem",
    "section": "",
    "text": "We need to estimate probability density p(x) of a random variable from observed values."
  },
  {
    "objectID": "docs/applications/MLE.html#linear-measurements-with-i.i.d.-noise",
    "href": "docs/applications/MLE.html#linear-measurements-with-i.i.d.-noise",
    "title": "1 Problem",
    "section": "2.1 Linear measurements with i.i.d. noise",
    "text": "2.1 Linear measurements with i.i.d. noise\nSuppose, we are given the set of observations:\n\nx_i = \\theta^\\top a_i + \\xi_i, \\quad i = [1,m],\n\nwhere * \\theta \\in \\mathbb{R}^n - unknown vector of parameters * \\xi_i are IID noise random variables with density p(z) * x_i - measurements, x \\in \\mathbb{R}^m\nWhich implies the following optimization problem:\n\n\\max\\limits_{\\theta} \\log p(x) = \\max_\\theta \\sum\\limits_{i=1}^m \\log p (x_i - \\theta^\\top a_i) = \\max_\\theta L(\\theta)\n\nWhere the sum goes from the fact, that all observation are independent, which leads to the fact, that p(\\xi) = \\prod\\limits_{i=1}^m p(\\xi_i). The target function is called log-likelihood function L(\\theta).\n\n2.1.1 Gaussian noise\n\np(z) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{z^2}{2 \\sigma^2}}\n\n\n\\log p(z) = - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{z^2}{2 \\sigma^2}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\dfrac{1}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{(x_i - \\theta^\\top a_i)^2}{2 \\sigma^2} \\right] \\\\\n&= - \\dfrac{m}{2} \\log (2 \\pi \\sigma^2) - \\dfrac{1}{2 \\sigma^2} \\sum\\limits_{i=1}^m (x_i - \\theta^\\top a_i)^2\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of gaussian noise is a least squares solution.\n\n\n2.1.2 Laplacian noise\n\np(z) = \\dfrac{1}{2a} e^{-\\frac{|z|}{a}}\n\n\n\\log p(z) = -  \\log (2a) - -\\dfrac{|z|}{a}\n\n\n\\begin{split}\nL(\\theta) &= \\sum\\limits_{i=1}^m \\left[ - \\log (2a) - -\\dfrac{|(x_i - \\theta^\\top a_i)|}{a} \\right] \\\\\n&= - m \\log (2 a) - \\dfrac{1}{a} \\sum\\limits_{i=1}^m |x_i - \\theta^\\top a_i|\n\\end{split}\n\nWhich means, the maximum likelihood estimation in case of Laplacian noise is a l_1-norm solution.\n\n\n2.1.3 Uniform noise\n\np(z) = \\begin{cases}\n  \\frac{1}{2a}, & -a \\leq z \\leq a, \\\\\n  0, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n\n\\log p(z) =  \\begin{cases}\n  - \\log(2a), & -a \\leq z \\leq a, \\\\\n  -\\infty, &  z&lt;-a \\text{ or } z&gt;a\n  \\end{cases}\n\n$$\nL() =\n\\begin{cases}\n  - m\\log(2a), & |x_i - \\theta^\\top a_i| \\leq a, \\\\\n  -\\infty, &  \\text{ otherwise }\n  \\end{cases}\n$$\nWhich means, the maximum likelihood estimation in case of uniform noise is any vector \\theta, which satisfies \\vert x_i - \\theta^\\top a_i \\vert \\leq a."
  },
  {
    "objectID": "docs/applications/MLE.html#binary-logistic-regression",
    "href": "docs/applications/MLE.html#binary-logistic-regression",
    "title": "1 Problem",
    "section": "2.2 Binary logistic regression",
    "text": "2.2 Binary logistic regression\nSuppose, we are given a set of binary random variables y_i \\in \\{0,1\\}. Let us parametrize the distribution function as a sigmoid, using linear transformation of the input as an argument of a sigmoid.\n\n\n\nPicture from Wikipedia\n\n\n\n\\begin{split}\np(y_i = 1) &= \\dfrac{\\text{exp}(\\theta_0^\\top x_i + \\theta_1)}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)} \\\\\np(y_i = 0) &= \\dfrac{1}{1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1)}\n\\end{split}\n\nLet’s assume, that first k observations are ones: y_1, \\ldots, y_k =1, y_{k+1}, \\ldots, y_m = 0. Then, log-likelihood function will be written as follows:\n\nL(\\theta_0, \\theta_1) = \\sum\\limits_{i=1}^k (\\theta_0^\\top x_i + \\theta_1) - \\sum\\limits_{i=1}^m \\log(1 + \\text{exp}(\\theta_0^\\top x_i + \\theta_1))"
  },
  {
    "objectID": "docs/applications/NN_Loss_Surface.html",
    "href": "docs/applications/NN_Loss_Surface.html",
    "title": "1 Scalar Projection",
    "section": "",
    "text": "1 Scalar Projection\nLet’s consider the training of our neural network by solving the following optimization problem:\n\n\\mathcal{L} (\\theta) \\to \\min_{\\theta \\in \\mathbb{R}^p}\n\nWe denote the initial point as \\theta_0, representing the weights of the neural network at initialization. The weights after training are denoted as \\hat{\\theta}.\nIn the given example, we have p = 105,866, which implies that we are seeking a minimum in a 105,866-dimensional space. Exploring this space is intriguing, and the underlying concept is as follows.\nInitially, we generate a random Gaussian direction w_1 \\in \\mathbb{R}^p, which inherits the magnitude of the original neural network weights for each parameter group. Subsequently, we sample the training and testing loss surfaces at points along the direction w_1, situated close to either \\theta_0 or \\hat{\\theta}.\nMathematically, this involves evaluating:\n\n\\mathcal{L} (\\alpha) = \\mathcal{L} (\\theta_0 + \\alpha w_1), \\text{ where } \\alpha \\in [-b, b].\n\nHere, \\alpha plays the role of a coordinate along the w_1 direction, and b stands for the bounds of interpolation. Visualizing \\mathcal{L} (\\alpha) enables us to project the p-dimensional surface onto a one-dimensional axis.\nIt is important to note that the characteristics of the resulting graph heavily rely on the chosen projection direction. It’s not feasible to maintain the entirety of the informationWhen transforming a space with 100,000 dimensions into a one-dimensional line through projection. However, certain properties can still be established. For instance, if \\mathcal{L} (\\alpha) \\mid_{\\alpha=0} is decreasing, this indicates that the point lies on a slope. Additionally, if the projection is non-convex, it implies that the original surface was not convex.\n\n\n\n\n2 Two dimensional projection\nWe can explore this idea further and draw the projection of the loss surface to the plane, which is defined by 2 random vectors. Note, that with 2 random gaussian vectors in the huge dimensional space are almost certainly orthogonal.\nSo, as previously, we generate random normalized gaussian vectors w_1, w_2 \\in \\mathbb{R}^p and evaluate the loss function\n\n\\mathcal{L} (\\alpha, \\beta) = \\mathcal{L} (\\theta_0 + \\alpha w_1 + \\beta w_2), \\text{ where } \\alpha, \\beta \\in [-b, b]^2.\n\nwhich immediately leads us to the following nice pictures:\n{% include_relative nn_vis_CNN_plane_no_drop.html %}\n{% include_relative nn_vis_CNN_plane_drop.html %}\n\n\n3 Code\nOpen In Colab{: .btn }"
  },
  {
    "objectID": "docs/applications/Neural_Lipschitz_constant.html",
    "href": "docs/applications/Neural_Lipschitz_constant.html",
    "title": "1 Lipschitz constant of a convolutional layer in neural network",
    "section": "",
    "text": "1 Lipschitz constant of a convolutional layer in neural network\nIt was observed, that small perturbation in Neural Network input could lead to significant errors, i.e. misclassifications.\n\n\n\nhttps://escholarship.org/content/qt3k2780bg/qt3k2780bg_noSplash_e0803cb722032c480ec3468d84e60e2a.pdf?t=qqf3iz\n\n\nLipschitz constant bounds the magnitude of the output of a function, so it cannot change drastically with a slight change in the input\n\n\\|NN(image) - NN(image+\\varepsilon)\\| \\leq L\\|\\varepsilon\\|\n\nIn this notebook we will try to estimate Lipschitz constant of some convolutional layer of a Neural Network.\n\n\n2 Code\nOpen In Colab{: .btn }"
  },
  {
    "objectID": "docs/benchmarks/index.html",
    "href": "docs/benchmarks/index.html",
    "title": "",
    "section": "",
    "text": "Here you can find comparison of different algorithms with respect to the different hyperparameter choice."
  },
  {
    "objectID": "docs/exercises/convex_functions.html",
    "href": "docs/exercises/convex_functions.html",
    "title": "1 Convex functions",
    "section": "",
    "text": "1 Convex functions\n\nShow, that f(x) = \\|x\\| is convex on \\mathbb{R}^n.\nShow, that f(x) = c^\\top x + b is convex and concave.\nShow, that f(x) = x^\\top Ax, where A\\succeq 0 - is convex on \\mathbb{R}^n.\nShow, that f(A) = \\lambda_{max}(A) - is convex, if A \\in S^n.\nProve, that -\\log\\det X is convex on X \\in S^n_{++}.\nShow, that f(x) is convex, using first and second order criteria, if f(x) = \\sum\\limits_{i=1}^n x_i^4.\nFind the set of x \\in \\mathbb{R}^n, where the function f(x) = \\dfrac{-1}{2(1 + x^\\top x)} is convex, strictly convex, strongly convex?\n\nFind the values of a,b,c, where f(x,y,z) = x^2 + 2axy + by^2 + cz^2 is convex, strictly convex, strongly convex?\nВыпуклы ли следующие функции: f(x) = e^x - 1, \\; x \\in \\mathbb{R};\\;\\;\\; f(x_1, x_2) = x_1x_2, \\; x \\in \\mathbb{R}^2_{++};\\;\\;\\; f(x_1, x_2) = 1/(x_1x_2), \\; x \\in \\mathbb{R}^2_{++}?\nДокажите, что множество S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\prod\\limits_{i=1}^n x_i \\geq 1 \\right\\} выпукло.\nProve, that function f(X) = \\mathbf{tr}(X^{-1}), X \\in S^n_{++} is convex, while g(X) = (\\det X)^{1/n}, X \\in S^n_{++} is concave.\nKullback–Leibler divergence between p,q \\in \\mathbb{R}^n_{++} is:\n\nD(p,q) = \\sum\\limits_{i=1}^n (p_i \\log(p_i/q_i) - p_i + q_i)\n\nProve, that D(p,q) \\geq 0 \\forall p,q \\in \\mathbb{R}^n_{++} and D(p,q) = 0 \\leftrightarrow p = q\nHint: \nD(p,q) = f(p) - f(q) - \\nabla f(q)^\\top (p-q), \\;\\;\\;\\; f(p) = \\sum\\limits_{i=1}^n p_i \\log p_i\n\nLet x be a real variable with the values a_1 &lt; a_2 &lt; \\ldots &lt; a_n with probabilities \\mathbb{P}(x = a_i) = p_i. Derive the convexity or concavity of the following functions from p on the set of \\left\\{p \\mid \\sum\\limits_{i=1}^n p_i = 1, p_i \\ge 0 \\right\\}\n\n$\\mathbb{E}x$\n$\\mathbb{P}\\{x \\ge \\alpha\\}$\n$\\mathbb{P}\\{\\alpha \\le x \\le \\beta\\}$\n$\\sum\\limits_{i=1}^n p_i \\log p_i​$\n$\\mathbb{V}x = \\mathbb{E}(x - \\mathbb{E}x)^2$\n$\\mathbf{quartile}(x) = {\\operatorname{inf}}\\left\\{ \\beta \\mid \\mathbb{P}\\{x \\le \\beta\\} \\ge 0.25 \\right\\}$\n\nОпределения выпуклости и сильной выпуклости. Критерии выпуклости и сильной выпуклости первого и второго порядков\nГеометрическая интерпретация выпуклости и сильной выпуклости. (подпирание прямой и параболой)\nПриведите различные три операции, сохраняющие выпуклость функции.\nДоказать, что для a,b \\ge 0; \\;\\;\\; \\theta \\in [0,1]\n\n$- \\log \\left( \\dfrac{a+b}{2}\\right) \\le -\\dfrac{\\log a + \\log b}{2}$\n$a^\\theta b^{1-\\theta} \\le \\theta a + (1 - \\theta)b$\nHölder’s inequality: \\sum\\limits_{i=1}^n x_i y_i \\le \\left( \\sum\\limits_{i=1}^n \\vert x_i\\vert ^p\\right)^{1/p} \\left( \\sum\\limits_{i=1}^n \\vert y_i\\vert^p\\right)^{1/p}. For p &gt;1, \\;\\; \\dfrac{1}{p} + \\dfrac{1}{q} = 1.\n\nFor x, y \\in \\mathbb{R}^n\nДоказать, что матричная норма f(X) = \\|X\\|_2 = \\sup\\limits_{y \\in \\mathbb{R}^n} \\dfrac{\\|Xy\\|_2}{\\|y\\|_2} выпукла.\nДоказать, что:\n\nесли f(x) - выпукла, то \\exp f(x) также выпукла.\nесли f(x) - выпукла, то g(x)^p выпукла для p \\ge 1, f(x) \\ge 0.\nесли f(x) - вогнута, то 1/f(x) выпукла для f(x) &gt; 0.\n\nВыпукла ли функция f(X, y) = y^T X^{-1}y на множестве \\mathbf{dom} f = \\{X, y \\mid X + X^T \\succeq 0\\} ? Известно, что эта функция выпукла, если X - симметричная матрица (упражнение - доказать). Докажите выпуклость или приведите простой контрпример.\nПусть функция h(x) - выпуклая на \\mathbb{R} неубывающая функция, кроме того: h(x) = 0 при x \\le 0. Докажите, что функция h\\left(\\|x\\|_2\\right) выпукла на \\mathbb{R}^n.\nIs the function returning the arithmetic mean of vector coordinates is a convex one: a(x) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i, what about geometric mean: g(x) = \\prod\\limits_{i=1}^n \\left(x_i \\right)^{1/n}?\nShow, that the following function is convex on the set of all positive denominators\n\nf(x) = \\dfrac{1}{x_1 - \\dfrac{1}{x_2 - \\dfrac{1}{x_3 - \\dfrac{1}{\\ldots}}}}, x \\in \\mathbb{R}^n\n\nВлияют ли линейные члены квадратичной функции на ее выпуклость? Сильную выпуклость?\nПусть f(x) : \\mathbb{R}^n \\to \\mathbb{R} такова, что \\forall x,y \\to f\\left( \\dfrac{x+y}{2}\\right) \\leq \\dfrac{1}{2}(f(x)+f(y)). Является ли такая функция выпуклой?\nFind the set, on which the function f(x,y) = e^{xy} will be convex.\nStudy the following function of two variables f(x,y) = e^{xy}.\n\nIs this function convex?\nProve, that this function will be convex on the line x = y.\nFind another set in \\mathbb{R}^2, on which this function will be convex.\n\nIs f(x) = -x \\ln x - (1-x) \\ln (1-x) convex?\nProve, that adding \\lambda \\|x\\|_2^2 to any convex function g(x) ensures strong convexity of a resulting function f(x) = g(x) + \\lambda \\|x\\|_2^2. Find the constant of the strong convexity \\mu.\nProve, that function\n\nf(x) = \\log\\left( \\sum\\limits_{i=1}^n e^{x_i}\\right)\n\nis convex using any differential criterion.\nProve, that a function f is strongly convex with parameter \\mu if and only if the function \nx \\mapsto f(x)- \\frac{\\mu}{2} \\|x\\|^{2}\n is convex.\nGive an example of a function, that satisfies Polyak Lojasiewicz condition, but doesn’t have convexity property.\nProve, that if g(x) - convex function, then f(x) = g(x) + \\dfrac{\\lambda}{2}\\|x\\|^2_2 will be strongly convex function.\nFind then f(x) = x^T A x is strongly convex and find strong convexity constant.\nLet f: \\mathbb{R}^n \\to \\mathbb{R} be the following function: \nf(x) = \\sum\\limits_{i=1}^k x_{\\lfloor i \\rfloor},\n where 1 \\leq k \\leq n, while the symbol x_{\\lfloor i \\rfloor} stands for the i-th component of sorted (x_{\\lfloor 1 \\rfloor} - maximum component of x and x_{\\lfloor n \\rfloor} - minimum component of x) vector of x. Show, that f is a convex function."
  },
  {
    "objectID": "docs/exercises/automatic_differentiation.html",
    "href": "docs/exercises/automatic_differentiation.html",
    "title": "1 Automatic differentiation",
    "section": "",
    "text": "1 Automatic differentiation\n\nCalculate the gradient of a Taylor series of a \\cos (x) using autograd library: ```python import autograd.numpy as np # Thinly-wrapped version of Numpy from autograd import grad\ndef taylor_cosine(x): # Taylor approximation to cosine function # Your np code here return ans ```\nIn the following code for the gradient descent for linear regression change the manual gradient computation to the PyTorch/jax autograd way. Compare those two approaches in time.\nIn order to do this, set the tolerance rate for the function value \\varepsilon = 10^{-9}. Compare the total time required to achieve the specified value of the function for analytical and automatic differentiation. Perform measurements for different values of n from np.logspace(1,4).\nFor each n value carry out at least 3 runs.\nimport numpy as np \n\n# Compute every step manually\n\n# Linear regression\n# f = w * x \n\n# here : f = 2 * x\nX = np.array([1, 2, 3, 4], dtype=np.float32)\nY = np.array([2, 4, 6, 8], dtype=np.float32)\n\nw = 0.0\n\n# model output\ndef forward(x):\n    return w * x\n\n# loss = MSE\ndef loss(y, y_pred):\n    return ((y_pred - y)**2).mean()\n\n# J = MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N * 2x(w*x - y)\ndef gradient(x, y, y_pred):\n    return np.dot(2*x, y_pred - y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\n# Training\nlearning_rate = 0.01\nn_iters = 20\n\nfor epoch in range(n_iters):\n    # predict = forward pass\n    y_pred = forward(X)\n\n    # loss\n    l = loss(Y, y_pred)\n\n    # calculate gradients\n    dw = gradient(X, Y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 2 == 0:\n        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n\nprint(f'Prediction after training: f(5) = {forward(5):.3f}')\nCalculate the 4th derivative of hyperbolic tangent function using Jax autograd.\nCompare analytic and autograd (with any framework) approach for the hessian of:\n\nf(x) = \\dfrac{1}{2}x^TAx + b^Tx + c\n\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = tr(AXB)\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\dfrac{1}{2} \\|Ax - b\\|^2_2\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nYou will work with the following function for this exercise,\n\nf(x,y)=e^{−\\left(sin(x)−cos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your own way to visualise it.\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nSuppose, we have the following function f(x) = \\frac{1}{2}\\|x\\|^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0:\n\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n\nYour goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose, \\alpha_0 = \\mathbb{1}^{10}.\n\n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n\n\\frac{\\partial L}{\\partial \\alpha} should be computed at each step using automatic differentiation. Choose any \\beta and the number of steps your need. Describe obtained results.\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x\n\n\n\n\n2 Materials\n\nHIPS autograd\nPyTorch autograd\nJax Autodiff cookbook\nStep-by-step jax excersises."
  },
  {
    "objectID": "docs/exercises/conjugate_functions.html",
    "href": "docs/exercises/conjugate_functions.html",
    "title": "1 Conjugate functions",
    "section": "",
    "text": "1 Conjugate functions\n\nFind f^*(y), if f(x) = ax + b\nFind f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = e^x\nFind f^*(y), if f(x) = x \\log x, x \\neq 0, \\;\\;\\; f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+\nFind f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) =\\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n\nFind f^*(y), if f(x) = -\\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}\nFind f^*(y), if f(x) = -0,5 - \\log x, \\;\\; x&gt;0\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nFind f^*(y), if f(x) = - (a^2 - x^2)^{1/2}, \\;\\;\\; \\vert x\\vert \\le a, \\;\\;\\; a&gt;0\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nFind f^*(y), if f(x) = \\|x\\|\nFind f^*(y), if f(x) = \\dfrac{1}{2}\\|x\\|^2\nName any 3 non-trivial facts about conjugate function.\nFind conjugate function to the f(x) = \\dfrac{1}{x}, \\;\\; x \\in \\mathbb{R}_{++}\nFind conjugate function to the f(x) = x^p, \\;\\; x \\in \\mathbb{R}_{++}, \\;\\; p&gt;1\nProve, that if f(x_1, x_2) = g_1(x_1) + g_2(x_2), then f^*(y_1, y_2) = g_1^*(y_1) + g_2^*(y_2)\nProve, that if f(x) = g(x-b), then f^*(y) = b^\\top y + g^*(y)\nProve, that if f(x) = \\alpha g(x) and  \\alpha &gt; 0 , then f^*(y) = \\alpha g^*(y/\\alpha)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y)"
  },
  {
    "objectID": "docs/exercises/index.html",
    "href": "docs/exercises/index.html",
    "title": "",
    "section": "",
    "text": "Files and links should be added."
  },
  {
    "objectID": "docs/exercises/convergence.html",
    "href": "docs/exercises/convergence.html",
    "title": "",
    "section": "",
    "text": "Show with the definition that the sequence \\left\\{ \\dfrac{1}{k} \\right\\}_{k=1}^\\infty does not have a linear convergence rate (but it converges to zero).\nShow with the definition that the sequence \\left\\{ \\dfrac{1}{k^k} \\right\\}_{k=1}^\\infty does not have a quadratic convergence rate (but it converges to zero).\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^k.\nDetermine the convergence or divergence of a given sequence r_{k} = 0.707^{2^k}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k^2}.\nDetermine the convergence or divergence of a given sequence r_{k} = \\frac{1}{k!}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^2}, & \\text{if } k\\text{ is odd} \\end{cases}.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{1}{k^{2k}}, & \\text{if } k\\text{ is odd} \\end{cases}.\nShow that the sequence x_k = 1 + (0.5)^{2^k} is quadratically converged to 1.\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\left(\\frac{1}{4}\\right)^{2^k}, & \\text{if } k\\text{ is even} \\\\ \\frac{x_{k-1}}{k}, & \\text{if } k\\text{ is odd} \\end{cases}.\nLet \\left\\{ r_k \\right\\}_{k=m}^\\infty be a sequence of non-negative numbers and let s &gt; 0 be some integer. Prove that sequence \\left\\{ r_k \\right\\}_{k=m+s}^\\infty is linearly convergent with constant q if and only if the sequence \\left\\{ r_k \\right\\}_{k=m}^\\infty converged linearly with constant q.\nDetermine the convergence type of a given sequence r_k =\\begin{cases} \\frac{1}{2^k}, & \\text{if } k\\text{ is odd} \\\\ \\frac{1}{3^{2k}}, & \\text{if } k\\text{ is even} \\end{cases}."
  },
  {
    "objectID": "docs/exercises/projection.html",
    "href": "docs/exercises/projection.html",
    "title": "1 Projection",
    "section": "",
    "text": "1 Projection\n\nLet us have two different points a, b \\in \\mathbb{R}^n. Prove that the set of points which in the Euclidean norm are closer to the point a than to b make up a half-space. Is this true for another norm?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S\nIllustrate the geometric inequality that connects \\pi_S(y), y \\notin S, x \\in S, from which it follows that \\pi_S(y) is a projection of the y point onto a convex set of S.\nFor which sets does the projection of the point outside this set exist? Unique?\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid c^T x \\ge b \\}\nFind \\pi_S (y) = \\pi if S = \\{x \\in \\mathbb{R}^n \\mid x = x_0 + X \\alpha, X \\in \\mathbb{R}^{n \\times m}, \\alpha \\in \\mathbb{R}^{m}\\}, y \\in S\nLet S \\subseteq \\mathbb{R}^n be a closed set, and x \\in \\mathbb{R}^n be a point not lying in it. Show that the projection in l_2 norm will be unique, while in l_\\infty norm this statement is not valid.\nFind the projection of the matrix X on a set of matrices of rank k, \\;\\;\\; X \\in \\mathbb{R}^{m \\times n}, k \\leq n \\leq m. In Frobenius norm and spectral norm.\nFind a projection of the X matrix on a set of symmetrical positive semi-definite matrices of X \\in \\mathbb{R}^{n \\times n}. In Frobenius norm and the scalar product associated with it.\nFind the projection \\pi_S(y) of point y onto the set  S = \\{x_1, x_2 \\in \\mathbb{R}^2 \\mid \\mid \\vert x_1\\vert + \\vert x_2\\vert = 1 \\}  in \\| \\cdot \\|_1 norm. Consider the different positions of y.\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\le x_i \\le \\beta_i, i = 1, \\ldots, n \\}.\nProve that projection is a nonexpansive operator, i.e. prove, that if S \\in \\mathbb{R}^{n} is nonempty, closed and convex set, then for any (x_{1}, x_{2}) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}\n\n\\lVert \\pi_{S}(x_{2}) - \\pi_{S}(x_{1}) \\rVert_{2} \\leq \\lVert x_{2} - x_{1} \\rVert_{2}"
  },
  {
    "objectID": "docs/exercises/uncategorized.html",
    "href": "docs/exercises/uncategorized.html",
    "title": "1 Uncategorized",
    "section": "",
    "text": "1 Uncategorized\n\nShow, that these conditions are equivalent:\n\n  \\|\\nabla f(x) - \\nabla f(z) \\| \\le L \\|x-z\\|\n\nand\n\nf(z) \\le f(x) + \\nabla f(x)^T(z-x) + \\frac L 2 \\|z-x\\|^2\n\nWe say that the function belongs to the class f  \\in C^{k,p}_L (Q) if it is k times continuously differentiable on Q, and the p derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} for \\mathbb{R}^n. Notice that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher is the order of the derivative, the stronger is the limitation (fewer functions belong to the class).\n\nProve that the function belongs to the class C_L^{2,1}. \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x)\\| \\leq L\n\nProve that the last condition can be rewritten in the form without loss of generality:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nShow that for gradient descent with the following stepsize selection strategies:\n\nconstant step h_k = \\dfrac{1}{L}\nDropping sequence h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0.\n\nyou can get the estimation of the function decrease at the iteration of the view:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - some constant, L - Lipschitz constant of the function gradient."
  },
  {
    "objectID": "docs/exercises/convex_sets.html",
    "href": "docs/exercises/convex_sets.html",
    "title": "1 Convex sets",
    "section": "",
    "text": "Show that the set is convex if and only if its intersection with any line is convex.\nShow that the convex hull of the S set is the intersection of all convex sets containing S.\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n \\alpha &lt; \\mathbb{E} f(x) &lt; \\beta, where \\mathbb{E}f(x) stands for expected value of f(x): \\mathbb{R} \\rightarrow \\mathbb{R} , i.e.  \\mathbb{E}f(x) = \\sum\\limits_{i=1}^n p_i f(a_i) \n$$ x^2 $$\n$$ x $$\n\nProve that if the set is convex, its interior is also convex. Is the opposite true?\nProve that if the set is convex, its closure is also convex. Is the opposite true?\nProve that the set of square symmetric positive definite matrices is convex.\nShow that the set of S is convex if and only if\n\n\\forall \\lambda_1, \\lambda_2 \\geq 0, \\quad (\\lambda_1, \\lambda_2) \\neq (0, 0):  \\lambda_1 S + \\lambda_2 S = (\\lambda_1 + \\lambda_2)S\n\nCalculate the Minkowski sum of the line segment and the square on the plane, the line segment and the triangle, the line segment and the circle, the line segment and the disk.\nFind the minimum value of k, at which the set of \\{x \\in \\mathbb{R}^2 \\mid (x_1^2 + 1) x_2\\le 2, x_2 \\ge k\\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nGive an example of two closed convex sets, the sum of which is not closed\nFind convex and conical hulls of the following sets:  \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2 \\},  \\{x \\in \\mathbb{R}^2 \\mid x_1^2 = x_2, x_1 \\ge 0 \\},  \\{x \\in \\mathbb{R}^2 \\mid x_1 x_2 = 1 \\} \nShow that the set of directions of the strict local descending of the differentiable function in a point is a convex cone.\nProve that K:\n\nK = \\{ x \\in \\mathbb{R}^3 \\mid x_1^2 - 2x_1x_3 + x_2^2 \\leq 0, x_3 \\geq 0 \\}\n\nis a convex cone.\nFind the convex hulls of the following sets:\n\n$x^2 + y^2 \\leq 1, xy = 0$\n$x^2 + y^2 = 1, x - y = 0$\n$x^2 + y^2 = 1, \\|x\\| \\leq 1, \\|y\\|$\n$y \\leq e^x, y \\geq \\|x\\|$\n\nFor an arbitrary set of S, let’s say \\tilde{S} consists of all segments of [a,b] with the ends of a,b \\in S. Is it true that \\tilde{S} = \\text{conv}(S) ?\nIs the given set a convex polyhedron (could be written in the form of Ax \\preceq b, Cx = d):\n\n$S = \\{ y_1a_1 + y_2a_2 \\mid -1 \\leq y_1, y_2 \\leq 1 \\}; a_1, a_2 \\in \\mathbb{R}^n$\n$S = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, \\mathbf{1}^\\top x = 1, \\sum\\limits_{i=1}^n x_ia_i = b_1, \\sum\\limits_{i=1}^n x_ia_i^2 = b_2 \\}; a_1, \\ldots a_n, b_1, b_2 \\in \\mathbb{R}$\n$S = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\|y\\|_2 = 1 \\}$\n$S = \\{x \\in \\mathbb{R}^n \\mid x \\succeq 0, x^\\top y \\leq 1, \\sum\\limits_{i=1}^n \\|y_i\\| = 1 \\}$\n\nLet S \\subseteq \\mathbb{R}^n is a set of solutions to the quadratic inequality:\n\nS = \\{x \\in \\mathbb{R}^n \\mid x^\\top A x + b^\\top x + c \\leq 0 \\}; A \\in \\mathbb{S}^n, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}\n\n\nShow that if A \\succeq 0, S is convex. Is the opposite true?\nShow that the intersection of S with the hyperplane defined by the g^\\top x + h = 0, g \\neq 0 is convex if A + \\lambda gg^\\top \\succeq 0 for some real \\lambda \\in \\mathbb{R}. Is the opposite true?\n\nShow that the hyperbolic set of  \\{x \\in \\mathbb{R}^n_+ | \\prod\\limits_{i=1}^n x_i \\geq 1 \\}  is convex. Hint: For 0 \\leq \\theta \\leq 1 it is valid, that a^\\theta b^{1 - \\theta} \\leq \\theta a + (1-\\theta)b with non-negative a,b.\nWhich of the sets are convex:\n\nStripe,  \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle,  \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen,  \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point,  \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\|_2 \\leq \\|x-y\\|_2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another,  \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points,  \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\} , where  S \\subseteq \\mathbb{R}^{n}  is convex and  X \\subseteq \\mathbb{R}^{n}  is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is  \\{x \\in \\mathbb{R}^n \\mid \\|x - a\\|_2 \\leq \\theta\\|xb\\|_2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}\n\nFind the conic hull of the set of rank k matrix products \\{XX^\\top \\mid X \\in \\mathbb{R}^{n \\times k}, \\mathbf{rank} X = k \\}?\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of  \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nLet S be such that \\forall x,y \\in S \\to \\frac{1}{2}(x+y) \\in S. Is this set convex?\nFind the conic hull of the following sets in \\mathbb{R}^2:\n\n$y = x^2$\n$y = x^2, x \\geq 0$\n$y = x^2 + x, x \\geq 0$\n$xy=1, x &gt; 0$\n$y = \\sin x, 0 \\leq x \\leq \\pi$\n$y = e^x$\n\nLet S_1 = \\{x^2 + y^2 \\leq 1 \\} is a disk of \\mathbb{R^3} and S_2 is a segment of \\left[(0,0,-1), (0,0,1)\\right]. How their convex combination with \\alpha, \\beta looks like.\nIs the next set convex?\n\n\\{a \\in \\mathbb{R}^k \\mid p(0) = 1, \\|p(t)\\| \\leq 1 \\;\\; \\forall \\alpha \\leq t \\leq \\beta, \\;\\; p(t) = a_1 + a_2t + \\ldots + a_kt^{k-1} \\}\n\nProve that in order for K \\subseteq \\mathbb{R}^n to be a convex cone, it is enough that K contains all possible non-negative combinations of its points.\nProve that in order for S \\subseteq \\mathbb{R}^n to be an affine set it is necessary and sufficient that S contains all possible affine combinations of its points.\nПусть S_1, \\ldots, S_k - произвольные непустые множества в \\mathbb{R}^n. Докажите, что:\n\n$ ( {i=1}^k S_i) = {i=1}^k ( S_i) $\n$ ( {i=1}^k S_i) = {i=1}^k ( S_i) $\n\nProve, that the set S \\subseteq \\mathbb{R}^n is convex if and only if (\\alpha + \\beta)S = \\alpha S + \\beta S for all non-negative \\alpha and \\beta\\quad (\\alpha, \\beta) \\neq (0, 0)\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. P = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}. Determine if the following sets of p are convex:\n\n$\\mathbb{P}(x &gt; \\alpha) \\le \\beta$\n$\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert$\n$$ x^{2}$$\n$\\mathbb{V}x \\ge \\alpha$\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\| \\mathbf{x} - \\mathbf{x}_c \\| \\leq r \\}) - is convex.\nProve, that if S is convex, then S+S = 2S. Give an counterexample in case, when S - is not convex.\nWhich of the following operations does not preserve convexity if X,Y \\subseteq \\mathbb{R}^n are convex sets?\n\n$X \\cup Y$\n$X \\times Y = \\left\\{ (x,y) \\; \\mid \\; x \\in X, y \\in Y \\right\\}$\n$\\alpha X + \\beta Y = \\{ \\alpha x + \\beta y \\; \\mid \\; x \\in X, \\; y \\in Y, \\; \\alpha, \\beta \\in \\mathbb{R} \\}$\n$\\alpha X = \\{ \\alpha x \\; \\mid \\; x \\in X, \\; \\alpha \\in \\mathbb{R_{-}} \\}$\n$X^{c} = \\{x \\in \\mathbb{R}^n \\; \\mid \\; x \\notin X\\}$\n\nShow, that $ \\{xx^: x ^n, x= 1\\} = \\{A ^n_+: (A) = 1\\}$.\n\n\n\n\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\\\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\\\{v_1, \\ldots, v_k\\\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\mu \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.\n\n\n\n\n\n\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{\\star}, S^{\\star\\star}, S^{\\star\\star\\star}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_\\star} are inter-conjugate, i.e. (B_p)^\\star = B_{p_\\star}, (B_{p_\\star})^\\star = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_\\star are conjugated, i.e. p^{-1} + p^{-1}\\_\\star = 1. You can assume, that p_\\star = \\infty if p = 1 and vice versa."
  },
  {
    "objectID": "docs/exercises/convex_sets.html#convex-functions",
    "href": "docs/exercises/convex_sets.html#convex-functions",
    "title": "1 Convex sets",
    "section": "",
    "text": "Consider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ✅ or ❎. Explain your answers\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\\\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\\\{v_1, \\ldots, v_k\\\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen’s inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\mu \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex."
  },
  {
    "objectID": "docs/exercises/convex_sets.html#conjugate-sets",
    "href": "docs/exercises/convex_sets.html#conjugate-sets",
    "title": "1 Convex sets",
    "section": "",
    "text": "Let \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{\\star}, S^{\\star\\star}, S^{\\star\\star\\star}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_\\star} are inter-conjugate, i.e. (B_p)^\\star = B_{p_\\star}, (B_{p_\\star})^\\star = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_\\star are conjugated, i.e. p^{-1} + p^{-1}\\_\\star = 1. You can assume, that p_\\star = \\infty if p = 1 and vice versa."
  },
  {
    "objectID": "docs/exercises/cvxpy.html",
    "href": "docs/exercises/cvxpy.html",
    "title": "1 CVXPY library",
    "section": "",
    "text": "1 CVXPY library\n\nConstrained linear least squares Solve the following problem with cvxpy library.\n\n\\begin{split} &\\|X \\theta - y\\|^2_2 \\to \\min\\limits_{\\theta \\in \\mathbb{R}^{n} } \\\\ \\text{s.t. } & 0_n \\leq \\theta \\leq 1_n \\end{split}\n\nLinear programming A linear program is an optimization problem with a linear objective and affine inequality constraints. A common standard form is the following:\n  \n     \\begin{array}{ll}\n     \\text{minimize}   & c^Tx \\\\\n     \\text{subject to} & Ax \\leq b.\n     \\end{array}\n\nHere A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, and c \\in \\mathbb{R}^n are problem data and x \\in \\mathbb{R}^{n} is the optimization variable. The inequality constraint Ax \\leq b is elementwise. Solve this problem with cvxpy library.\nList the installed solvers in cvxpy using cp.installed_solvers() method.\nSolve the following optimization problem using CVXPY:\n\n\\begin{array}{ll}\n\\text{minimize} & |x| - 2\\sqrt{y}\\\\\n\\text{subject to} & 2 \\geq e^x \\\\\n& x + y = 5,\n\\end{array}\n\nwhere x,y \\in \\mathbb{R} are variables. Find the optimal values of x and y.\nRisk budget allocation Suppose an amount x_i&gt;0 is invested in n assets, labeled i=1,..., n, with asset return covariance matrix \\Sigma \\in \\mathcal{S}_{++}^n. We define the risk of the investments as the standard deviation of the total return\n\nR(x) = (x^T\\Sigma x)^{1/2}.\n\nWe define the (relative) risk contribution of asset i (in the portfolio x) as\n\n\\rho_i = \\frac{\\partial \\log R(x)}{\\partial \\log x_i} =\n\\frac{\\partial R(x)}{R(x)} \\frac{x_i}{\\partial x_i}, \\quad i=1, \\ldots, n.\n\nThus \\rho_i gives the fractional increase in risk per fractional increase in investment i. We can express the risk contributions as\n\n\\rho_i = \\frac{x_i (\\Sigma x)_i} {x^T\\Sigma x}, \\quad i=1, \\ldots, n,\n\nfrom which we see that \\sum_{i=1}^n \\rho_i = 1. For general x, we can have \\rho_i &lt;0, which means that a small increase in investment i decreases the risk. Desirable investment choices have \\rho_i&gt;0, in which case we can interpret \\rho_i as the fraction of the total risk contributed by the investment in asset i. Note that the risk contributions are homogeneous, i.e., scaling x by a positive constant does not affect \\rho_i.\n\nProblem statement: In the risk budget allocation problem, we are given \\Sigma and a set of desired risk contributions \\rho_i^\\mathrm{des}&gt;0 with \\bf{1}^T \\rho^\\mathrm{des}=1; the goal is to find an investment mix x\\succ 0, \\bf{1}^Tx =1, with these risk contributions. When \\rho^\\mathrm{des} = (1/n)\\bf{1}, the problem is to find an investment mix that achieves so-called risk parity.\na) Explain how to solve the risk budget allocation problem using convex optimization. Hint. Minimize (1/2)x^T\\Sigma x - \\sum_{i=1}^n \\rho_i^\\mathrm{des} \\log x_i.\nb) Find the investment mix that achieves risk parity for the return covariance matrix \\Sigma below. python      import numpy as np      import cvxpy as cp      Sigma = np.array(np.matrix(\"\"\"6.1  2.9  -0.8  0.1;                           2.9  4.3  -0.3  0.9;                          -0.8 -0.3   1.2 -0.7;                           0.1  0.9  -0.7  2.3\"\"\"))      rho = np.ones(4)/4\n\n\n\n\n2 Materials\n\nCVXPY exercises\nAdditional Exercises for Convex Optimization"
  },
  {
    "objectID": "docs/theory/Conjugate function.html",
    "href": "docs/theory/Conjugate function.html",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "Let f: \\mathbb{R}^n \\to \\mathbb{R}. The function f^*: \\mathbb{R}^n \\to \\mathbb{R} is called convex conjugate (Fenchel’s conjugate, dual, Legendre transform) f(x) and is defined as follows:\n\nf^*(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right).\n\nLet’s notice, that the domain of the function f^* is the set of those y, where the supremum is finite. \n\n\n\nf^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)\n  \n\n\n\n\nThe scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right) = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where $ _{x ; g} g(x,y)$ is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n{: .example} &gt;Find f^*(y), if f(x) = ax + b. &gt;\n\n\nSolution\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x \nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n{: .question} Find f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n{: .example} &gt;Find f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}. \nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = e^x. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}. \nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}. \nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}. \nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x. \nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y). \nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y). \nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i. \nHence, f^*(y) = 0.\n\n\n\n{: .example} &gt;Revenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by &gt; &gt;\n&gt;M(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n&gt; &gt; &gt;The function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as &gt;\n&gt;M(p) = (−S)^*(−p).\n&gt; &gt;Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed)."
  },
  {
    "objectID": "docs/theory/Conjugate function.html#properties",
    "href": "docs/theory/Conjugate function.html#properties",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "f^*(y) - is always a closed convex function (a point-wise supremum of closed convex functions) on y. (Function f:X\\rightarrow R is called closed if \\mathbf{epi}(f) is a closed set in X\\times R.)\nFenchel–Young inequality:\n\n  f(x) + f^*(y) \\ge \\langle y,x \\rangle\n  \nLet the functions f(x), f^\\star(y), f^{\\star\\star}(x) be defined on the \\mathbb{R}^n. Then f^{\\star\\star}(x) = f(x) if and only if f(x) - is a proper convex function (Fenchel - Moreau theorem). (proper convex function = closed convex function)\nConsequence from Fenchel–Young inequality: f(x) \\ge f^{\\star\\star}(x).\n\n\n\nIn case of differentiable function, f(x) - convex and differentiable, \\mathbf{dom}\\; f = \\mathbb{R}^n. Then x^\\star = \\underset{x}{\\operatorname{argmin}} \\langle x,y\\rangle - f(x). Therefore y = \\nabla f(x^\\star). That’s why:\n\n  f^\\star(y) = \\langle \\nabla f(x^\\star), x^\\star \\rangle - f(x^\\star)\n  \n\n  f^\\star(y) = \\langle \\nabla f(z), z \\rangle - f(z), \\;\\;\\;\\;\\;\\; y = \\nabla f(z), \\;\\; z \\in \\mathbb{R}^n\n  \nLet f(x,y) = f_1(x) + f_2(y), where f_1, f_2 - convex functions, then\n\n  f^*(p,q) = f_1^*(p) + f_2^*(q)\n  \nLet f(x) \\le g(x)\\;\\; \\forall x \\in X. Let also f^\\star(y), g^\\star(y) be defined on Y. Then \\forall x \\in X, \\forall y \\in Y\n\n  f^\\star(y) \\ge g^\\star(y) \\;\\;\\;\\;\\;\\; f^{\\star\\star}(x) \\le g^{\\star\\star}(x)"
  },
  {
    "objectID": "docs/theory/Conjugate function.html#examples",
    "href": "docs/theory/Conjugate function.html#examples",
    "title": "1 Conjugate (dual) function",
    "section": "",
    "text": "The scheme of recovering the convex conjugate is pretty algorithmic: 1. Write down the definition f^\\star(y) = \\sup\\limits_{x \\in \\mathbf{dom} \\; f} \\left( \\langle y,x\\rangle - f(x)\\right) = \\sup\\limits_{x \\in \\mathbf{dom} \\; g} g(x,y). 1. Find those y, where $ _{x ; g} g(x,y)$ is finite. That’s the domain of the dual function f^\\star(y). 1. Find x^\\star, which maximize g(x,y) as a function on x. f^\\star(y) = g(x^\\star, y).\n{: .example} &gt;Find f^*(y), if f(x) = ax + b. &gt;\n\n\nSolution\n\n\n\nBy definition: \nf^*(y) = \\sup\\limits_{x \\in \\mathbb{R}} [ yx - f(x) ]=\\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\quad \\mathbf{dom} \\; f^* = \\{y \\in \\mathbb{R} : \\sup\\limits_{x \\in \\mathbb{R}} g(x,y) \\text{ is finite}\\}\n\nConsider the function whose supremum is the conjugate: \ng(x,y) =  yx - f(x) = yx - ax - b = x(y - a) - b.\n\nLet’s determine the domain of the function (i.e. those y for which \\sup is finite). This is a single point, y = a. Otherwise one may choose such x \nThus, we have: \\mathbf{dom} \\; f^* = \\{a\\}; f^*(a) = -b\n\n\n\n{: .question} Find f^*(y), if f(x) = \\dfrac{1}{x}, \\;\\; x\\in \\mathbb{R}_{++}.\n{: .example} &gt;Find f^*(y), if f(x) = -\\log x, \\;\\; x\\in \\mathbb{R}_{++}. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx + \\log x.\n\nThis function is unbounded above when y \\ge 0. Therefore, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y &lt; 0\\}. \nThis function is concave and its maximum is achieved at the point with zero gradient: \n\\dfrac{\\partial}{\\partial x} (yx + \\log x) = \\dfrac{1}{x} + y = 0.\n Thus, we have x = -\\dfrac1y and the conjugate function is: \nf^*(y) = -\\log(-y) - 1.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = e^x. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = yx - e^x.\n\nThis function is unbounded above when y &lt; 0. Thus, the domain of f^* is \\mathbf{dom} \\; f^* = \\{y \\ge 0\\}. \nThe maximum of this function is achieved when x = \\log y. Hence: \nf^*(y) = y \\log y - y,\n assuming 0 \\log 0 = 0.\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = x \\log x, x \\neq 0, and f(0) = 0, \\;\\;\\; x \\in \\mathbb{R}_+. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = xy - x \\log x.\n\nThis function is upper bounded for all y. Therefore, \\mathbf{dom} \\; f^* = \\mathbb{R}. \nThe maximum of this function is achieved when x = e^{y-1}. Hence: \nf^*(y) = e^{y-1}.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) =\\frac{1}{2} x^T A x, \\;\\;\\; A \\in \\mathbb{S}^n_{++}. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\frac{1}{2} x^T A x.\n\nThis function is upper bounded for all y. Thus, \\mathbf{dom} \\; f^* = \\mathbb{R}. \nThe maximum of this function is achieved when x = A^{-1}y. Hence: \nf^*(y) =  \\frac{1}{2} y^T A^{-1} y.\n\n\n\n\n{: .example} &gt;Find f^*(y), if f(x) = \\max\\limits_{i} x_i, \\;\\;\\; x \\in \\mathbb{R}^n. &gt;\n\n\nSolution\n\n\n\nConsider the function whose supremum defines the conjugate: \ng(x,y) = \\langle y,x\\rangle - f(x) = y^T x - \\max\\limits_{i} x_i.\n\nObserve that if vector y has at least one negative component, this function is not bounded by x. \nIf y \\succeq 0 and 1^T y &gt; 1, then y \\notin \\mathbf{dom} \\; f^*(y). \nIf y \\succeq 0 and 1^T y &lt; 1, then y \\notin \\mathbf{dom} \\; f^*(y). \nOnly left with y \\succeq 0 and 1^T y = 1. In this case, x^T y \\le \\max\\limits_i x_i. \nHence, f^*(y) = 0.\n\n\n\n{: .example} &gt;Revenue and profit functions. We consider a business or enterprise that consumes n resources and produces a product that can be sold. We let r = (r_1, \\ldots , r_n) denote the vector of resource quantities consumed, and S(r) denote the sales revenue derived from the product produced (as a function of the resources consumed). Now let p_i denote the price (per unit) of resource i, so the total amount paid for resources by the enterprise is p^\\top r. The profit derived by the firm is then S(r) − p^\\top r. Let us fix the prices of the resources, and ask what is the maximum profit that can be made, by wisely choosing the quantities of resources consumed. This maximum profit is given by &gt; &gt;\n&gt;M(p) = \\sup\\limits_{r}\\left( S(r) - p^\\top r \\right)\n&gt; &gt; &gt;The function M(p) gives the maximum profit attainable, as a function of the resource prices. In terms of conjugate functions, we can express M as &gt;\n&gt;M(p) = (−S)^*(−p).\n&gt; &gt;Thus the maximum profit (as a function of resource prices) is closely related to the conjugate of gross sales (as a function of resources consumed)."
  },
  {
    "objectID": "docs/theory/Convex_function.html",
    "href": "docs/theory/Convex_function.html",
    "title": "1 Convex function",
    "section": "",
    "text": "The function f(x), which is defined on the convex set S \\subseteq \\mathbb{R}^n, is called convex on S, if:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\nfor any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1.\nIf the above inequality holds as strict inequality x_1 \\neq x_2 and 0 &lt; \\lambda &lt; 1, then the function is called strictly convex on S.\n\n\n\nFigure 1: Difference between convex and non-convex function\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\n\nf(x) = x^p, \\; p &gt; 1,\\; x \\in \\mathbb{R}_+\nf(x) = \\|x\\|^p,\\; p &gt; 1, x \\in \\mathbb{R}^n\nf(x) = e^{cx},\\; c \\in \\mathbb{R}, x \\in \\mathbb{R}\nf(x) = -\\ln x,\\; x \\in \\mathbb{R}_{++}\nf(x) = x\\ln x,\\; x \\in \\mathbb{R}_{++}\nThe sum of the largest k coordinates f(x) = x_{(1)} + \\ldots + x_{(k)},\\; x \\in \\mathbb{R}^n\nf(X) = \\lambda_{max}(X),\\; X = X^T\nf(X) = - \\log \\det X, \\; X \\in S^n_{++}"
  },
  {
    "objectID": "docs/theory/Convex_function.html#first-order-differential-criterion-of-convexity",
    "href": "docs/theory/Convex_function.html#first-order-differential-criterion-of-convexity",
    "title": "1 Convex function",
    "section": "4.1 First-order differential criterion of convexity",
    "text": "4.1 First-order differential criterion of convexity\nThe differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x)\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x\n\n\n\n\nFigure 4: Convex function is greater or equal than Taylor linear approximation at any point"
  },
  {
    "objectID": "docs/theory/Convex_function.html#second-order-differential-criterion-of-convexity",
    "href": "docs/theory/Convex_function.html#second-order-differential-criterion-of-convexity",
    "title": "1 Convex function",
    "section": "4.2 Second-order differential criterion of convexity",
    "text": "4.2 Second-order differential criterion of convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq 0\n\nIn other words, \\forall y \\in \\mathbb{R}^n:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq 0"
  },
  {
    "objectID": "docs/theory/Convex_function.html#connection-with-epigraph",
    "href": "docs/theory/Convex_function.html#connection-with-epigraph",
    "title": "1 Convex function",
    "section": "4.3 Connection with epigraph",
    "text": "4.3 Connection with epigraph\nThe function is convex if and only if its epigraph is a convex set.\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet a norm \\Vert \\cdot \\Vert be defined in the space U. Consider the set:\n\nK := \\{(x,t) \\in U \\times \\mathbb{R}^+ : \\Vert x \\Vert \\leq t \\}\n\nwhich represents the epigraph of the function x \\mapsto \\Vert x \\Vert. This set is called the cone norm. According to the statement above, the set K is convex.\nIn the case where U = \\mathbb{R}^n and \\Vert x \\Vert = \\Vert x \\Vert_2 (Euclidean norm), the abstract set K transitions into the set:\n\n\\{(x,t) \\in \\mathbb{R}^n \\times \\mathbb{R}^+ : \\Vert x \\Vert_2 \\leq t \\}"
  },
  {
    "objectID": "docs/theory/Convex_function.html#connection-with-sublevel-set",
    "href": "docs/theory/Convex_function.html#connection-with-sublevel-set",
    "title": "1 Convex function",
    "section": "4.4 Connection with sublevel set",
    "text": "4.4 Connection with sublevel set\nIf f(x) - is a convex function defined on the convex set S \\subseteq \\mathbb{R}^n, then for any \\beta sublevel set \\mathcal{L}_\\beta is convex.\nThe function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is closed if and only if for any \\beta sublevel set \\mathcal{L}_\\beta is closed."
  },
  {
    "objectID": "docs/theory/Convex_function.html#reduction-to-a-line",
    "href": "docs/theory/Convex_function.html#reduction-to-a-line",
    "title": "1 Convex function",
    "section": "4.5 Reduction to a line",
    "text": "4.5 Reduction to a line\nf: S \\to \\mathbb{R} is convex if and only if S is a convex set and the function g(t) = f(x + tv) defined on \\left\\{ t \\mid x + tv \\in S \\right\\} is convex for any x \\in S, v \\in \\mathbb{R}^n, which allows checking convexity of the scalar function to establish convexity of the vector function."
  },
  {
    "objectID": "docs/theory/Convex_function.html#first-order-differential-criterion-of-strong-convexity",
    "href": "docs/theory/Convex_function.html#first-order-differential-criterion-of-strong-convexity",
    "title": "1 Convex function",
    "section": "6.1 First-order differential criterion of strong convexity",
    "text": "6.1 First-order differential criterion of strong convexity\nDifferentiable f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is \\mu-strongly convex if and only if \\forall x,y \\in S:\n\nf(y) \\ge f(x) + \\nabla f^T(x)(y-x) + \\dfrac{\\mu}{2}\\|y-x\\|^2\n\nLet y = x + \\Delta x, then the criterion will become more tractable:\n\nf(x + \\Delta x) \\ge f(x) + \\nabla f^T(x)\\Delta x + \\dfrac{\\mu}{2}\\|\\Delta x\\|^2"
  },
  {
    "objectID": "docs/theory/Convex_function.html#second-order-differential-criterion-of-strong-convexity",
    "href": "docs/theory/Convex_function.html#second-order-differential-criterion-of-strong-convexity",
    "title": "1 Convex function",
    "section": "6.2 Second-order differential criterion of strong convexity",
    "text": "6.2 Second-order differential criterion of strong convexity\nTwice differentiable function f(x) defined on the convex set S \\subseteq \\mathbb{R}^n is called \\mu-strongly convex if and only if \\forall x \\in \\mathbf{int}(S) \\neq \\emptyset:\n\n\\nabla^2 f(x) \\succeq \\mu I\n\nIn other words:\n\n\\langle y, \\nabla^2f(x)y\\rangle \\geq \\mu \\|y\\|^2"
  },
  {
    "objectID": "docs/theory/Convex_optimization_problem.html",
    "href": "docs/theory/Convex_optimization_problem.html",
    "title": "1 Convex optimization problem",
    "section": "",
    "text": "1 Convex optimization problem\nNote, that there is an agreement in notation of mathematical programming. The problems of the following type are called Convex optimization problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & g_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& Ax = b,\n\\end{split}\n\\tag{COP}\n\nwhere all the functions f(x), g_1(x), \\ldots, g_m(x) are convex and all the equality constraints are affine. It sounds a bit strange, but not all convex problems are convex optimization problems.\n\n\\tag{CP}\nf(x) \\to \\min\\limits_{x \\in S},\n\nwhere f(x) is a convex function, defined on the convex set S. The necessity of affine equality constraint is essential (see Slater’s condition in {% include link.html title = ‘Duality’ %}).\nFor example, this problem is not a convex optimization problem (but implies minimizing the convex function over the convex set):\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& (x_1 + x_2)^2 = 0,\n\\end{split}\n\\tag{CP}\n\nwhile the following equivalent problem is a convex optimization problem\n\n\\begin{split}\n& x_1^2 + x_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & \\dfrac{x_1}{1 + x_2^2} \\leq 0\\\\\n& x_1 + x_2 = 0,\n\\end{split}\n\\tag{COP}\n\nSuch confusion in notation is sometimes being avoided by naming problems of type \\text{(CP)} as abstract form convex optimization problem.\n\n\n2 Materials\n\nConvex Optimization — Boyd & Vandenberghe @ Stanford"
  },
  {
    "objectID": "docs/theory/Optimality.html",
    "href": "docs/theory/Optimality.html",
    "title": "1 Background",
    "section": "",
    "text": "Let S \\subset \\mathbb{R}^n be a compact set and f(x) a continuous function on S. So that, the point of the global minimum of the function f (x) on S exists.\n\n\n\n\nConsider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\"
  },
  {
    "objectID": "docs/theory/Optimality.html#extreme-value-weierstrass-theorem",
    "href": "docs/theory/Optimality.html#extreme-value-weierstrass-theorem",
    "title": "1 Background",
    "section": "",
    "text": "Let S \\subset \\mathbb{R}^n be a compact set and f(x) a continuous function on S. So that, the point of the global minimum of the function f (x) on S exists."
  },
  {
    "objectID": "docs/theory/Optimality.html#lagrange-multipliers",
    "href": "docs/theory/Optimality.html#lagrange-multipliers",
    "title": "1 Background",
    "section": "",
    "text": "Consider simple yet practical case of equality constraints:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, i = 1, \\ldots, p\n\\end{split}\n\nThe basic idea of Lagrange method implies the switch from conditional to unconditional optimization through increasing the dimensionality of the problem:\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p \\nu_i h_i(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p} \\\\"
  },
  {
    "objectID": "docs/theory/Optimality.html#optimization-on-the-general-set-s.",
    "href": "docs/theory/Optimality.html#optimization-on-the-general-set-s.",
    "title": "1 Background",
    "section": "2.1 Optimization on the general set S.",
    "text": "2.1 Optimization on the general set S.\nDirection d \\in \\mathbb{R}^n is a feasible direction at x^* \\in S \\subseteq \\mathbb{R}^n if small steps along d do not take us outside of S.\nConsider a set S \\subseteq \\mathbb{R}^n and a function f : \\mathbb{R}^n \\to \\mathbb{R}. Suppose that x^* \\in S is a point of local minimum for f over S, and further assume that f is continuously differentiable around x^*.\n\nThen for every feasible direction d \\in \\mathbb{R}^n at x^* it holds that \\nabla f(x^*)^\\top d \\geq 0.\nIf, additionally, S is convex then\n\n\\nabla f(x^*)^\\top(x − x^*) \\geq 0, \\forall x \\in S."
  },
  {
    "objectID": "docs/theory/Optimality.html#unconstrained-optimization",
    "href": "docs/theory/Optimality.html#unconstrained-optimization",
    "title": "1 Background",
    "section": "2.2 Unconstrained optimization",
    "text": "2.2 Unconstrained optimization\n\n2.2.1 General case\nLet f(x): \\mathbb{R}^n \\to \\mathbb{R} be a twice differentiable function.\n\n\\tag{UP}\nf(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\n\nIf x^* - is a local minimum of f(x), then:\n\n\\tag{UP:Nec.}\n\\nabla f(x^*) = 0\n\nIf f(x) at some point x^* satisfies the following conditions:\n\n\\tag{UP:Suff.}\nH_f(x^*) = \\nabla^2 f(x^*) \\succ (\\prec) 0,\n\nthen (if necessary condition is also satisfied) x^* is a local minimum(maximum) of f(x).\nNote, that if \\nabla f(x^*) = 0, \\nabla^2 f(x^*) = 0, i.e. the hessian is positive semidefinite, we cannot be sure if x^* is a local minimum (see Peano surface f(x,y) = (2x^2 - y)(y - x^2)).\n\n\n2.2.2 Convex case\nIt should be mentioned, that in the convex case (i.e., f(x) is convex) necessary condition becomes sufficient. Moreover, we can generalize this result on the class of non-differentiable convex functions.\nLet f(x): \\mathbb{R}^n \\to \\mathbb{R}, then the point x^* is the solution of \\text{(UP)} if and only if:\n\n0_n \\in \\partial f(x^*)\n\nOne more important result for convex unconstrained case sounds as follows. If f(x): S \\to \\mathbb{R} - convex function defined on the convex set S, then: * Any local minima is the global one. * The set of the local minimizers S^* is convex. * If f(x) - strictly or strongly (different cases 😀) convex function, then S^* contains only one single point S^* = x^*."
  },
  {
    "objectID": "docs/theory/Optimality.html#optimization-with-equality-conditions",
    "href": "docs/theory/Optimality.html#optimization-with-equality-conditions",
    "title": "1 Background",
    "section": "2.3 Optimization with equality conditions",
    "text": "2.3 Optimization with equality conditions\n\n2.3.1 Intuition\nThings are pretty simple and intuitive in unconstrained problem. In this section we will add one equality constraint, i.e.\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h(x) = 0\n\\end{split}\n\nWe will try to illustrate approach to solve this problem through the simple example with f(x) = x_1 + x_2 and h(x) = x_1^2 + x_2^2 - 2.\n\n\n\n\n\n\n\n\n\nGenerally: in order to move from  x_F  along the budget set towards decreasing the function, we need to guarantee two conditions:\n\n\\langle \\delta x, \\nabla h(x_F) \\rangle = 0\n\n\n\\langle \\delta x, - \\nabla f(x_F) \\rangle &gt; 0\n\nLet’s assume, that in the process of such a movement we have come to the point where \n-\\nabla f(x) = \\nu \\nabla h(x)\n\n\n\\langle  \\delta x, - \\nabla f(x)\\rangle = \\langle  \\delta x, \\nu\\nabla h(x)\\rangle = 0  \n\nThen we came to the point of the budget set, moving from which it will not be possible to reduce our function. This is the local minimum in the constrained problem :) \nSo let’s define a Lagrange function (just for our convenience):\n\nL(x, \\nu) = f(x) + \\nu h(x)\n\nThen the point  x^*  is the local minimum of the problem described above, if and only if:\n\n\\begin{split}\n& \\text{Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\text{ that's written above}\\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\text{ budget constraint}\\\\\n& \\text{Sufficient conditions} \\\\\n& \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle &gt; 0,\\\\\n& \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h(x^*)^\\top y = 0\n\\end{split}\n\nWe should notice that L(x^*, \\nu^*) = f(x^*).\n\n\n2.3.2 General formulation\n\n\\tag{ECP}\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nSolution\n\nL(x, \\nu) = f(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f(x) + \\nu^\\top h(x)\n\nLet  f(x)  and  h_i(x)  be twice differentiable at the point  x^*  and continuously differentiable in some neighborhood  x^* . The local minimum conditions for  x \\in \\mathbb{R}^n, \\nu \\in \\mathbb{R}^p  are written as\n\n\\begin{split}\n& \\text{ECP: Necessary conditions} \\\\\n& \\nabla_x L(x^*, \\nu^*) = 0 \\\\\n& \\nabla_\\nu L(x^*, \\nu^*) = 0 \\\\\n& \\text{ECP: Sufficient conditions} \\\\\n& \\langle y , \\nabla^2_{xx} L(x^*, \\nu^*) y \\rangle \\geq 0,\\\\\n& \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0\n\\end{split}\n\nDepending on the behavior of the Hessian, the critical points can have a different character."
  },
  {
    "objectID": "docs/theory/Optimality.html#optimization-with-inequality-conditions",
    "href": "docs/theory/Optimality.html#optimization-with-inequality-conditions",
    "title": "1 Background",
    "section": "2.4 Optimization with inequality conditions",
    "text": "2.4 Optimization with inequality conditions\n\n2.4.1 Example\n\nf(x) = x_1^2 + x_2^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\n\nThus, if the constraints of the type of inequalities are inactive in the constrained problem, then don’t worry and write out the solution to the unconstrained problem. However, this is not the whole story 🤔. Consider the second childish example\n\nf(x) = (x_1 - 1)^2 + (x_2 + 1)^2 \\;\\;\\;\\; g(x) = x_1^2 + x_2^2 - 1\n\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\n\n\n\n\n\n\n\nSo, we have a problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0\n\\end{split}\n\nTwo possible cases:\n\n\n\n\n\n\n\ng(x) \\leq 0 is inactive. g(x^*) &lt; 0\ng(x) \\leq 0 is active. g(x^*) = 0\n\n\n\n\ng(x^*) &lt; 0   \\nabla f(x^*) = 0  \\nabla^2 f(x^*) &gt; 0\nNecessary conditions  g(x^*) = 0  - \\nabla f(x^*) = \\lambda \\nabla g(x^*), \\lambda &gt; 0  Sufficient conditions  \\langle y, \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle &gt; 0,  \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla g(x^*)^\\top y = 0\n\n\n\nCombining two possible cases, we can write down the general conditions for the problem:\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} \\\\\n\\text{s.t. } & g(x) \\leq 0 \\\\\n&\n\\end{split}\n\nLet’s define the Lagrange function:\n\nL (x, \\lambda) = f(x) + \\lambda g(x)\n\nThe classical Karush-Kuhn-Tucker first and second order optimality conditions for a local minimizer x^*, stated under the linear independence constraint qualification (LICQ), can be written as follows:\nIf x^* is a local minimum of the problem described above, then there exists a unique Lagrange multiplier \\lambda^* such that:\n\n\\begin{split}\n    & (1) \\; \\nabla_x L (x^*, \\lambda^*) = 0 \\\\\n    & (2) \\; \\lambda^* \\geq 0 \\\\\n    & (3) \\; \\lambda^* g(x^*) = 0 \\\\\n    & (4) \\; g(x^*) \\leq 0\\\\\n    & (5) \\; \\forall y \\in C(x^*):  \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*) y \\rangle \\geq 0 \\\\\n    &  \\text{where } C(x^*) = \\{y \\ \\in \\mathbb{R}^n |  \\nabla f(x^*) ^\\top y \\leq 0 \\text{ and } \\forall i \\in I(x^*):  \\nabla g_i(x^*)^⊤ y \\leq 0\n    \\} \\text{ is the critical cone.} \\\\\n    & I(x^*) = \\{i| g_i(x^*) = 0\\} \\\\\n\\end{split}\n\nIt’s noticeable, that L(x^*, \\lambda^*) = f(x^*). Conditions \\lambda^* = 0 , (1), (4) are the first scenario realization, and conditions \\lambda^* &gt; 0 , (1), (3) - the second one.\n\n\n2.4.2 General formulation\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nThis formulation is a general problem of mathematical programming.\nThe solution involves constructing a Lagrange function:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)"
  },
  {
    "objectID": "docs/theory/Optimality.html#necessary-conditions",
    "href": "docs/theory/Optimality.html#necessary-conditions",
    "title": "1 Background",
    "section": "3.1 Necessary conditions",
    "text": "3.1 Necessary conditions\nLet x^*, (\\lambda^*, \\nu^*) be a solution to a mathematical programming problem with zero duality gap (the optimal value for the primal problem p^* is equal to the optimal value for the dual problem d^*). Let also the functions  f_0, f_i, h_i  be differentiable.\n\n$\\nabla_x L(x^*, \\lambda^*, \\nu^*) = 0$\n$\\nabla_\\nu L(x^*, \\lambda^*, \\nu^*) = 0$\n$$ ^*_i , i = 1,,m$$\n$\\lambda^*_i f_i(x^*) = 0, i = 1,\\ldots,m$\n$f_i(x^*) \\leq 0, i = 1,\\ldots,m$"
  },
  {
    "objectID": "docs/theory/Optimality.html#some-regularity-conditions",
    "href": "docs/theory/Optimality.html#some-regularity-conditions",
    "title": "1 Background",
    "section": "3.2 Some regularity conditions",
    "text": "3.2 Some regularity conditions\nThese conditions are needed in order to make KKT solutions the necessary conditions. Some of them even turn necessary conditions into sufficient (for example, Slater’s). Moreover, if you have regularity, you can write down necessary second order conditions \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle \\geq 0 with semi-definite hessian of Lagrangian.\n\nSlater’s condition. If for a convex problem (i.e., assuming minimization,  f_0,f_{i} are convex and h_{i} are affine), there exists a point x such that h(x)=0 and f_{i}(x)&lt;0 (existance of a strictly feasible point), then we have a zero duality gap and KKT conditions become necessary and sufficient.\nLinearity constraint qualification If f_{i} and h_{i} are affine functions, then no other condition is needed.\nFor other examples, see wiki."
  },
  {
    "objectID": "docs/theory/Optimality.html#sufficient-conditions",
    "href": "docs/theory/Optimality.html#sufficient-conditions",
    "title": "1 Background",
    "section": "3.3 Sufficient conditions",
    "text": "3.3 Sufficient conditions\nFor smooth, non-linear optimization problems, a second order sufficient condition is given as follows. The solution  x^{*},\\lambda ^{*},\\nu ^{*}, which satisfies the KKT conditions (above) is a constrained local minimum if for the Lagrangian,\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x)\n\nthe following conditions hold:\n\n\\begin{split}\n& \\langle y , \\nabla^2_{xx} L(x^*, \\lambda^*, \\nu^*) y \\rangle &gt; 0 \\\\\n& \\forall y \\neq 0 \\in \\mathbb{R}^n : \\nabla h_i(x^*)^\\top y = 0, \\nabla f_0(x^*) ^\\top y \\leq 0,\\nabla f_j(x^*)^\\top y \\leq 0 \\\\\n& i = 1,\\ldots, p \\quad \\forall j: f_j(x^*) = 0\n\\end{split}"
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html",
    "href": "docs/theory/Rates_of_convergence.html",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "In order to compare perfomance of algorithms we need to define a terminology for different types of convergence. Let \\{x_k\\} be a sequence in \\mathbb{R}^n that converges to some point x^*\n\n\nWe can define the linear convergence in a two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and  0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note, that sometimes this type of convergence is also called exponential or geometric.\n\n\n\nThe convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or  0 &lt; C_k &lt; \\infty, C_k \\to 0. Note, that superlinear convergence is also linear convergence (one can even say, that it is linear convergence with q=0).\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and  0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression.\n\n\n\n\n\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and  0 &lt; C &lt; \\infty.\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#linear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#linear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "We can define the linear convergence in a two different forms:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^k \\quad\\text{or} \\quad \\| x_{k+1} - x^* \\|_2 \\leq q\\| x_k - x^* \\|_2,\n\nfor all sufficiently large k. Here q \\in (0, 1) and  0 &lt; C &lt; \\infty. This means that the distance to the solution x^* decreases at each iteration by at least a constant factor bounded away from 1. Note, that sometimes this type of convergence is also called exponential or geometric."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#superlinear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#superlinear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "The convergence is said to be superlinear if:\n\n\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^2} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C_k\\| x_k - x^* \\|_2,\n\nwhere q \\in (0, 1) or  0 &lt; C_k &lt; \\infty, C_k \\to 0. Note, that superlinear convergence is also linear convergence (one can even say, that it is linear convergence with q=0)."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#sublinear-convergence",
    "href": "docs/theory/Rates_of_convergence.html#sublinear-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\nwhere q &lt; 0 and  0 &lt; C &lt; \\infty. Note, that sublinear convergence means, that the sequence is converging slower, than any geometric progression."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#quadratic-convergence",
    "href": "docs/theory/Rates_of_convergence.html#quadratic-convergence",
    "title": "1 Speed of convergence",
    "section": "",
    "text": "\\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k} \\qquad \\text{or} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2,\n\nwhere q \\in (0, 1) and  0 &lt; C &lt; \\infty.\n\nQuasi-Newton methods for unconstrained optimization typically converge superlinearly, whereas Newton’s method converges quadratically under appropriate assumptions. In contrast, steepest descent algorithms converge only at a linear rate, and when the problem is ill-conditioned the convergence constant q is close to 1."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#root-test",
    "href": "docs/theory/Rates_of_convergence.html#root-test",
    "title": "1 Speed of convergence",
    "section": "2.1 Root test",
    "text": "2.1 Root test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of non-negative numbers, converging to zero, and let\n\nq = \\lim_{k \\to \\infty} \\sup_k \\; r_k ^{1/k}\n\n\nIf 0 \\leq q \\lt 1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q = 1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case q \\gt 1 is impossible."
  },
  {
    "objectID": "docs/theory/Rates_of_convergence.html#ratio-test",
    "href": "docs/theory/Rates_of_convergence.html#ratio-test",
    "title": "1 Speed of convergence",
    "section": "2.2 Ratio test",
    "text": "2.2 Ratio test\nLet \\{r_k\\}_{k=m}^\\infty be a sequence of strictly positive numbers converging to zero. Let\n\nq = \\lim_{k \\to \\infty} \\dfrac{r_{k+1}}{r_k}\n\n\nIf there exists q and 0 \\leq q \\lt  1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with constant q.\nIn particular, if q = 0, then \\{r_k\\}_{k=m}^\\infty has superlinear convergence.\nIf q does not exist, but q = \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k} \\lt  1, then \\{r_k\\}_{k=m}^\\infty has linear convergence with a constant not exceeding q.\nIf  \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} =1, then \\{r_k\\}_{k=m}^\\infty has sublinear convergence.\nThe case  \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\gt 1 is impossible.\nIn all other cases (i.e., when  \\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} \\lt  1 \\leq  \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k}) we cannot claim anything concrete about the convergence rate \\{r_k\\}_{k=m}^\\infty."
  },
  {
    "objectID": "docs/theory/Conjugate_set.html",
    "href": "docs/theory/Conjugate_set.html",
    "title": "1 Conjugate (dual) set",
    "section": "",
    "text": "Пусть S \\subseteq \\mathbb{R}^n - произвольное непустое множество. Тогда cопряженное к нему множество определяется, как:\n\nS^* = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S\\}\n\n\n\n\nМножество S^{**} называется вторым сопряженным к множеству S, если:\n\nS^{**} = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}\n\n\n\n\n\nМножества S_1 и S_2 называются взаимосопряженными, если S_1^* = S_2, S_2^* = S_1.\nМножество S называется самосопряженным, если S^{*} = S.\n\n\n\n\n\nСопряженное множество всегда замкнуто, выпукло и содержит нуль.\nДля произвольного множества S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup  \\{0\\}) }\n  \nЕсли S_1 \\subseteq S_2, то S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*\nЕсли S - замкнуто, выпукло, включает 0, то S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*\n\n\n\n\n\n\nДоказать, что S^* = \\left(\\overline{S}\\right)^*.\nРешение:\n\nS \\subset \\overline{S} \\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*\nПусть p \\in S^* и x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Тогда в силу непрерывности функции f(x) = p^Tx, имеем: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. Значит, p \\in  \\left(\\overline{S}\\right)^*, отсюда S^* \\subset \\left(\\overline{S}\\right)^*\n\n\n\n\nДоказать, что \\left( \\mathbf{conv}(S) \\right)^* = S^*.\nРешение:\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*\nПусть p \\in S^*, x_0 \\in \\mathbf{conv}(S), т.е. x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nЗначит, p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. Значит, p \\in  \\left( \\mathbf{conv}(S) \\right)^*, отсюда S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*\n\n\n\n\nДоказать, что если B(0,r) - шар радиуса r по некоторой норме с центром в нуле, то \\left( B(0,r) \\right)^* = B(0,1/r).\nРешение:\n\nПусть B(0,r) = X, B(0,1/r) = Y. Возьмем вектор нормали p \\in X^*, тогда для любого x \\in X: p^Tx \\ge -1.\nИз всех точек шара X возьмем такую x \\in X, что скалярное произведение её на p: p^Tx было бы минимально, тогда это точка x = -\\frac{p}{\\|p\\|}r\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\|}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nЗначит, X^* \\subset Y.\nТеперь пусть p \\in Y. Нам надо показать, что p \\in X^*, т.е. \\langle p, x\\rangle \\geq -1. Достаточно применить неравенство Коши - Буняковского:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nПоследнее исходит из того, что p \\in B(0,1/r), а x \\in B(0,r).\nЗначит, Y \\subset X^*."
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#double-conjugate-set",
    "href": "docs/theory/Conjugate_set.html#double-conjugate-set",
    "title": "1 Conjugate (dual) set",
    "section": "",
    "text": "Множество S^{**} называется вторым сопряженным к множеству S, если:\n\nS^{**} = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\ge -1 \\;\\; \\forall x \\in S^*\\}"
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#inter-conjugate-and-self-conjugate-sets",
    "href": "docs/theory/Conjugate_set.html#inter-conjugate-and-self-conjugate-sets",
    "title": "1 Conjugate (dual) set",
    "section": "",
    "text": "Множества S_1 и S_2 называются взаимосопряженными, если S_1^* = S_2, S_2^* = S_1.\nМножество S называется самосопряженным, если S^{*} = S."
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#properties",
    "href": "docs/theory/Conjugate_set.html#properties",
    "title": "1 Conjugate (dual) set",
    "section": "",
    "text": "Сопряженное множество всегда замкнуто, выпукло и содержит нуль.\nДля произвольного множества S \\subseteq \\mathbb{R}^n:\n\n   S^{**} = \\overline{ \\mathbf{conv} (S \\cup  \\{0\\}) }\n  \nЕсли S_1 \\subseteq S_2, то S_2^* \\subseteq S_1^*.\n\\left( \\bigcup\\limits_{i=1}^m S_i \\right)^* = \\bigcap\\limits_{i=1}^m S_i^*\nЕсли S - замкнуто, выпукло, включает 0, то S^{**} = S.\nS^* = \\left(\\overline{S}\\right)^*"
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#examples",
    "href": "docs/theory/Conjugate_set.html#examples",
    "title": "1 Conjugate (dual) set",
    "section": "",
    "text": "Доказать, что S^* = \\left(\\overline{S}\\right)^*.\nРешение:\n\nS \\subset \\overline{S} \\rightarrow \\left(\\overline{S}\\right)^* \\subset S^*\nПусть p \\in S^* и x_0 \\in \\overline{S}, x_0 = \\underset{k \\to \\infty}{\\operatorname{lim}} x_k. Тогда в силу непрерывности функции f(x) = p^Tx, имеем: p^T x_k \\ge -1 \\to p^Tx_0 \\ge -1. Значит, p \\in  \\left(\\overline{S}\\right)^*, отсюда S^* \\subset \\left(\\overline{S}\\right)^*\n\n\n\n\nДоказать, что \\left( \\mathbf{conv}(S) \\right)^* = S^*.\nРешение:\n\nS \\subset \\mathbf{conv}(S) \\to \\left( \\mathbf{conv}(S) \\right)^* \\subset S^*\nПусть p \\in S^*, x_0 \\in \\mathbf{conv}(S), т.е. x_0 = \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0.\nЗначит, p^T x_0 = \\sum\\limits_{i=1}^k\\theta_i p^Tx_i \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) = 1 \\cdot (-1) = -1. Значит, p \\in  \\left( \\mathbf{conv}(S) \\right)^*, отсюда S^* \\subset \\left( \\mathbf{conv}(S) \\right)^*\n\n\n\n\nДоказать, что если B(0,r) - шар радиуса r по некоторой норме с центром в нуле, то \\left( B(0,r) \\right)^* = B(0,1/r).\nРешение:\n\nПусть B(0,r) = X, B(0,1/r) = Y. Возьмем вектор нормали p \\in X^*, тогда для любого x \\in X: p^Tx \\ge -1.\nИз всех точек шара X возьмем такую x \\in X, что скалярное произведение её на p: p^Tx было бы минимально, тогда это точка x = -\\frac{p}{\\|p\\|}r\n\n  p^T x = p^T \\left(-\\frac{p}{\\|p\\|}r \\right) = -\\|p\\|r \\ge -1\n  \n\n  \\|p\\| \\le \\frac{1}{r} \\in Y\n  \nЗначит, X^* \\subset Y.\nТеперь пусть p \\in Y. Нам надо показать, что p \\in X^*, т.е. \\langle p, x\\rangle \\geq -1. Достаточно применить неравенство Коши - Буняковского:\n\n  \\|\\langle p, x\\rangle\\| \\leq \\|p\\| \\|x\\| \\leq \\dfrac{1}{r} \\cdot r = 1\n  \nПоследнее исходит из того, что p \\in B(0,1/r), а x \\in B(0,r).\nЗначит, Y \\subset X^*."
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#dual-cones-properties",
    "href": "docs/theory/Conjugate_set.html#dual-cones-properties",
    "title": "1 Conjugate (dual) set",
    "section": "2.1 Dual cones properties",
    "text": "2.1 Dual cones properties\n\nПусть K - замкнутый выпуклый конус. Тогда K^{**} = K.\nДля произвольного множества S \\subseteq \\mathbb{R}^n и конуса K \\subseteq \\mathbb{R}^n:\n\n  \\left( S + K \\right)^* = S^* \\cap K^*\n  \nПусть K_1, \\ldots, K_m - конусы в \\mathbb{R}^n, тогда:\n\n  \\left( \\sum\\limits_{i=1}^m K_i \\right)^* = \\bigcap\\limits_{i=1}^m K_i^*\n  \nПусть K_1, \\ldots, K_m - конусы в \\mathbb{R}^n. Пусть также их пересечение имеет внутреннюю точку, тогда:\n\n  \\left( \\bigcap\\limits_{i=1}^m K_i \\right)^* = \\sum\\limits_{i=1}^m K_i^*"
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#examples-1",
    "href": "docs/theory/Conjugate_set.html#examples-1",
    "title": "1 Conjugate (dual) set",
    "section": "2.2 Examples",
    "text": "2.2 Examples\nНайти сопряженнй конус для монотонного неотрицательного конуса:\n\nK = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\right\\}\n\nРешение:\nЗаметим, что:\n\n\\sum\\limits_{i=1}^nx_iy_i = y_1 (x_1-x_2) + (y_1 + y_2)(x_2 - x_3) + \\ldots + (y_1 + y_2 + \\ldots + y_{n-1})(x_{n-1} - x_n) + (y_1 + \\ldots + y_n)x_n\n\nТак как в представленной сумме в каждом слагаемом второй множитель неотрицательный, то:\n\ny_1 \\ge 0, \\;\\; y_1 + y_2 \\ge 0, \\;\\;\\ldots, \\;\\;y_1 + \\ldots + y_n \\ge 0\n\nЗначит, K^* = \\left\\{ y \\mid \\sum\\limits_{i=1}^k y_i \\ge 0, k = \\overline{1,n}\\right\\}"
  },
  {
    "objectID": "docs/theory/Conjugate_set.html#polyhedra",
    "href": "docs/theory/Conjugate_set.html#polyhedra",
    "title": "1 Conjugate (dual) set",
    "section": "2.3 Polyhedra",
    "text": "2.3 Polyhedra\nМножество решений системы линейных неравенств и равенств представляет собой многогранник:\n\nAx \\preceq b, \\;\\;\\; Cx = d\n\nЗдесь A \\in \\mathbb{R}^{m\\times n}, C \\in \\mathbb{R}^{p \\times n} , а неравенство - поэлементное.\n\n\n2.3.0.1 Теорема:\nПусть x_1, \\ldots, x_m \\in \\mathbb{R}^n. Сопряженным к многогранному множеству:\n\nS = \\mathbf{conv}(x_1, \\ldots, x_k) + \\mathbf{cone}(x_{k+1}, \\ldots, x_m)\n\nявляется полиэдр (многогранник):\n\nS^* = \\left\\{ p \\in \\mathbb{R}^n \\mid \\langle p, x_i\\rangle \\ge -1, i = \\overline{1,k} ; \\langle p, x_i\\rangle \\ge 0, i = \\overline{k+1,m} \\right\\}\n\n\n\n2.3.0.2 Доказательство:\n\nПусть S = X, S^* = Y. Возьмем некоторый p \\in X^*, тогда \\langle p, x_i\\rangle \\ge -1,  i = \\overline{1,k}. В то же время для любых \\theta &gt; 0, i = \\overline{k+1,m}:\n\n  \\langle p, x_i\\rangle \\ge -1 \\to \\langle p, \\theta x_i\\rangle \\ge -1\n  \n\n  \\langle p, x_i\\rangle \\ge -\\frac{1}{\\theta} \\to \\langle p, x_i\\rangle \\ge 0\n  \nЗначит, p \\in Y \\to X^* \\subset Y.\nПусть, напротив, p \\in Y. Для любой точки x \\in X:\n\n   x = \\sum\\limits_{i=1}^m\\theta_i x_i \\;\\;\\;\\;\\;\\; \\sum\\limits_{i=1}^k\\theta_i = 1, \\theta_i \\ge 0\n  \nЗначит:\n\n  \\langle p, x\\rangle = \\sum\\limits_{i=1}^m\\theta_i \\langle p, x_i\\rangle  = \\sum\\limits_{i=1}^k\\theta_i \\langle p, x_i\\rangle + \\sum\\limits_{i=k+1}^m\\theta_i \\langle p, x_i\\rangle \\ge \\sum\\limits_{i=1}^k\\theta_i (-1) + \\sum\\limits_{i=1}^k\\theta_i \\cdot 0 = -1\n  \nЗначит, p \\in X^* \\to Y \\subset X^*.\n\n\n\n2.3.1 5\nНайти и изобразить на плоскости множество, сопряженное к многогранному конусу:\n\nS = \\mathbf{cone} \\left\\{ (-3,1), (2,3), (4,5)\\right\\}\n\nРешение:\nИспользуя теорему выше:\n\nS^* = \\left\\{ -3p_1 + p_2 \\ge 0, 2p_1 + 3p_2 \\ge 0, 4p_1+5p_2 \\ge 0 \\right\\}\n\n\n\n2.3.2 Лемма (теорема) Фаркаша (Фаркаша - Минковского)\nПусть A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Тогда имеет решение одна и только одна из следующих двух систем:\n\n1) \\; Ax = b, x \\ge 0\\;\\;\\;\\;\\;\\;\n\n\n2) \\; p^\\top A \\ge 0, \\langle p,b\\rangle &lt; 0\n\nAx = b при x \\geq 0 означает, что b лежит в конусе, натянутым на столбцы матрицы A.\npA \\geq 0, \\; \\langle p, b \\rangle &lt; 0 означает, что существует разделяющая гиперплоскость между вектором b и конусом из столбцов матрицы A.\n\n2.3.2.1 Следствие:\nПусть A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m. Тогда имеет решение одна и только одна из следующих двух систем:\n\n1) \\; Ax \\le b \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n\n\n2) \\; p^\\top A = 0, \\langle p,b\\rangle &lt; 0, p \\ge 0\n\nЕсли в задаче линейного программирования на минимум допустимое множество непусто и целевая функция ограничена на нём снизу, то задача имеет решение."
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html",
    "href": "docs/theory/convex sets/Affine_sets.html",
    "title": "1 Line",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line passing through them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\theta \\in \\mathbb{R}\n\n\n\n\nFigure 1: Illustration of a line between two vectors x_1 and x_2"
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#affine-combination",
    "href": "docs/theory/convex sets/Affine_sets.html#affine-combination",
    "title": "1 Line",
    "section": "3.1 Affine combination",
    "text": "3.1 Affine combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called affine combination of x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1."
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#affine-hull",
    "href": "docs/theory/convex sets/Affine_sets.html#affine-hull",
    "title": "1 Line",
    "section": "3.2 Affine hull",
    "text": "3.2 Affine hull\nThe set of all affine combinations of points in set S is called the affine hull of S:\n\n\\mathbf{aff}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1\\right\\}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nThe set \\mathbf{aff}(S) is the smallest affine set containing S."
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#interior",
    "href": "docs/theory/convex sets/Affine_sets.html#interior",
    "title": "1 Line",
    "section": "3.3 Interior",
    "text": "3.3 Interior\nThe interior of the set S is defined as the following set:\n\n\\mathbf{int} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\subset S\\}\n\nwhere B(\\mathbf{x}, \\varepsilon) = \\mathbf{x} + \\varepsilon B is the ball centered at point \\mathbf{x} with radius \\varepsilon."
  },
  {
    "objectID": "docs/theory/convex sets/Affine_sets.html#relative-interior",
    "href": "docs/theory/convex sets/Affine_sets.html#relative-interior",
    "title": "1 Line",
    "section": "3.4 Relative Interior",
    "text": "3.4 Relative Interior\nThe relative interior of the set S is defined as the following set:\n\n\\mathbf{relint} (S) = \\{\\mathbf{x} \\in S \\mid \\exists \\varepsilon &gt; 0, \\; B(\\mathbf{x}, \\varepsilon) \\cap \\mathbf{aff} (S) \\subseteq S\\}\n\n\n\n\nFigure 2: Difference between interior and relative interior\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny non-empty convex set S \\subseteq \\mathbb{R}^n has a non-empty relative interior \\mathbf{relint}(S).\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nGive an example of a set S \\subseteq \\mathbb{R}^n, which has an empty interior, but at the same time has a non-empty relative interior \\mathbf{relint}(S)."
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html",
    "href": "docs/theory/convex sets/Projection.html",
    "title": "1 Definitions",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}\n\n\n\n\nProjection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the ball\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0"
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html#distance-between-point-and-set",
    "href": "docs/theory/convex sets/Projection.html#distance-between-point-and-set",
    "title": "1 Definitions",
    "section": "",
    "text": "The distance d from point \\mathbf{y} \\in \\mathbb{R}^n to closed set S \\subset \\mathbb{R}^n:\n\nd(\\mathbf{y}, S, \\| \\cdot \\|) = \\inf\\{\\|x - y\\| \\mid x \\in S \\}"
  },
  {
    "objectID": "docs/theory/convex sets/Projection.html#projection-of-a-point-on-set",
    "href": "docs/theory/convex sets/Projection.html#projection-of-a-point-on-set",
    "title": "1 Definitions",
    "section": "",
    "text": "Projection of a point \\mathbf{y} \\in \\mathbb{R}^n on set S \\subseteq \\mathbb{R}^n is a point \\pi_S(\\mathbf{y}) \\in S:\n\n\\| \\pi_S(\\mathbf{y}) - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{y}\\|, \\forall \\mathbf{x} \\in S\n\n\nif a set is open, and a point is beyond this set, then its projection on this set does not exist.\nif a point is in set, then its projection is the point itself\n\n\\pi_S(\\mathbf{y}) = \\underset{\\mathbf{x}}{\\operatorname{argmin}} \\|\\mathbf{x}-\\mathbf{y}\\|\n\nLet S \\subseteq \\mathbb{R}^n - convex closed set. Let the point \\mathbf{y} \\in \\mathbb{R}^n и \\mathbf{\\pi} \\in S. Then if for all \\mathbf{x} \\in S the inequality holds:\n\n  \\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle \\ge 0,\n  \nthen \\pi is the projection of the point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi.\nLet S \\subseteq \\mathbb{R}^n - affine set. Let we have points \\mathbf{y} \\in \\mathbb{R}^n and \\mathbf{\\pi} \\in S. Then \\pi is a projection of point \\mathbf{y} on S, so \\pi_S (\\mathbf{y}) = \\pi if and only if for all \\mathbf{x} \\in S the inequality holds:\n\n\n\\langle \\pi  -\\mathbf{y}, \\mathbf{x} - \\pi\\rangle = 0\n\n\nSufficient conditions of existence of a projection. If S \\subseteq \\mathbb{R}^n - closed set, then the projection on set S exists for any point.\nSufficient conditions of uniqueness of a projection. If S \\subseteq \\mathbb{R}^n - closed convex set, then the projection on set S is unique for any point.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_0\\| \\le R \\}, y \\notin S\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 1: Projection of point to the ball\n\n\n\nBuild a hypothesis from the figure: \\pi = x_0 + R \\cdot \\frac{y - x_0}{\\|y - x_0\\|}\nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  \\left( x_0 - y + R \\frac{y - x_0}{\\|y - x_0\\|} \\right)^T\\left( x - x_0 - R \\frac{y - x_0}{\\|y - x_0\\|} \\right) =\n  \n\n  \\left( \\frac{(y - x_0)(R - \\|y - x_0\\|)}{\\|y - x_0\\|} \\right)^T\\left( \\frac{(x-x_0)\\|y-x_0\\|-R(y - x_0)}{\\|y - x_0\\|} \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|^2} \\left(y - x_0 \\right)^T\\left( \\left(x-x_0\\right)\\|y-x_0\\|-R\\left(y - x_0\\right) \\right) =\n  \n\n  \\frac{R - \\|y - x_0\\|}{\\|y - x_0\\|} \\left( \\left(y - x_0 \\right)^T\\left( x-x_0\\right)-R\\|y - x_0\\| \\right) =\n  \n\n  \\left(R - \\|y - x_0\\| \\right) \\left( \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|}-R \\right)\n  \nThe first factor is negative for point selection y. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality:\n\n  (y - x_0 )^T( x-x_0) \\le \\|y - x_0\\|\\|x-x_0\\|\n  \n\n  \\frac{(y - x_0 )^T( x-x_0)}{\\|y - x_0\\|} - R \\le \\frac{\\|y - x_0\\|\\|x-x_0\\|}{\\|y - x_0\\|} - R = \\|x - x_0\\| - R \\le 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid c^T x = b \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projection of point to the ball\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\alpha c. Coefficient \\alpha is chosen so that \\pi \\in S: c^T \\pi = b, so:\n\n  c^T (y + \\alpha c) = b\n  \n\n  c^Ty + \\alpha c^T c = b\n  \n\n  c^Ty = b - \\alpha c^T c\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + \\alpha c - y)^T(x - y - \\alpha c) =\n  \n\n   \\alpha c^T(x - y - \\alpha c) =\n  \n\n   \\alpha (c^Tx) - \\alpha (c^T y) - \\alpha^2 (c^Tc) =\n  \n\n   \\alpha b - \\alpha (b - \\alpha c^T c) - \\alpha^2 c^Tc =\n  \n\n   \\alpha b - \\alpha b + \\alpha^2 c^T c - \\alpha^2 c^Tc = 0 \\ge 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\pi_S (y) = \\pi, if S = \\{x \\in \\mathbb{R}^n \\mid Ax = b, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\}, y \\notin S.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nFigure 3: Projection of point to the set of linear equations\n\n\n\nBuild a hypothesis from the figure: \\pi = y + \\sum\\limits_{i=1}^m\\alpha_i A_i = y + A^T \\alpha. Coefficient \\alpha is chosen so that \\pi \\in S: A \\pi = b, so:\n\n  A(y + A^T\\alpha) = b\n  \n\n  Ay = b - A A^T\\alpha\n  \nCheck the inequality for a convex closed set: (\\pi - y)^T(x - \\pi) \\ge 0\n\n  (y + A^T\\alpha  - y)^T(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T A(x - y - A^T\\alpha) =\n  \n\n   \\alpha^T (Ax) - \\alpha^T (A y) - \\alpha^T (AA^T \\alpha) =\n  \n\n   \\alpha^T b - \\alpha^T (b - A A^T\\alpha) - \\alpha^T AA^T \\alpha =\n  \n\n   \\alpha^T b - \\alpha^T b + \\alpha^T AA^T \\alpha - \\alpha^T AA^T \\alpha = 0 \\ge 0"
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html",
    "href": "docs/methods/adaptive_metrics/Newton.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}.\n\n\n\nLet us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\n\n\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#newtons-method-to-find-the-equation-roots",
    "href": "docs/methods/adaptive_metrics/Newton.html#newtons-method-to-find-the-equation-roots",
    "title": "1 Intuition",
    "section": "",
    "text": "Consider the function \\varphi(x): \\mathbb{R} \\to \\mathbb{R}. Let there be equation \\varphi(x^*) = 0. Consider a linear approximation of the function \\varphi(x) near the solution (x^* - x = \\Delta x):\n\n\\varphi(x^*) = \\varphi(x + \\Delta x) \\approx \\varphi(x) + \\varphi'(x)\\Delta x.\n\nWe get an approximate equation:\n\n\\varphi(x) + \\varphi'(x) \\Delta x = 0\n\nWe can assume that the solution to equation \\Delta x = - \\dfrac{\\varphi(x)}{\\varphi'(x)} will be close to the optimal \\Delta x^* = x^* - x.\nWe get an iterative scheme:\n\nx_{k+1} = x_k - \\dfrac{\\varphi(x_k)}{\\varphi'(x_k)}.\n\n\nThis reasoning can be applied to the unconditional minimization task of the f(x) function by writing down the necessary extremum condition:\n\nf'(x^*) = 0\n\nHere \\varphi(x) = f'(x), \\; \\varphi'(x) = f''(x). Thus, we get the Newton optimization method in its classic form:\n\n\\tag{Newton}\nx_{k+1} = x_k - \\left[ f''(x_k)\\right]^{-1}f'(x_k).\n\nWith the only clarification that in the multidimensional case: x \\in \\mathbb{R}^n, \\; f'(x) = \\nabla f(x) \\in \\mathbb{R}^n, \\; f''(x) = \\nabla^2 f(x) \\in \\mathbb{R}^{n \\times n}."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#second-order-taylor-approximation-of-the-function",
    "href": "docs/methods/adaptive_metrics/Newton.html#second-order-taylor-approximation-of-the-function",
    "title": "1 Intuition",
    "section": "",
    "text": "Let us now give us the function f(x) and a certain point x_k. Let us consider the square approximation of this function near x_k:\n\n\\tilde{f}(x) = f(x_k) + \\langle f'(x_k), x - x_k\\rangle + \\frac{1}{2} \\langle f''(x_k)(x-x_k), x-x_k \\rangle.\n\nThe idea of the method is to find the point x_{k+1}, that minimizes the function \\tilde{f}(x), i.e. \\nabla \\tilde{f}(x_{k+1}) = 0.\n\n\n\\begin{align*}\n\\nabla \\tilde{f}(x_{k+1}) &= f'(x_{k}) + f''(x_{k})(x_{k+1} - x_k) = 0 \\\\\nf''(x_{k})(x_{k+1} - x_k) &= -f'(x_{k}) \\\\\n\\left[ f''(x_k)\\right]^{-1} f''(x_{k})(x_{k+1} - x_k) &= -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}) \\\\\nx_{k+1} &= x_k -\\left[ f''(x_k)\\right]^{-1} f'(x_{k}).\n\\end{align*}\n\nLet us immediately note the limitations related to the necessity of the Hessian’s non-degeneracy (for the method to exist), as well as its positive definiteness (for the convergence guarantee).\n\n\nQuadratic approximation and Newton step (in green) for varying starting points (in red). Note that when the starting point is far from the global minimizer (in 0), the Newton step totally overshoots the global minimizer. Picture was taken from the post."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#theorem",
    "href": "docs/methods/adaptive_metrics/Newton.html#theorem",
    "title": "1 Intuition",
    "section": "2.1 Theorem",
    "text": "2.1 Theorem\nLet f(x) be a strongly convex twice continuously differentiated function at \\mathbb{R}^n, for the second derivative of which inequalities are executed: \\mu I_n\\preceq f''(x) \\preceq L I_n . Then Newton’s method with a constant step locally converges to solving the problem with superlinear speed. If, in addition, Hessian is Lipschitz continuous, then this method converges locally to x^* at a quadratic rate."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Newton.html#possible-directions",
    "href": "docs/methods/adaptive_metrics/Newton.html#possible-directions",
    "title": "1 Intuition",
    "section": "3.1 Possible directions",
    "text": "3.1 Possible directions\n\nNewton’s damped method (adaptive stepsize)\nQuasi-Newton methods (we don’t calculate the Hessian, we build its estimate - BFGS)\nQuadratic evaluation of the function by the first order oracle (superlinear convergence)\nThe combination of the Newton method and the gradient descent (interesting direction)\nHigher order methods (most likely useless)"
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html",
    "href": "docs/methods/adaptive_metrics/CG.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Originally, the conjugate gradients method was created to solve a system of linear equations.\n\nAx = b\n\nWithout special efforts the problem can be presented in the form of minimization of the quadratic function, and then generalized on a case of non quadratic function. We will start with the parabolic case and try to construct a conjugate gradients method for it. Let us consider the classical problem of minimization of the quadratic function:\n\nf(x) = \\frac{1}{2}x^\\top A x - b^\\top x + c \\to \\min\\limits_{x \\in \\mathbb{R}^n }\n\nHere x \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{n \\times n}, b \\in \\mathbb{R}^n, c \\in \\mathbb{R}."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/CG.html#example-1",
    "href": "docs/methods/adaptive_metrics/CG.html#example-1",
    "title": "1 Introduction",
    "section": "4.1 Example 1",
    "text": "4.1 Example 1\nProve that if a set of vectors d_1, \\ldots, d_k - are A-conjugate (each pair of vectors is A-conjugate), these vectors are linearly independent. A \\in \\mathbb{S}^n_{++}.\nSolution:\nWe’ll show, that if \\sum\\limits_{i=1}^k\\alpha_k d_k = 0, than all coefficients should be equal to zero:\n\n  \\begin{align*}\n  0 &= \\sum\\limits_{i=1}^n\\alpha_k d_k \\\\\n    &= d_j^\\top A \\left( \\sum\\limits_{i=1}^n\\alpha_k d_k \\right) \\\\\n    &=  \\sum\\limits_{i=1}^n \\alpha_k d_j^\\top A d_k  \\\\\n    &=  \\alpha_j d_j^\\top A d_j  + 0 + \\ldots + 0\\\\\n  \\end{align*}\n  \nThus, \\alpha_j = 0, for all other indices one have perform the same process"
  },
  {
    "objectID": "docs/methods/adaptive_metrics/KFAC.html",
    "href": "docs/methods/adaptive_metrics/KFAC.html",
    "title": "1 Intuition",
    "section": "",
    "text": "1 Intuition\n\n\n2 References\n\nNotes on the Limitations of the Empirical Fisher Approximation\n\n\n\n3 Code\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "docs/methods/index.html",
    "href": "docs/methods/index.html",
    "title": "1 General formulation",
    "section": "",
    "text": "\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. }  g_i(x) \\leq& 0, \\; i = 1,\\ldots,m\\\\\nh_j(x) =& 0, \\; j = 1,\\ldots,k\\\\\n\\end{split}\n\nSome necessary or/and sufficient conditions are known (See {% include link.html title=‘Optimality conditions. KKT’%} and {% include link.html title=‘Convex optimization problem’ %}) * In fact, there might be very challenging to recognize the convenient form of optimization problem. * Analytical solution of KKT could be inviable.\n\n\nTypically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x"
  },
  {
    "objectID": "docs/methods/index.html#iterative-methods",
    "href": "docs/methods/index.html#iterative-methods",
    "title": "1 General formulation",
    "section": "",
    "text": "Typically, the methods generate an infinite sequence of approximate solutions\n\n\\{x_t\\},\n\nwhich for a finite number of steps (or better - time) converges to an optimal (at least one of the optimal) solution x_*.\n\ndef GeneralScheme(x, epsilon):\n    while not StopCriterion(x, epsilon):\n        OracleResponse = RequestOracle(x)\n        x = NextPoint(x, OracleResponse)\n    return x"
  },
  {
    "objectID": "docs/methods/index.html#unsolvability",
    "href": "docs/methods/index.html#unsolvability",
    "title": "1 General formulation",
    "section": "2.1 Unsolvability",
    "text": "2.1 Unsolvability\nIn general, optimization problems are unsolvable. ¯\\(ツ)/¯\nConsider the following simple optimization problem of a function over unit cube:\n\n\\begin{split}\n& \\min_{x \\in \\mathbb{R}^n} f(x)\\\\\n\\text{s.t. } &  x \\in \\mathbb{B}^n\n\\end{split}\n\nWe assume, that the objective function f (\\cdot) : \\mathbb{R}^n \\to \\mathbb{R} is Lipschitz continuous on \\mathbb{B}^n:\n\n| f (x) − f (y) | \\leq L \\| x − y \\|_{\\infty} \\forall x,y \\in \\mathbb{B}^n,\n\nwith some constant L (Lipschitz constant). Here \\mathbb{B}^n - the n-dimensional unit cube\n\n\\mathbb{B}^n = \\{x \\in \\mathbb{R}^n \\mid 0 \\leq x_i \\leq 1, i = 1, \\ldots, n\\}\n\nOur goal is to find such \\tilde{x}: \\vert f(\\tilde{x}) - f^*\\vert \\leq \\varepsilon for some positive \\varepsilon. Here f^* is the global minima of the problem. Uniform grid with p points on each dimension guarantees at least this quality:\n\n\\| \\tilde{x} − x_* \\|_{\\infty} \\leq \\frac{1}{2p},\n\nwhich means, that\n\n|f (\\tilde{x}) − f (x_*)| \\leq \\frac{L}{2p}\n\nOur goal is to find the p for some \\varepsilon. So, we need to sample $ ()^n$ points, since we need to measure function in p^n points. Doesn’t look scary, but if we’ll take L = 2, n = 11, \\varepsilon = 0.01, computations on the modern personal computers will take 31,250,000 years."
  },
  {
    "objectID": "docs/methods/index.html#stopping-rules",
    "href": "docs/methods/index.html#stopping-rules",
    "title": "1 General formulation",
    "section": "2.2 Stopping rules",
    "text": "2.2 Stopping rules\n\nArgument closeness:\n\n  \\| x_k - x_*  \\|_2 &lt; \\varepsilon\n  \nFunction value closeness:\n\n  \\| f_k - f^* \\|_2 &lt; \\varepsilon\n  \nCloseness to a critical point\n\n  \\| f'(x_k) \\|_2 &lt; \\varepsilon\n  \n\nBut x_* and f^* = f(x_*) are unknown!\nSometimes, we can use the trick:\n\n\\|x_{k+1} - x_k \\| = \\|x_{k+1} - x_k + x_* - x_* \\| \\leq \\|x_{k+1} - x_* \\| + \\| x_k - x_* \\| \\leq 2\\varepsilon\n\nNote: it’s better to use relative changing of these values, i.e. \\dfrac{\\|x_{k+1} - x_k \\|_2}{\\| x_k \\|_2}."
  },
  {
    "objectID": "docs/methods/index.html#local-nature-of-the-methods",
    "href": "docs/methods/index.html#local-nature-of-the-methods",
    "title": "1 General formulation",
    "section": "2.3 Local nature of the methods",
    "text": "2.3 Local nature of the methods"
  },
  {
    "objectID": "docs/methods/line_search/golden_search.html",
    "href": "docs/methods/line_search/golden_search.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nThe idea is quite similar to the dichotomy method. There are two golden points on the line segment (left and right) and the insightful idea is, that on the next iteration one of the points will remain the golden point.\n\n\n\n2 Algorithm\ndef golden_search(f, a, b, epsilon):\n    tau = (sqrt(5) + 1) / 2\n    y = a + (b - a) / tau**2\n    z = a + (b - a) / tau\n    while b - a &gt; epsilon:\n        if f(y) &lt;= f(z):\n            b = z\n            z = y\n            y = a + (b - a) / tau**2\n        else:\n            a = y\n            y = z\n            z = a + (b - a) / tau\n    return (a + b) / 2\n\n\n3 Bounds\n\n|x_{k+1} - x_*| \\leq b_{k+1} - a_{k+1} = \\left( \\frac{1}{\\tau} \\right)^{N-1} (b - a) \\approx 0.618^k(b-a),\n\nwhere \\tau = \\frac{\\sqrt{5} + 1}{2}.\n\nThe geometric progression constant more than the dichotomy method - 0.618 worse than 0.5\nThe number of function calls is less than for the dichotomy method - 0.707 worse than 0.618 - (for each iteration of the dichotomy method, except for the first one, the function is calculated no more than 2 times, and for the gold method - no more than one)"
  },
  {
    "objectID": "docs/methods/line_search/inexact.html",
    "href": "docs/methods/line_search/inexact.html",
    "title": "1 Sufficient decrease",
    "section": "",
    "text": "This strategy of inexact line search works well in practice, as well as it has the following geometric interpretation:\n\n1 Sufficient decrease\nLet’s consider the following scalar function while being at a specific point of x_k:\n\n\\phi(\\alpha) = f(x_k - \\alpha\\nabla f(x_k)), \\alpha \\geq 0\n\nconsider first order approximation of \\phi(\\alpha):\n\n\\phi(\\alpha) \\approx f(x_k) - \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\nA popular inexact line search condition stipulates that \\alpha should first of all give sufficient decrease in the objective function f, as measured by the following inequality:\n\nf(x_k - \\alpha \\nabla f (x_k)) \\leq f(x_k) - c_1 \\cdot \\alpha\\nabla f(x_k)^\\top \\nabla f(x_k)\n\nfor some constant c_1 \\in (0,1). (Note, that c_1 = 1 stands for the first order Taylor approximation of \\phi(\\alpha)). This is also called Armijo condition. The problem of this condition is, that it could accept arbitrary small values \\alpha, which may slow down solution of the problem. In practice, c_1 is chosen to be quite small, say c_1 \\approx 10^{−4}.\n\n\n2 Curvature condition\nTo rule out unacceptably short steps one can introduce a second requirement:\n\n-\\nabla f (x_k - \\alpha \\nabla f(x_k))^\\top \\nabla f(x_k) \\geq c_2 \\nabla f(x_k)^\\top(- \\nabla f(x_k))\n\nfor some constant c_2 \\in (c_1,1), where c_1 is a constant from Armijo condition. Note that the left-handside is simply the derivative \\nabla_\\alpha \\phi(\\alpha), so the curvature condition ensures that the slope of \\phi(\\alpha) at the target point is greater than c_2 times the initial slope \\nabla_\\alpha \\phi(\\alpha)(0). Typical values of c_2 \\approx 0.9 for Newton or quasi-Newton method. The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions.\n\n\n3 Goldstein conditions\nLet’s consider also 2 linear scalar functions \\phi_1(\\alpha), \\phi_2(\\alpha):\n\n\\phi_1(\\alpha) = f(x_k) - c_1 \\alpha \\|\\nabla f(x_k)\\|^2\n\nand\n\n\\phi_2(\\alpha) = f(x_k) - c_2 \\alpha \\|\\nabla f(x_k)\\|^2\n\nNote, that Goldstein-Armijo conditions determine the location of the function \\phi(\\alpha) between \\phi_1(\\alpha) and \\phi_2(\\alpha). Typically, we choose c_1 = \\rho and c_2 = 1 - \\rho, while  \\rho \\in (0.5, 1).\n\n\n\n4 References\n\nNumerical Optimization by J.Nocedal and S.J.Wright.\nInteractive Wolfe Line Search Example by fmin library."
  },
  {
    "objectID": "docs/methods/fom/SGD.html",
    "href": "docs/methods/fom/SGD.html",
    "title": "1 Summary",
    "section": "",
    "text": "Suppose, our target function is the sum of functions.\n\n\\min\\limits_{\\theta \\in \\mathbb{R}^{p}} g(\\theta) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\theta)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost (calculation of gradient is linear in n).\nThus, we can switch from the full gradient calculation to its unbiased estimator:\n\n\\theta_{k+1} = \\theta_k - \\alpha_k\\nabla f_{i_k} (\\theta),\n\nwhere we randomly choose i_k index of point at each iteration uniformly:\n\n\\mathbb{E}[\\nabla f_{i_k} (\\theta)] = \\sum_{i=1}^n p(i_k=i) \\nabla f_i(\\theta) = \\dfrac{1}{n}\\sum_{i=1}^n \\nabla f_i(\\theta) = \\nabla g(\\theta)\n\nIterations could be n times cheaper! But convergence requires \\alpha_k \\to 0."
  },
  {
    "objectID": "docs/methods/fom/SGD.html#general-setup",
    "href": "docs/methods/fom/SGD.html#general-setup",
    "title": "1 Summary",
    "section": "2.1 General setup",
    "text": "2.1 General setup\nWe consider classic finite-sample average minimization:\n\n\\min_{x \\in \\mathbb{R}^p} f(x) = \\min_{x \\in \\mathbb{R}^p}\\frac{1}{n} \\sum_{i=1}^n f_i(x)\n\nLet us consider stochastic gradient descent assuming \\nabla f is Lipschitz:\n\n\\tag{SGD}\nx_{k+1} = x_k - \\alpha_k \\nabla f_{i_k}(x_k)\n\nLipschitz continiity implies:\n\nf(x_{k+1}) \\leq f(x_k) + \\langle \\nabla f(x_k), x_{k+1} - x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|^2\n\nusing (\\text{SGD}):\n\nf(x_{k+1}) \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2\n\nNow let’s take expectation with respect to i_k:\n\n\\mathbb{E}[f(x_{k+1})] \\leq \\mathbb{E}[f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\nabla f_{i_k}(x_k)\\rangle + \\alpha_k^2\\frac{L}{2} \\|\\nabla f_{i_k}(x_k)\\|^2]\n\nUsing linearity of expectation:\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\langle \\nabla f(x_k),  \\mathbb{E}[\\nabla f_{i_k}(x_k)]\\rangle + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nSince uniform sampling implies unbiased estimate of gradient: \\mathbb{E}[\\nabla f_{i_k}(x_k)] = \\nabla f(x_k):\n\n\\mathbb{E}[f(x_{k+1})] \\leq f(x_k) - \\alpha_k \\|\\nabla f(x_k)\\|^2 + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]"
  },
  {
    "objectID": "docs/methods/fom/SGD.html#polyak-lojasiewicz-conditions",
    "href": "docs/methods/fom/SGD.html#polyak-lojasiewicz-conditions",
    "title": "1 Summary",
    "section": "2.2 Polyak-Lojasiewicz conditions",
    "text": "2.2 Polyak-Lojasiewicz conditions\n\n\\tag{PL}\n\\frac{1}{2}\\|\\nabla f(x)\\|_2^2 \\geq \\mu(f(x) - f^*), \\forall x \\in \\mathbb{R}^p\n\nThis inequality simply requires that the gradient grows faster than a quadratic function as we move away from the optimal function value. Note, that strong convexity implies \\text{PL}, but not vice versa. Using \\text{PL} we can write:\n\n\\mathbb{E}[f(x_{k+1})] - f^* \\leq (1 - 2\\alpha_k \\mu) [f(x_k) - f^*] + \\alpha_k^2\\frac{L}{2} \\mathbb{E}[\\|\\nabla f_{i_k}(x_k)\\|^2]\n\nThis bound already indicates, that we have something like linear convergence if far from solution and gradients are similar, but no progress if close to solution or have high variance in gradients at the same time."
  },
  {
    "objectID": "docs/methods/fom/SGD.html#stochastic-subgradient-descent",
    "href": "docs/methods/fom/SGD.html#stochastic-subgradient-descent",
    "title": "1 Summary",
    "section": "2.3 Stochastic subgradient descent",
    "text": "2.3 Stochastic subgradient descent\n\n\\tag{SSD}\nx_{k+1} = x_k - \\alpha_k g_{i_k}\n\nfor some g_{i_k} \\in \\partial f_{i_k}(x_k).\nFor convex f we have\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 \\mathbb{E}[\\|g_{i_k}\\|^2]\n\nHere we can see, that step-size \\alpha_k controls how fast we move towards solution. And squared step-size \\alpha_k^2 controls how much variance moves us away. Usually, we bound \\mathbb{E}[\\|g_{i_k}\\|^2] by some constant B^2.\n\n\\mathbb{E}[\\|x_{k+1} - x^*\\|^2] = \\|x_{k} - x^*\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle + \\alpha_k^2 B^2\n\nIf we also have strong convexity:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu) \\|x_{k-1} - x^*\\|^2 + \\alpha_k^2 B^2\n\nAnd finally, with \\alpha_k = \\alpha &lt; \\frac{2}{\\mu}:\n\n\\mathbb{E}[\\|x_{k} - x^*\\|^2] \\leq (1 - 2\\alpha_k \\mu)^{k} R^2 + \\frac{\\alpha B^2}{2\\mu},\n\nwhere $R = |x_0- x^*| $"
  },
  {
    "objectID": "docs/methods/fom/fom.html",
    "href": "docs/methods/fom/fom.html",
    "title": "1 Materials",
    "section": "",
    "text": "Now we have only first order information from the oracle.\n\n1 Materials\n\nVisualization of optimization algorithms."
  },
  {
    "objectID": "docs/methods/fom/Lookahead.html",
    "href": "docs/methods/fom/Lookahead.html",
    "title": "1 Summary",
    "section": "",
    "text": "1 Summary\nThe lookahead method provides an interesting way to accelerate and stabilize algorithms of stochastic gradient descent family. The main idea is quite simple:\n\nSet some number k. Take initial parameter weights x_0 = \\hat{x}_0\nDo k steps with your favorite optimization algorithm: \\hat{x}_1, \\ldots, \\hat{x}_k\nTake some value between initial x_0 and \\hat{x}_k:\n\n  x_{t+1} = (1 - \\alpha)x_{t} + \\alpha\\hat{x_k}\n  \nUpdate \\hat{x_0} with the last output of the algorithm.\nRepeat profit\n\nAuthors introduced separation on the fast weights and slow weights, which naturally arise in the described procedure. The paper contains proof for optimal step-size of the quadratic loss function and provides understanding why this technique could reduce variance of {% include link.html title=“Stochastic gradient descent” %} in the noisy quadratic case. Moreover, this work compares the convergence rate in dependency of condition number of the squared system.\nIt is worth to say, that author claims significant improvement in practical huge scale settings (ImageNet, CIFAR10,CIFAR100)\n \n\n\n2 Pros\n\nInteresting idea, costs almost nothing, why not to try?\nWorks with any SGD-like optimizer (SGD, Adam, RmsProp)\nAnalytical approach to quadratic case.\nWide set of empirical tests (Image classification, Neural Translation, LSTM training)\n\n\n\n3 Cons\n\nLack of test loss pictures, the majority of them obtained for the train loss/accuracy\nLack of pictures with different batch sizes\nDifficult to analyze the method analytically"
  },
  {
    "objectID": "docs/methods/fom/SAG.html",
    "href": "docs/methods/fom/SAG.html",
    "title": "",
    "section": "",
    "text": "A classical problem of minimizing finite sum of the smooth and convex functions was considered.\n\n\\min\\limits_{x \\in \\mathbb{R}^{p}} g(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost. Baseline solution to the problem is to calculate the loss function and the corresponding gradient vector only on the small subset of indicies from i = 1, \\ldots, n, which usually refers as {% include link.html title=‘Stochastic gradient descent’%}. The authors claims, that the convergence rate of proposed algorithm is the same a for the full {% include link.html title=‘Gradient descent’%} method (\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) for convex functions and \\mathcal{O}\\left(\\dfrac{1}{k}\\right) for strongly convex objectives), but the iteration costs remains the same as for the stochastic version.\nThe method itself takes the following form:\n\n\\tag{SAG}\nx_{k+1}=x_{k}-\\frac{\\alpha_{k}}{p} \\sum_{i=1}^{p} y^{i}_{k}\n\nwhere at each iteration only random summand of a gradient is updated:\n\n\\tag{SAG}\ny^{i}_{k}=\\left\\{\\begin{array}{ll}{f_{i}^{\\prime}\\left(x_{k}\\right)}, & {\\text { if } i=i_{k}} \\\\ {y^{i}_{k-1}}, & {\\text { otherwise }}\\end{array}\\right.\n\n\nThere is a dependency on dimensionality factor n in bounds. However, it can be improved using restart technique.\nEmpirical results were only shown on logistic regression with Tikhonov regularization problems on different datasets.\nBatch and non- uniform versions are also presented in the paper.\nThe first known paper, that contains proof of linear convergence for the convex case."
  },
  {
    "objectID": "docs/methods/fom/SAG.html#summary",
    "href": "docs/methods/fom/SAG.html#summary",
    "title": "",
    "section": "",
    "text": "A classical problem of minimizing finite sum of the smooth and convex functions was considered.\n\n\\min\\limits_{x \\in \\mathbb{R}^{p}} g(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n\nThis problem usually arises in Deep Learning, where the gradient of the loss function is calculating over the huge number of data points, which could be very expensive in terms of the iteration cost. Baseline solution to the problem is to calculate the loss function and the corresponding gradient vector only on the small subset of indicies from i = 1, \\ldots, n, which usually refers as {% include link.html title=‘Stochastic gradient descent’%}. The authors claims, that the convergence rate of proposed algorithm is the same a for the full {% include link.html title=‘Gradient descent’%} method (\\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) for convex functions and \\mathcal{O}\\left(\\dfrac{1}{k}\\right) for strongly convex objectives), but the iteration costs remains the same as for the stochastic version.\nThe method itself takes the following form:\n\n\\tag{SAG}\nx_{k+1}=x_{k}-\\frac{\\alpha_{k}}{p} \\sum_{i=1}^{p} y^{i}_{k}\n\nwhere at each iteration only random summand of a gradient is updated:\n\n\\tag{SAG}\ny^{i}_{k}=\\left\\{\\begin{array}{ll}{f_{i}^{\\prime}\\left(x_{k}\\right)}, & {\\text { if } i=i_{k}} \\\\ {y^{i}_{k-1}}, & {\\text { otherwise }}\\end{array}\\right.\n\n\nThere is a dependency on dimensionality factor n in bounds. However, it can be improved using restart technique.\nEmpirical results were only shown on logistic regression with Tikhonov regularization problems on different datasets.\nBatch and non- uniform versions are also presented in the paper.\nThe first known paper, that contains proof of linear convergence for the convex case."
  },
  {
    "objectID": "docs/methods/fom/SAG.html#bounds",
    "href": "docs/methods/fom/SAG.html#bounds",
    "title": "",
    "section": "2 Bounds",
    "text": "2 Bounds\nFor a constant step size \\alpha = \\dfrac{1}{16 L}, where L stands for the Lipschitz constant of a gradient of each function $ f_i(x) $ (in practice, it means that $ L = _{i=1, , n} L_i $).\n\n\\mathbb{E}\\left[g\\left(\\overline{x}_{k}\\right)\\right]-g\\left(x^{*}\\right) \\leqslant \\frac{32 n}{k} C_{0},\n\nwhere $ C_0=g(x_0)-g(x^*)+ \\| x_0 - x\\|2 +$ in convex case and\n\n\\mathbb{E}\\left[g\\left(x_{k}\\right)\\right]-g\\left(x^*\\right) \\leqslant\\left(1-\\min \\left\\{\\frac{\\mu}{16 L}, \\frac{1}{8 n}\\right\\}\\right)^{k} C_{0}\n\nin \\mu - strongly convex case."
  },
  {
    "objectID": "docs/methods/fom/ADAM.html",
    "href": "docs/methods/fom/ADAM.html",
    "title": "",
    "section": "",
    "text": "Adam is the stochastic first order optimization algorithm, that uses historical information about stochastic gradients and incorporates it in attempt to estimate second order moment of stochastic gradients.\n\n\\tag{ADAM}\n\\begin{align*}\nx_{k+1} &= x_k - \\alpha_k \\dfrac{\\widehat{m_k}}{\\sqrt{\\widehat{v_k}} + \\epsilon} \\\\\n\\tag{First moment estimation}\n\\widehat{m_k} &= \\dfrac{m_k}{1 - \\beta_1^k} \\\\\nm_k &= \\beta_1 m_{k-1} + (1 - \\beta_1) g_k \\\\\n\\tag{Second moment estimation}\n\\widehat{v_k} &= \\dfrac{v_k}{1 - \\beta_2^k} \\\\\nv_k &= \\beta_2 v_{k-1} + (1 - \\beta_2)g_k^2 \\\\\n\\end{align*}\n\nAll vector operations are element-wise. \\alpha = 0.001, \\beta_1 = 0.9, \\beta_2 = 0.999 - the default values for hyperparameters (\\epsilon here is needed for avoiding zero division problems) and g_k = \\nabla f(x_k, \\xi_k) is the sample of stochastic gradient.\n\nWe can consider this approach as normalization of each parameter by using individual learning rates on  \\mathcal{N} (0,1), since \\mathbb{E}\\_{\\xi_k}[g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{m_k}] and \\mathbb{E}\\_{\\xi_k}[g_k \\odot g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{v_k}].\nThere are some issues with Adam effectiveness and some works, stated, that adaptive metrics methods could lead to worse generalization.\nThe name came from “Adaptive Moment estimation”."
  },
  {
    "objectID": "docs/methods/fom/ADAM.html#summary",
    "href": "docs/methods/fom/ADAM.html#summary",
    "title": "",
    "section": "",
    "text": "Adam is the stochastic first order optimization algorithm, that uses historical information about stochastic gradients and incorporates it in attempt to estimate second order moment of stochastic gradients.\n\n\\tag{ADAM}\n\\begin{align*}\nx_{k+1} &= x_k - \\alpha_k \\dfrac{\\widehat{m_k}}{\\sqrt{\\widehat{v_k}} + \\epsilon} \\\\\n\\tag{First moment estimation}\n\\widehat{m_k} &= \\dfrac{m_k}{1 - \\beta_1^k} \\\\\nm_k &= \\beta_1 m_{k-1} + (1 - \\beta_1) g_k \\\\\n\\tag{Second moment estimation}\n\\widehat{v_k} &= \\dfrac{v_k}{1 - \\beta_2^k} \\\\\nv_k &= \\beta_2 v_{k-1} + (1 - \\beta_2)g_k^2 \\\\\n\\end{align*}\n\nAll vector operations are element-wise. \\alpha = 0.001, \\beta_1 = 0.9, \\beta_2 = 0.999 - the default values for hyperparameters (\\epsilon here is needed for avoiding zero division problems) and g_k = \\nabla f(x_k, \\xi_k) is the sample of stochastic gradient.\n\nWe can consider this approach as normalization of each parameter by using individual learning rates on  \\mathcal{N} (0,1), since \\mathbb{E}\\_{\\xi_k}[g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{m_k}] and \\mathbb{E}\\_{\\xi_k}[g_k \\odot g_k] = \\mathbb{E}\\_{\\xi_k}[\\widehat{v_k}].\nThere are some issues with Adam effectiveness and some works, stated, that adaptive metrics methods could lead to worse generalization.\nThe name came from “Adaptive Moment estimation”."
  },
  {
    "objectID": "docs/methods/fom/ADAM.html#bounds",
    "href": "docs/methods/fom/ADAM.html#bounds",
    "title": "",
    "section": "2 Bounds",
    "text": "2 Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert \\mathbb{E} [f(x_k)] - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert \\mathbb{E}[x_k] - x^* \\Vert \\leq\n\n\n\n\nConvex\n \\mathcal{O}\\left(\\dfrac{1}{\\sqrt{k}} \\right) \nSublinear\n\n\n\n\nVersion of Adam for a strongly convex functions is considered in this work. The obtained rate is $ ( )$, while the version for truly linear rate remains undiscovered."
  },
  {
    "objectID": "docs/methods/Autograd.html",
    "href": "docs/methods/Autograd.html",
    "title": "1 Idea",
    "section": "",
    "text": "Automatic differentiation is a scheme, that allows you to compute a value of gradient of function with a cost of computing function itself only twice. ## Chain rule We will illustrate some important matrix calculus facts for specific cases ### Univariate chain rule Suppose, we have the following functions R: \\mathbb{R} \\to \\mathbb{R} , L: \\mathbb{R} \\to \\mathbb{R} and W \\in \\mathbb{R}. Then\n\n\\dfrac{\\partial R}{\\partial W} = \\dfrac{\\partial R}{\\partial L} \\dfrac{\\partial L}{\\partial W}\n\n\n\nThe simplest example:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), x_2(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\dfrac{\\partial f}{\\partial x_2} \\dfrac{\\partial x_2}{\\partial t}\n\nNow, we’ll consider f: \\mathbb{R}^n \\to \\mathbb{R}:\n\n\\dfrac{\\partial }{\\partial t} f(x_1(t), \\ldots, x_n(t)) = \\dfrac{\\partial f}{\\partial x_1} \\dfrac{\\partial x_1}{\\partial t} + \\ldots + \\dfrac{\\partial f}{\\partial x_n} \\dfrac{\\partial x_n}{\\partial t}\n\nBut if we will add another dimension f: \\mathbb{R}^n \\to \\mathbb{R}^m, than the j-th output of f will be:\n\n\\dfrac{\\partial }{\\partial t} f_j(x_1(t), \\ldots, x_n(t)) = \\sum\\limits_{i=1}^n \\dfrac{\\partial f_j}{\\partial x_i} \\dfrac{\\partial x_i}{\\partial t} = \\sum\\limits_{i=1}^n J_{ji}  \\dfrac{\\partial x_i}{\\partial t},\n\nwhere matrix J \\in \\mathbb{R}^{m \\times n} is the jacobian of the f. Hence, we could write it in a vector way:\n\n\\dfrac{\\partial f}{\\partial t} = J \\dfrac{\\partial x}{\\partial t}\\quad \\iff \\quad \\left(\\dfrac{\\partial f}{\\partial t}\\right)^\\top =  \\left( \\dfrac{\\partial x}{\\partial t}\\right)^\\top J^\\top\n\n\n\n\nThe whole idea came from the applying chain rule to the computation graph of primitive operations\n\nL = L\\left(y\\left(z(w,x,b)\\right), t\\right)\n\n\n\n\\begin{aligned}\n&z = wx+b   &\\frac{\\partial z}{\\partial w} =x, \\frac{\\partial z}{\\partial x} =w, \\frac{\\partial z}{\\partial b} =0  \\\\\n&y = \\sigma(z) &\\frac{\\partial y}{\\partial z} =\\sigma'(z)\\\\\n&L = \\dfrac{1}{2}(y-t)^2 &\\frac{\\partial L}{\\partial y} =y-t, \\frac{\\partial L}{\\partial t} = t -y\n\\end{aligned}\n\nAll frameworks for automatic differentiation construct (implicitly or explicitly) computation graph. In deep learning we typically want to compute the derivatives of the loss function L w.r.t. each intermediate parameters in order to tune them via gradient descent. For this purpose it is convenient to use the following notation:\n\n\\overline{v_i} = \\dfrac{\\partial L}{\\partial v_i}\n\nLet v_1, . . . , v_N be a topological ordering of the computation graph (i.e. parents come before children). v_N denotes the variable we’re trying to compute derivatives of (e.g. loss).\n\n\n\nFor i = 1, \\ldots, N:\n\nCompute v_i as a function of its parents.\n\n\n\n\n\n\n$\\overline{v_N} = 1$\nFor i = N-1, \\ldots, 1:\n\nCompute derivatives \\overline{v_i} = \\sum\\limits_{j \\in \\text{Children}(v_i)}\\overline{v_j}\\dfrac{\\partial v_j}{\\partial v_i}\n\n\nNote, that \\overline{v_j} term is coming from the children of \\overline{v_i}, while \\dfrac{\\partial v_j}{\\partial v_i} is already precomputed effectively.\n\n\n\n\n\nThe reason why it works so fast in practice is that the Jacobian of the operations are already developed in effective manner in automatic differentiation frameworks. Typically, we even do not construct or store the full Jacobian, doing matvec directly instead.\n\n\n\ny = \\exp{(z)} \\qquad J = \\text{diag}(\\exp(z)) \\qquad \\overline{z} = \\overline{y} J\n\nSee the examples of Vector-Jacobian Products from autodidact library:\ndefvjp(anp.add,         lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, g))\ndefvjp(anp.multiply,    lambda g, ans, x, y : unbroadcast(x, y * g),\n                        lambda g, ans, x, y : unbroadcast(y, x * g))\ndefvjp(anp.subtract,    lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, -g))\ndefvjp(anp.divide,      lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\ndefvjp(anp.true_divide, lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\n\n\n\n\nInteresting, that the similar idea could be used to compute Hessian-vector products, which is essential for second order optimization or conjugate gradient methods. For a scalar-valued function f : \\mathbb{R}^n \\to \\mathbb{R} with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point x \\in \\mathbb{R}^n is written as \\partial^2 f(x). A Hessian-vector product function is then able to evaluate\n\nv \\mapsto \\partial^2 f(x) \\cdot v\n\nfor any vector v \\in \\mathbb{R}^n.\nThe trick is not to instantiate the full Hessian matrix: if n is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store. Luckily, grad (in the jax/autograd/pytorch/tensorflow) already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity\n\n\\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x),\n\nwhere g(x) = \\partial f(x) \\cdot v is a new vector-valued function that dots the gradient of f at x with the vector v. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know grad is efficient.\nimport jax.numpy as jnp\n\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
  },
  {
    "objectID": "docs/methods/Autograd.html#backpropagation",
    "href": "docs/methods/Autograd.html#backpropagation",
    "title": "1 Idea",
    "section": "",
    "text": "The whole idea came from the applying chain rule to the computation graph of primitive operations\n\nL = L\\left(y\\left(z(w,x,b)\\right), t\\right)\n\n\n\n\\begin{aligned}\n&z = wx+b   &\\frac{\\partial z}{\\partial w} =x, \\frac{\\partial z}{\\partial x} =w, \\frac{\\partial z}{\\partial b} =0  \\\\\n&y = \\sigma(z) &\\frac{\\partial y}{\\partial z} =\\sigma'(z)\\\\\n&L = \\dfrac{1}{2}(y-t)^2 &\\frac{\\partial L}{\\partial y} =y-t, \\frac{\\partial L}{\\partial t} = t -y\n\\end{aligned}\n\nAll frameworks for automatic differentiation construct (implicitly or explicitly) computation graph. In deep learning we typically want to compute the derivatives of the loss function L w.r.t. each intermediate parameters in order to tune them via gradient descent. For this purpose it is convenient to use the following notation:\n\n\\overline{v_i} = \\dfrac{\\partial L}{\\partial v_i}\n\nLet v_1, . . . , v_N be a topological ordering of the computation graph (i.e. parents come before children). v_N denotes the variable we’re trying to compute derivatives of (e.g. loss).\n\n\n\nFor i = 1, \\ldots, N:\n\nCompute v_i as a function of its parents.\n\n\n\n\n\n\n$\\overline{v_N} = 1$\nFor i = N-1, \\ldots, 1:\n\nCompute derivatives \\overline{v_i} = \\sum\\limits_{j \\in \\text{Children}(v_i)}\\overline{v_j}\\dfrac{\\partial v_j}{\\partial v_i}\n\n\nNote, that \\overline{v_j} term is coming from the children of \\overline{v_i}, while \\dfrac{\\partial v_j}{\\partial v_i} is already precomputed effectively."
  },
  {
    "objectID": "docs/methods/Autograd.html#jacobian-vector-product",
    "href": "docs/methods/Autograd.html#jacobian-vector-product",
    "title": "1 Idea",
    "section": "",
    "text": "The reason why it works so fast in practice is that the Jacobian of the operations are already developed in effective manner in automatic differentiation frameworks. Typically, we even do not construct or store the full Jacobian, doing matvec directly instead.\n\n\n\ny = \\exp{(z)} \\qquad J = \\text{diag}(\\exp(z)) \\qquad \\overline{z} = \\overline{y} J\n\nSee the examples of Vector-Jacobian Products from autodidact library:\ndefvjp(anp.add,         lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, g))\ndefvjp(anp.multiply,    lambda g, ans, x, y : unbroadcast(x, y * g),\n                        lambda g, ans, x, y : unbroadcast(y, x * g))\ndefvjp(anp.subtract,    lambda g, ans, x, y : unbroadcast(x, g),\n                        lambda g, ans, x, y : unbroadcast(y, -g))\ndefvjp(anp.divide,      lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))\ndefvjp(anp.true_divide, lambda g, ans, x, y : unbroadcast(x,   g / y),\n                        lambda g, ans, x, y : unbroadcast(y, - g * x / y**2))"
  },
  {
    "objectID": "docs/methods/Autograd.html#hessian-vector-product",
    "href": "docs/methods/Autograd.html#hessian-vector-product",
    "title": "1 Idea",
    "section": "",
    "text": "Interesting, that the similar idea could be used to compute Hessian-vector products, which is essential for second order optimization or conjugate gradient methods. For a scalar-valued function f : \\mathbb{R}^n \\to \\mathbb{R} with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point x \\in \\mathbb{R}^n is written as \\partial^2 f(x). A Hessian-vector product function is then able to evaluate\n\nv \\mapsto \\partial^2 f(x) \\cdot v\n\nfor any vector v \\in \\mathbb{R}^n.\nThe trick is not to instantiate the full Hessian matrix: if n is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store. Luckily, grad (in the jax/autograd/pytorch/tensorflow) already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity\n\n\\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x),\n\nwhere g(x) = \\partial f(x) \\cdot v is a new vector-valued function that dots the gradient of f at x with the vector v. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know grad is efficient.\nimport jax.numpy as jnp\n\ndef hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html",
    "href": "docs/methods/zom/bee_algorithm.html",
    "title": "1 Algorithm",
    "section": "",
    "text": "The Bee Algorithm was mathematically first described relatively recently. It is one of the representatives of a large family of algorithms that allow modeling swarm intelligence. This article will provide an example of the application of the Bee Algorithm to search for the global extremum of the function. The two-dimensional Schwefel function, having a large number of local minima, and the Rosenbrock function, whose global minimum lies in a narrow, parabolic valley, were chosen as the target functions.\n\n\nA colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here.\n\n\n\nAnd now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While\n\n\n\n\nTwo standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube  x_i \\in [-500, 500]  for all  i = 1, ..., d .\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube  x_i \\in [-5, 10]  for all  i = 1, ..., d , although it may be restricted to the hypercube  x_i \\in [-2.048, 2.048]  for all  i = 1, ..., d .\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension"
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#intuition",
    "href": "docs/methods/zom/bee_algorithm.html#intuition",
    "title": "1 Algorithm",
    "section": "",
    "text": "A colony of honey bees can spread over long distances (more than 10 km) and in several directions, while using a huge number of food sources. A colony can only be successful by deploying its foragers to good fields. The main idea is that flower fields that provide large amounts of nectar or pollen that are easy to collect with less energy consumption should be visited by more bees, whereas areas with less nectar or pollen should receive fewer bees.\nThe search for food begins with the sending of scout bees in search of honey flower fields. Scout bees search randomly through their journey from one patch to another. Also throughout the harvest season, the colony continues its research, keeping a percentage of the entire population as bee scouts.\nWhen the bees return to the hive, those who found a source which is above a certain threshold (a combination of some constituents, such as sugar percentage regarding the source) deposit their nectar or pollen and go to the dance floor to perform their waggle dance. This mysterious dance is essential for colony communication and contains three vital pieces of information about flower spots: direction, distance, and source quality.\nThe nectar search process is described in more detail here."
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#mathematical-interpretation",
    "href": "docs/methods/zom/bee_algorithm.html#mathematical-interpretation",
    "title": "1 Algorithm",
    "section": "",
    "text": "And now imagine that the location of the global extremum is the site where the most nectar, and this site is the only one, that is, in other places there is nectar, but less. And bees do not live on a plane, where it is enough to know two coordinates to determine the location of sites, but in a multidimensional space, where each coordinate represents one parameter of a function that needs to be optimized. The amount of nectar found is the value of the target function at this point.\nThe list below shows the pseudocode for a simple Bee Algorithm.\n\nInitialize the set of parameters: number of scout bees - n, number of elite bees - e, number of selected regions out of n points - m, number of recruited around elite regions - nep, number of recruited around other selected (m-e) regions - nsp, and stopping criteria.\nEvery bee evaluates the value of target function\nWhile (stopping criteria not met): //Forming new population\n\nElite bees (e) that have better fitness are selected and saved for the next population\nSelect sites for neighbourhood search (m-e)\nRecruit bees around selected sites and evaluate fitness. More bees will be recruited around elite points(nep) and fewer bees will be recruited around the remaining selected points(nsp).\nSelect the bee with the highest fitness from each site.\nAssign remaining bees (n-m-e) to search randomly and evaluate their fitness.\n\nEnd While"
  },
  {
    "objectID": "docs/methods/zom/bee_algorithm.html#examples",
    "href": "docs/methods/zom/bee_algorithm.html#examples",
    "title": "1 Algorithm",
    "section": "",
    "text": "Two standard functions problems were selected to test the bee algorithm. Code implementation of The Bee Algorithm in Python is described here\nThe following parameters were set for this test:\n\npopulation n = 300\nnumber of elite bees e = 5\nselected sites m = 15\nbees round elite points nep = 30\nbees around selected points nsp = 10\nstopping criteria: max_iteration = 2000\n\nA random point is selected from the definition area to initialize the algorithm.\n\n\nThe Schwefel function is complex, with many local minima. The plot shows the two-dimensional form of the function.\n\n\n\nschwef\n\n\nThe function is usually evaluated on the hypercube  x_i \\in [-500, 500]  for all  i = 1, ..., d .\n\nf(x_1 \\cdots x_d) = 418.9829 \\cdot d -\\sum_{i=1}^d (x_i sin(\\sqrt{|x_i|}))\n\nThe function has one global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 420.9687\n\nThe plot below shows drop of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nSchwefel function\n\n\n\n\n\nThe Rosenbrock function, also referred to as the Valley or Banana function, is a popular test problem for gradient-based optimization algorithms. It is shown in the plot below in its two-dimensional form.\n\n\n\nrosen\n\n\nThe function is unimodal, and the global minimum lies in a narrow, parabolic valley. However, even though this valley is easy to find, convergence to the minimum is difficult.\nThe function is usually evaluated on the hypercube  x_i \\in [-5, 10]  for all  i = 1, ..., d , although it may be restricted to the hypercube  x_i \\in [-2.048, 2.048]  for all  i = 1, ..., d .\n\nf(x_1 \\cdots x_d) = \\sum_{i=1}^{d-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2) \\\\-2.048 \\leq x_i \\leq 2.048\n\nThe function global minimum:\n\nf(x_1 \\cdots x_d) = 0, \\quad x_i = 1\n\nTo test the algorithm the four-dimensional Rosenbrock function was chosen. The fall of the objective function averaged over 100 runs of the algorithm can be observed in the following graph.\n\n\n\nRosenbrock function\n\n\nMoreover, the number of iterations required to find the point at which the value of the target function differs from the optimal one by no more than 0.2%(new stopping criteria) has also been tested for some dimensions of the Rosenbrock function. One can see the results at the chart below.\n\n\n\nRosenbrock_dimension"
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html",
    "href": "docs/methods/zom/nelder-mead.html",
    "title": "1 Problem",
    "section": "",
    "text": "Sometimes the multidimensional function is so difficult to evaluate that even expressing the 1^{\\text{st}} derivative for gradient-based methods of finding optimum becomes an impossible task. In this case, we can only rely on the values of the function at each point. Or, in other words, on the 0 order oracle calls.\nLet’s take, for instance, Mishra’s Bird function:\n\nf(x,y) = \\sin{y} \\cdot e^{\\left( 1 - \\cos{x} \\right)^2} + \\cos{x} \\cdot e^{\\left( 1 - \\sin{y} \\right)^2} + (x - y)^2\n\n\nThis function is usually subjected to the domain (x+5)^2 + (y+5)^2 &lt; 25, but for the sake of picture beauty we will mainly use domain [-10; 0] \\times [-10; 0]."
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#related-definitions",
    "href": "docs/methods/zom/nelder-mead.html#related-definitions",
    "title": "1 Problem",
    "section": "2.1 Related definitions:",
    "text": "2.1 Related definitions:\n\n\\textbf{Simplex} – polytope with the least possible number of vertices in n-dimensional space. (So, it’s (n+1)-polytope.) In our 2D case it will be triangle.\n\\textbf{Best point }x_1 – vertex of the simplex, function value in which is the smallest among all vertices.\n\\textbf{Worst point }x_{n+1} – vertex of the simplex, function value in which is the largest among all vertices.\n\\textbf{Other points }x_2, \\ldots, x_n – vertices of the simplex, ordered in such way that f(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1}). This implies that \\{ x_1, x_2, \\ldots, x_n \\} are best points in relation to x_{n+1} and \\{ x_2, \\ldots, x_n, x_{n+1} \\} are worst points in relation to x_1.\n\\textbf{Centroid }x_o – center of mass in the polytope. In Nelder-Mead the centroid is calculated for the polytope, constituted by best vertices. In our 2D case it will be the center of the triangle side, which contains 2 best points x_o = \\dfrac{x_1 + x_2}{2}."
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#main-idea",
    "href": "docs/methods/zom/nelder-mead.html#main-idea",
    "title": "1 Problem",
    "section": "2.2 Main idea",
    "text": "2.2 Main idea\nThe algorithm maintains the set of test points in the form of simplex. For each point the function value is calculated and points are ordered accordingly. Depending on those values, the simplex exchanges the worst point of the set for the new one, which is closer to the local minimum. In some sense, the simplex is crawling to the minimal value in the domain.\nThe simplex movements finish when its sides become too small (termination condition by sides) or its area becomes too small (termination condition by area). I prefer the second condition, because it takes into account cases when simplex becomes degenerate (three or more vertices on one axis)."
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#steps-of-the-algorithm",
    "href": "docs/methods/zom/nelder-mead.html#steps-of-the-algorithm",
    "title": "1 Problem",
    "section": "2.3 Steps of the algorithm",
    "text": "2.3 Steps of the algorithm\n1. Ordering\nOrder vertices according to values in them:\n\nf(x_1) \\leqslant f(x_2) \\leqslant \\ldots \\leqslant f(x_n) \\leqslant f(x_{n+1})\n\nCheck the termination condition. Possible exit with solution x_{\\min} = x_1.\n2. Centroid calculation\n\nx_o = \\dfrac{\\sum\\limits_{k=1}^{n}{x_k}}{n}\n\n3. Reflection\nCalculate the reflected point x_r:\n\nx_r = x_o + \\alpha \\left( x_o - x_{n+1} \\right)\n\nwhere \\alpha – reflection coefficient, \\alpha &gt; 0. (If \\alpha \\leqslant 0, reflected point x_r will not overlap the centroid)\nThe next step is figured out according to the value of f(x_r) in dependency to values in points x_1 (best) and x_n (second worst): * f(x_r) &lt; f(x_1): Go to step 4. * f(x_1) \\leqslant f(x_r) &lt; f(x_n): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1. * f(x_r) \\geqslant f(x_n): Go to step 5.\n4. Expansion\nCalculate the expanded point x_e:\n\nx_e = x_o + \\gamma \\left( x_r - x_o \\right)\n\nwhere \\gamma – expansion coefficient, \\gamma &gt; 1. (If \\gamma &lt; 1, expanded point x_e will be contracted towards centroid, if \\gamma = 1: x_e = x_r)\nThe next step is figured out according to the ratio between f(x_e) and f(x_r): * f(x_e) &lt; f(x_r): new simplex with x_{n+1} \\rightarrow x_e. Go to step 1. * f(x_e) &gt; f(x_r): new simplex with x_{n+1} \\rightarrow x_r. Go to step 1.\n5. Contraction\nCalculate the contracted point x_c:\n\nx_c = x_o + \\beta \\left( x_{n+1} - x_o \\right)\n\nwhere \\beta – contraction coefficient, 0 &lt; \\beta \\leqslant 0.5. (If \\beta &gt; 0.5, contraction is insufficient, if \\beta \\leqslant 0, contracted point x_c overlaps the centroid)\nThe next step is figured out according to the ratio between f(x_c) and f(x_{n+1}): * f(x_c) &lt; f(x_{n+1}): new simplex with x_{n+1} \\rightarrow x_c. Go to step 1. * f(x_c) \\geqslant f(x_{n+1}): Go to step 6.\n6. Shrinkage\nReplace all points of simplex x_i with new ones, except for the best point x_1:\n\nx_i = x_1 + \\sigma \\left( x_i - x_1 \\right)\n\nwhere \\sigma – shrinkage coefficient, 0 &lt; \\sigma &lt; 1. (If \\sigma \\geqslant 1, shrinked point x_i overlaps the best point x_1, if \\sigma \\leqslant 0, shrinked point x_i becomes extended)\nGo to step 1."
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#some-random-initial-simplex-and-default-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#some-random-initial-simplex-and-default-set-of-parameters",
    "title": "1 Problem",
    "section": "3.1 Some random initial simplex and default set of parameters",
    "text": "3.1 Some random initial simplex and default set of parameters"
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#different-initial-simplex-and-same-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#different-initial-simplex-and-same-set-of-parameters",
    "title": "1 Problem",
    "section": "3.2 Different initial simplex and same set of parameters",
    "text": "3.2 Different initial simplex and same set of parameters"
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#same-initial-simplex-and-different-set-of-parameters",
    "href": "docs/methods/zom/nelder-mead.html#same-initial-simplex-and-different-set-of-parameters",
    "title": "1 Problem",
    "section": "3.3 Same initial simplex and different set of parameters",
    "text": "3.3 Same initial simplex and different set of parameters"
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#round-domain",
    "href": "docs/methods/zom/nelder-mead.html#round-domain",
    "title": "1 Problem",
    "section": "3.4 Round domain",
    "text": "3.4 Round domain"
  },
  {
    "objectID": "docs/methods/zom/nelder-mead.html#examples-with-all-sets-of-simplexes",
    "href": "docs/methods/zom/nelder-mead.html#examples-with-all-sets-of-simplexes",
    "title": "1 Problem",
    "section": "3.5 Examples with all sets of simplexes",
    "text": "3.5 Examples with all sets of simplexes"
  },
  {
    "objectID": "docs/materials/index.html",
    "href": "docs/materials/index.html",
    "title": "",
    "section": "",
    "text": "Universal gradient descent. Alexander Gasnikov - (in Russian) - probably, the most comprehensive book on the modern numerical methods, which covers a lot of theoretical and practical aspects of mathematical programming. {% include tabs.html bibtex = ‘@article{gasnikov2017universal, title={Universal gradient descent}, author={Gasnikov, Alexander}, journal={arXiv preprint arXiv:1711.00394}, year={2017} }’ inline = ‘True’ %}\nConvex Optimization: Algorithms and Complexity by Sébastien Bubeck. {% include tabs.html bibtex = ‘@article{bubeck2015convex, title={Convex optimization: Algorithms and complexity}, author={Bubeck, S{'e}bastien and others}, journal={Foundations and Trends{} in Machine Learning}, volume={8}, number={3-4}, pages={231–357}, year={2015}, publisher={Now Publishers, Inc.} }’ inline = ‘True’ %}\nConvex Optimization materials by Stephen Boyd and Lieven Vandenberghe. {% include tabs.html bibtex = ‘@book{boyd2004convex, title={Convex optimization}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}, publisher={Cambridge university press} }’ inline = ‘True’ %}\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright. {% include tabs.html bibtex = ‘@book{nocedal2006numerical, title={Numerical optimization}, author={Nocedal, Jorge and Wright, Stephen}, year={2006}, publisher={Springer Science & Business Media} }’ file=‘assets/files/NumericalOptimization.pdf’ inline = ‘True’ %}\nLectures on Convex Optimization by Yurii Nesterov {% include tabs.html bibtex = ‘@book{nesterov2018lectures, title={Lectures on convex optimization}, author={Nesterov, Yurii}, volume={137}, publisher={Springer} }’ file=‘assets/files/Nesterov_the_best.pdf’ inline = ‘True’ %}\nMinimume-volume ellipsoids {% include tabs.html bibtex = ‘@book{todd2016minimum, title={Minimum-volume ellipsoids: Theory and algorithms}, author={Todd, Michael J}, year={2016}, publisher={SIAM} }’ file=‘assets/files/MVE_book.pdf’ inline = ‘True’ %}\nМетоды оптимизации, Часть I. Введение в выпуклый анализ и теорию оптимизации {% include tabs.html bibtex = ‘@article{жадан2014методы, title={Методы оптимизации. Часть 1. Введение в выпуклый анализ и теорию оптимизации: учебное пособие}, author={Жадан, ВГ}, journal={М.: МФТИ}, year={2014} }’ file=‘assets/files/Zhadan_1.pdf’ inline = ‘True’ %}\nМетоды оптимизации, Часть II. Численные алгоритмы {% include tabs.html bibtex = ‘@article{жадан2015методы, title={Методы оптимизации. Часть 2. Численные алгоритмы: учебное пособие}, author={Жадан, ВГ}, journal={М.: МФТИ}, year={2015} }’ file=‘assets/files/Zhadan_2.pdf’ inline = ‘True’ %}"
  },
  {
    "objectID": "docs/materials/index.html#books",
    "href": "docs/materials/index.html#books",
    "title": "",
    "section": "",
    "text": "Universal gradient descent. Alexander Gasnikov - (in Russian) - probably, the most comprehensive book on the modern numerical methods, which covers a lot of theoretical and practical aspects of mathematical programming. {% include tabs.html bibtex = ‘@article{gasnikov2017universal, title={Universal gradient descent}, author={Gasnikov, Alexander}, journal={arXiv preprint arXiv:1711.00394}, year={2017} }’ inline = ‘True’ %}\nConvex Optimization: Algorithms and Complexity by Sébastien Bubeck. {% include tabs.html bibtex = ‘@article{bubeck2015convex, title={Convex optimization: Algorithms and complexity}, author={Bubeck, S{'e}bastien and others}, journal={Foundations and Trends{} in Machine Learning}, volume={8}, number={3-4}, pages={231–357}, year={2015}, publisher={Now Publishers, Inc.} }’ inline = ‘True’ %}\nConvex Optimization materials by Stephen Boyd and Lieven Vandenberghe. {% include tabs.html bibtex = ‘@book{boyd2004convex, title={Convex optimization}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}, publisher={Cambridge university press} }’ inline = ‘True’ %}\nNumerical Optimization by Jorge Nocedal and Stephen J. Wright. {% include tabs.html bibtex = ‘@book{nocedal2006numerical, title={Numerical optimization}, author={Nocedal, Jorge and Wright, Stephen}, year={2006}, publisher={Springer Science & Business Media} }’ file=‘assets/files/NumericalOptimization.pdf’ inline = ‘True’ %}\nLectures on Convex Optimization by Yurii Nesterov {% include tabs.html bibtex = ‘@book{nesterov2018lectures, title={Lectures on convex optimization}, author={Nesterov, Yurii}, volume={137}, publisher={Springer} }’ file=‘assets/files/Nesterov_the_best.pdf’ inline = ‘True’ %}\nMinimume-volume ellipsoids {% include tabs.html bibtex = ‘@book{todd2016minimum, title={Minimum-volume ellipsoids: Theory and algorithms}, author={Todd, Michael J}, year={2016}, publisher={SIAM} }’ file=‘assets/files/MVE_book.pdf’ inline = ‘True’ %}\nМетоды оптимизации, Часть I. Введение в выпуклый анализ и теорию оптимизации {% include tabs.html bibtex = ‘@article{жадан2014методы, title={Методы оптимизации. Часть 1. Введение в выпуклый анализ и теорию оптимизации: учебное пособие}, author={Жадан, ВГ}, journal={М.: МФТИ}, year={2014} }’ file=‘assets/files/Zhadan_1.pdf’ inline = ‘True’ %}\nМетоды оптимизации, Часть II. Численные алгоритмы {% include tabs.html bibtex = ‘@article{жадан2015методы, title={Методы оптимизации. Часть 2. Численные алгоритмы: учебное пособие}, author={Жадан, ВГ}, journal={М.: МФТИ}, year={2015} }’ file=‘assets/files/Zhadan_2.pdf’ inline = ‘True’ %}"
  },
  {
    "objectID": "docs/materials/index.html#courses",
    "href": "docs/materials/index.html#courses",
    "title": "",
    "section": "2 Courses",
    "text": "2 Courses\n\nConvex Optimization and Approximation course by Moritz Hardt @ UC Berkley.\nConvex Optimization course by Ryan Tibshirani @ CMU.\nConvex Optimization course by Lieven Vandenberghe @ UCLA.\nConvex Optimization course by Suvrit Sra @ UC Berkley.\nAdvanced Optimization and Randomized Methods course by Alex Smola and Suvrit Sra @ CMU.\nOptimizaion methods course by Alexandr Katrutsa @ MIPT.\nConvex Analysis and Optimization course by Dimitri Bertsekas @ MIT.\nOptimization for Machine Learning course by Martin Jaggi @ EPFL.\nOptimization for Machine Learning course by Suvrit Sra.\nМетоды оптимизации lectures by Alexander Gasnikov @ MIPT.\nМетоды оптимизации seminars by Daniil Merkulov @ MIPT."
  },
  {
    "objectID": "docs/materials/index.html#blogs-and-personal-pages",
    "href": "docs/materials/index.html#blogs-and-personal-pages",
    "title": "",
    "section": "3 Blogs and personal pages",
    "text": "3 Blogs and personal pages\n\nI’m a bandit blog by Sébastien Bubeck.\nBlog by Moritz Hardt.\nBlog by Sebastian Pokutta with great cheat sheets on optimization.\nBlog by Sebastian Ruder about NLP and optimization.\nPersonal page of Peter Richtarik with announcements and news.\nPersonal page of Suvrit Sra.\nBlog by Fabian Pedregosa.\nBlog with beatiful insights about modern non-convex optimization.\nMachine Learning Research Blog by Francis Bach."
  },
  {
    "objectID": "docs/materials/index.html#software-and-apps",
    "href": "docs/materials/index.html#software-and-apps",
    "title": "",
    "section": "4 Software and apps",
    "text": "4 Software and apps\n\nSci hub telegram bot allows you to access almost all the scientific papers in one click."
  },
  {
    "objectID": "docs/materials/index.html#other",
    "href": "docs/materials/index.html#other",
    "title": "",
    "section": "5 Other",
    "text": "5 Other\n\nNice set of python applied math etudes\nExample functions to test optimization algorithms.\n100 numpy exercises\nCollection of Interactive Machine Learning Examples\nML Python libraries overview (Russ)\nNice Visualisation of some ML ideas\nAn Interactive Tutorial on Numerical Optimization"
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html",
    "href": "docs/materials/tutorials/Colab tutorial.html",
    "title": "1 Github features",
    "section": "",
    "text": "This tutorial will highlight some\nThis tool perfectly fits 90% of your tasks. * Fast prototyping * Sharing and simultaneous work"
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#running-any-jupyter-notebook-from-github",
    "href": "docs/materials/tutorials/Colab tutorial.html#running-any-jupyter-notebook-from-github",
    "title": "1 Github features",
    "section": "1.1 Running any jupyter notebook from github",
    "text": "1.1 Running any jupyter notebook from github\nIf you want to open any notebook, that is already stored in any public github repository, it is enough to paste user/repo name in the field below and just open it. Modified notebook could be downloaded locally or saved to your google drive storage."
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#commiting-to-github-from-colab",
    "href": "docs/materials/tutorials/Colab tutorial.html#commiting-to-github-from-colab",
    "title": "1 Github features",
    "section": "1.2 Commiting to github from colab",
    "text": "1.2 Commiting to github from colab\nHowever, you can save any changes directly to the github through the commits. In order to do this, you’ll need to authorize colab to work with your github account. It’s up to you to provide this access or not. All these things work even with private repos."
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#dark-theme",
    "href": "docs/materials/tutorials/Colab tutorial.html#dark-theme",
    "title": "1 Github features",
    "section": "3.1 Dark theme",
    "text": "3.1 Dark theme\nTools -&gt; Settings -&gt; Site -&gt; Theme -&gt; Dark"
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#monokai",
    "href": "docs/materials/tutorials/Colab tutorial.html#monokai",
    "title": "1 Github features",
    "section": "3.2 Monokai",
    "text": "3.2 Monokai\nTools -&gt; Settings -&gt; Site -&gt; Editor -&gt; Editor colorization"
  },
  {
    "objectID": "docs/materials/tutorials/Colab tutorial.html#bonus",
    "href": "docs/materials/tutorials/Colab tutorial.html#bonus",
    "title": "1 Github features",
    "section": "3.3 Bonus",
    "text": "3.3 Bonus\nTools -&gt; Settings -&gt; Miscellaneous -&gt; Kitty mode"
  },
  {
    "objectID": "docs/methods/zom/zom.html",
    "href": "docs/methods/zom/zom.html",
    "title": "1 Code",
    "section": "",
    "text": "Now we have only zero order information from the oracle. Typical speed of convegence of these methods is sublinear. A lot of methods are referred both to zero order methods and global optimization.\n\n1 Code\n\nGlobal optimization illustration - Open In Colab{: .btn }\nNevergrad library - Open In Colab{: .btn }\nOptuna quickstart Open In Colab{: .btn }"
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html",
    "href": "docs/methods/zom/simulated-annealing.html",
    "title": "1 Problem",
    "section": "",
    "text": "We need to optimize the global optimum of a given function on some space using only the values of the function in some points on the space.\n\n\\min_{x \\in X} F(x) = F(x^*)\n\nSimulated Annealing is a probabilistic technique for approximating the global optimum of a given function."
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#steps-of-the-algorithm",
    "href": "docs/methods/zom/simulated-annealing.html#steps-of-the-algorithm",
    "title": "1 Problem",
    "section": "2.1 Steps of the Algorithm",
    "text": "2.1 Steps of the Algorithm\nStep 1 Let  k = 0  - current iteration, T = T_k - initial temperature.\nStep 2 Let x_k \\in X - some random point from our space\nStep 3 Let decrease the temperature by following rule T_{k+1} = \\alpha T_k where  0 &lt; \\alpha &lt; 1 - some constant that often is closer to 1\nStep 4 Let x_{k+1} = g(x_k) - the next point which was obtained from previous one by some random rule. It is usually assumed that this rule works so that each subsequent approximation should not differ very much.\nStep 5 Calculate \\Delta E = E(x_{k+1}) - E(x_{k}), where E(x) - the function that determines the energy of the system at this point. It is supposed that energy has the minimum in desired value x^*.\nStep 6 If \\Delta E &lt; 0 then the approximation found is better than it was. So accept x_{k+1} as new started point at the next step and go to the step Step 3\nStep 7 If \\Delta E &gt;= 0, then we accept x_{k+1} with the probability of P(\\Delta E) = \\exp^{-\\Delta E / T_k}. If we don’t accept x_{k+1}, then we let k = k+ 1. Go to the step Step 3\nThe algorithm can stop working according to various criteria, for example, achieving an optimal state or lowering the temperature below a predetermined level T_{min}."
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#convergence",
    "href": "docs/methods/zom/simulated-annealing.html#convergence",
    "title": "1 Problem",
    "section": "2.2 Convergence",
    "text": "2.2 Convergence\nAs it mentioned in Simulated annealing: a proof of convergence the algorithm converges almost surely to a global maximum."
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#illustration",
    "href": "docs/methods/zom/simulated-annealing.html#illustration",
    "title": "1 Problem",
    "section": "2.3 Illustration",
    "text": "2.3 Illustration\nA gif from Wikipedia:"
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#the-problem",
    "href": "docs/methods/zom/simulated-annealing.html#the-problem",
    "title": "1 Problem",
    "section": "3.1 The Problem",
    "text": "3.1 The Problem\nLet E(x) - the number of intersections, where x - the array of placement queens at the field (the number in array means the column, the index of the number means the row).\nThe problem is to find x^* where E(x^*) =  \\min_{x \\in X} E(x) - the global minimum, that is predefined and equals to 0 (no two queens threaten each other).\nIn this code x_0 = [0,1,2,...,N] that means all queens are placed at the board’s diagonal . So at the beginning E = N(N-1), because every queen intersects others."
  },
  {
    "objectID": "docs/methods/zom/simulated-annealing.html#results",
    "href": "docs/methods/zom/simulated-annealing.html#results",
    "title": "1 Problem",
    "section": "3.2 Results",
    "text": "3.2 Results\nResults of applying this algorithm with \\alpha = 0.95 to the N queens puzzle for N = 10 averaged by 100 runs are below:\n\nResults of running the code for N from 4 to 40 and measuring the time it takes to find the solution averaged by 100 runs are below:"
  },
  {
    "objectID": "docs/methods/Simplex.html",
    "href": "docs/methods/Simplex.html",
    "title": "1 What is LP",
    "section": "",
    "text": "Generally speaking, all problems with linear objective and linear equalitiesconstraints could be considered as Linear Programming. However, there are some widely accepted formulations.\n\n\\tag{LP.Basic}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n\\end{align*}\n\n\nfor some vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}. Where the inequalities are interpreted component-wise.\n\n\nThis form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\n\n\\tag{LP.Canonical}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\n\n\n\n\n\nImagine, that you have to construct a diet plan from some set of products: 🍌🍰🍗🥚🐟. Each of the products has its own vector of nutrients. Thus, all the food information could be processed through the matrix W. Let also assume, that we have the vector of requirements for each of nutrients r \\in \\mathbb{R}^n. We need to find the cheapest configuration of the diet, which meets all the requirements:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^p} c^{\\top}x \\\\\n\\text{s.t. } & Wx \\geq r\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#standard-form",
    "href": "docs/methods/Simplex.html#standard-form",
    "title": "1 What is LP",
    "section": "",
    "text": "This form seems to be the most intuitive and geometric in terms of visualization. Let us have vectors c \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m and matrix A \\in \\mathbb{R}^{m \\times n}.\n\n\\tag{LP.Standard}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#canonical-form",
    "href": "docs/methods/Simplex.html#canonical-form",
    "title": "1 What is LP",
    "section": "",
    "text": "\\tag{LP.Canonical}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax \\leq b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#real-world-problems",
    "href": "docs/methods/Simplex.html#real-world-problems",
    "title": "1 What is LP",
    "section": "",
    "text": "Imagine, that you have to construct a diet plan from some set of products: 🍌🍰🍗🥚🐟. Each of the products has its own vector of nutrients. Thus, all the food information could be processed through the matrix W. Let also assume, that we have the vector of requirements for each of nutrients r \\in \\mathbb{R}^n. We need to find the cheapest configuration of the diet, which meets all the requirements:\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^p} c^{\\top}x \\\\\n\\text{s.t. } & Wx \\geq r\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#basic-transformations",
    "href": "docs/methods/Simplex.html#basic-transformations",
    "title": "1 What is LP",
    "section": "2.1 Basic transformations",
    "text": "2.1 Basic transformations\nInequality to equality by increasing the dimension of the problem by m.\n\nAx \\leq b \\leftrightarrow\n\\begin{cases}\nAx + z =  b\\\\\nz \\geq 0\n\\end{cases}\n\nunsigned variables to nonnegative variables.\n\nx \\leftrightarrow\n\\begin{cases}\nx = x_+ - x_-\\\\\nx_+ \\geq 0 \\\\\nx_- \\geq 0\n\\end{cases}"
  },
  {
    "objectID": "docs/methods/Simplex.html#chebyshev-approximation-problem",
    "href": "docs/methods/Simplex.html#chebyshev-approximation-problem",
    "title": "1 What is LP",
    "section": "2.2 Chebyshev approximation problem",
    "text": "2.2 Chebyshev approximation problem\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_\\infty \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\max_{i} |a_i^\\top x - b_i|\n\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}, x \\in \\mathbb{R}^n} t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#l_1-approximation-problem",
    "href": "docs/methods/Simplex.html#l_1-approximation-problem",
    "title": "1 What is LP",
    "section": "2.3 l_1 approximation problem",
    "text": "2.3 l_1 approximation problem\n\n\\min_{x \\in \\mathbb{R}^n} \\|Ax - b\\|_1 \\leftrightarrow \\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^n |a_i^\\top x - b_i|\n\n\n\\begin{align*}\n&\\min_{t \\in \\mathbb{R}^n, x \\in \\mathbb{R}^n} \\mathbf{1}^\\top t \\\\\n\\text{s.t. } & a_i^\\top x - b_i \\leq t_i, \\; i = 1,\\dots, n\\\\\n& -a_i^\\top x + b_i \\leq t_i, \\; i = 1,\\dots, n\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#main-lemma",
    "href": "docs/methods/Simplex.html#main-lemma",
    "title": "1 What is LP",
    "section": "3.1 Main lemma",
    "text": "3.1 Main lemma\nIf all components of \\lambda_B are non-positive and B is feasible, then B is optimal.\nProof:\n\n\\begin{align*}\n\\exists x^*: Ax^* &\\leq b, c^\\top x^* &lt; c^\\top x_B \\\\\nA_B x^* &\\leq b_B \\\\\n\\lambda_B^\\top A_B x^* &\\geq \\lambda_B^\\top b_B \\\\\nc^\\top x^* & \\geq \\lambda_B^\\top A_B x_B \\\\\nc^\\top x^* & \\geq c^\\top  x_B \\\\\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#changing-basis",
    "href": "docs/methods/Simplex.html#changing-basis",
    "title": "1 What is LP",
    "section": "3.2 Changing basis",
    "text": "3.2 Changing basis\nSuppose, some of the coefficients of \\lambda_B are positive. Then we need to go through the edge of the polytope to the new vertex (i.e., switch the basis)\n\n\nx_{B'} = x_B + \\mu d = A^{-1}_{B'} b_{B'}"
  },
  {
    "objectID": "docs/methods/Simplex.html#finding-an-initial-basic-feasible-solution",
    "href": "docs/methods/Simplex.html#finding-an-initial-basic-feasible-solution",
    "title": "1 What is LP",
    "section": "3.3 Finding an initial basic feasible solution",
    "text": "3.3 Finding an initial basic feasible solution\nLet us consider \\text{LP.Canonical}.\n\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n} c^{\\top}x \\\\\n\\text{s.t. } & Ax = b\\\\\n& x_i \\geq 0, \\; i = 1,\\dots, n\n\\end{align*}\n\nThe proposed algorithm requires an initial basic feasible solution and corresponding basis. To compute this solution and basis, we start by multiplying by −1 any row i of Ax = b such that b_i &lt; 0. This ensures that b \\geq 0. We then introduce artificial variables z \\in \\mathbb{R}^m and consider the following LP:\n\n\\tag{LP.Phase 1}\n\\begin{align*}\n&\\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}^m} 1^{\\top}z \\\\\n\\text{s.t. } & Ax + Iz = b\\\\\n& x_i, z_j \\geq 0, \\; i = 1,\\dots, n \\; j = 1,\\dots, m\n\\end{align*}\n\nwhich can be written in canonical form \\min\\{\\tilde{c}^\\top \\tilde{x} \\mid \\tilde{A}\\tilde{x} = \\tilde{b}, \\tilde{x} \\geq 0\\} by setting\n\n\\tilde{x} = \\begin{bmatrix}x\\\\z\\end{bmatrix}, \\quad \\tilde{A} = [A \\; I], \\quad \\tilde{b} = b, \\quad \\tilde{c} = \\begin{bmatrix}0_n\\\\1_m\\end{bmatrix}\n\nAn initial basis for \\text{LP.Phase 1} is \\tilde{A}_B = I, \\tilde{A}_N = A with corresponding basic feasible solution \\tilde{x}_N = 0, \\tilde{x}_B = \\tilde{A}^{-1}_B \\tilde{b} = \\tilde{b} \\geq 0. We can therefore run the simplex method on \\text{LP.Phase 1}, which will converge to an optimum \\tilde{x}^*. \\tilde{x} = (\\tilde{x}_N \\; \\tilde{x}_B). There are several possible outcomes:\n\n\\tilde{c}^\\top \\tilde{x} &gt; 0. Original primal is infeasible.\n\\tilde{c}^\\top \\tilde{x} = 0 \\to 1^\\top z^* = 0. The obtained solution is a start point for the original problem (probably with slight modification)."
  },
  {
    "objectID": "docs/methods/Simplex.html#klee-minty-example",
    "href": "docs/methods/Simplex.html#klee-minty-example",
    "title": "1 What is LP",
    "section": "4.1 Klee Minty example",
    "text": "4.1 Klee Minty example\nIn the following problem simplex algorithm needs to check 2^n - 1 vertexes with x_0 = 0.\n\n\\begin{align*} & \\max_{x \\in \\mathbb{R}^n} 2^{n-1}x_1 + 2^{n-2}x_2 + \\dots + 2x_{n-1} + x_n\\\\\n\\text{s.t. } & x_1 \\leq 5\\\\\n& 4x_1 + x_2 \\leq 25\\\\\n& 8x_1 + 4x_2 + x_3 \\leq 125\\\\\n& \\ldots\\\\\n& 2^n x_1 + 2^{n-1}x_2 + 2^{n-2}x_3 + \\ldots + x_n \\leq 5^n\\ & x \\geq 0\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/Simplex.html#strong-duality",
    "href": "docs/methods/Simplex.html#strong-duality",
    "title": "1 What is LP",
    "section": "4.2 Strong duality",
    "text": "4.2 Strong duality\nThere are four possibilities: * Both the primal and the dual are infeasible. * The primal is infeasible and the dual is unbounded. * The primal is unbounded and the dual is infeasible. * Both the primal and the dual are feasible and their optimal values are equal."
  },
  {
    "objectID": "docs/methods/fom/Shampoo.html",
    "href": "docs/methods/fom/Shampoo.html",
    "title": "1 Summary",
    "section": "",
    "text": "1 Summary\nThe idea of maintaining second order statistics from accumulated stochastic gradients is the cornerstone of the stochastic first order optimization. Conceptually, guys threat parameter of each layer as a matrix and compute left and right preconditioner instead of one matrix preconditioner to the vectorized parameters, which allows to reduce the number of computations and the amount of memory, required to store."
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html",
    "href": "docs/methods/fom/Mirror_descent.html",
    "title": "1 Возвращение к истокам",
    "section": "",
    "text": "Метод зеркального спуска является естественным обобщением метода проекции субградиента в случае обобщения l_2 нормы на более общий случай какой-то функции расстояния."
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#dual-norm",
    "href": "docs/methods/fom/Mirror_descent.html#dual-norm",
    "title": "1 Возвращение к истокам",
    "section": "0.1 Dual norm:",
    "text": "0.1 Dual norm:\nОпределение: Сопряженной нормой \\|\\cdot\\|_* к данной \\|\\cdot\\| называется:\n\n\\|y\\|_* = \\max \\{\\langle y,x\\rangle: \\|x\\|=1 \\}\n\nПример: (\\|\\cdot \\|_p)_* = \\|\\cdot \\|_q, \\qquad \\dfrac{1}{p} + \\dfrac{1}{q} = 1\nДоказательство:\nНеравенство Гельдера:\n\n\\sum_{k=1}^n |x_k\\,y_k| \\le \\biggl( \\sum_{k=1}^n |x_k|^p \\biggr)^{\\frac{1}{p}} \\biggl( \\sum_{k=1}^n |y_k|^q \\biggr)^{\\frac{1}{q}}\n\\text{ for all }x, y \\in \\mathbb{C}^n\n\nСвойства: * Двойственная норма \\|\\cdot\\|_* является нормой * l_2 норма сопряжена сама себе * Двойственная норма к двойственной норме - исходная норма * (\\|\\cdot\\|_1)_* = \\|\\cdot\\|_\\infty, \\;(\\|\\cdot\\|_\\infty)_* = \\|\\cdot\\|_1 * Обобщенное неравенство Коши Шварца: \\langle y,x \\rangle \\leq \\|y\\|_*\\|x\\|, следствие: \\|x\\|^2 \\pm 2 \\langle y,x \\rangle + \\|y\\|_*^2 \\geq 0"
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#bregman-divergence",
    "href": "docs/methods/fom/Mirror_descent.html#bregman-divergence",
    "title": "1 Возвращение к истокам",
    "section": "0.2 Bregman divergence",
    "text": "0.2 Bregman divergence\nПопробуем интуитивно ввести понятие обобщенного расстояния, именуемого расстоянием Брэгмана. Для каждой точки y она возвращает расстояние этой точки до x - V_x(y). В самом простом случае можно взять V_x(y) = \\frac{1}{2}\\|x-y\\|^2, \\;\\; \\nabla V_x(y) = y-x. Рассмотрим уже классическую запись:\n\n\\begin{align*}\n\\|x_{k+1} - y\\|^2  &= \\|x_{k+1}-x_k \\|^2 + \\|x_k - y\\|^2 - 2 \\langle x_k - x_{k+1} ,x_k - y\\rangle \\\\\n\\tag{Req1}\nV_{x_{k+1}}(y) &= V_{x_{k+1}}(x_k) + V_{x_{k}}(y) - \\langle \\nabla V_{x_{k+1}(x_k)}, x_k - y \\rangle\n\\end{align*}\n\nДля вводимого обобщенного расстояния будем требовать выполнения (Req1), кроме того (как будет видно при получении оценок), приятным свойством было бы еще следующее требование:\n\n\\tag{Req2}\nV_x(y) \\geq \\frac{1}{2} \\|x-y\\|^2\n\nОпределение: Дивергенцией (расстоянием) Брэгмана называется функция следующая V_x(y). Пусть S \\subseteq \\mathbb{R}^n - замкнутое выпуклое множество, тогда функция \\phi : S \\to \\mathbb{R} называется прокс-функцией (distance generating function), если \\phi является 1 - сильно выпуклой, т.е.:\n\n\\phi(y) \\geq \\phi(x) + \\langle \\nabla \\phi(x), y-x\\rangle + \\frac{1}{2} \\|y-x\\|^2, \\qquad \\forall x,y \\in S\n\nТогда прокс-функцией индуцируется расстояние Брэгмана:\n\nV_x(y) = \\phi(y) - \\phi(x) - \\langle\\nabla \\phi(x), y-x\\rangle\n\nЗаметим, что определение сильной выпуклости зависит от выбора прямой нормы \\|\\cdot\\|. Это важное замечание, поскольку именно это свойство позволит в будущем подстраивать расстояние под геометрию пространства.\n\n0.2.1 Examples\n\nВыберем норму в прямом пространстве \\|\\cdot\\| = \\|\\cdot\\|_2, пусть \\phi(x) = \\frac{1}{2}\\|x\\|^2, тогда расстояние Брэгмана V_x(y) = \\frac{1}{2}\\|x-y\\|^2. Такой выбор совпадает с тем, что мы видели ранее в методе проекции субградиента\nВыберем теперь другую норму \\|\\cdot\\| = \\| \\cdot \\|_1, пусть \\phi(x) = \\sum\\limits_{i \\in [n]}x_i \\log x_i - антиэнтропия. Тогда эта функция будет 1 сильно выпукла на выпуклом множестве S : \\left\\{x \\in S : x \\geq 0, \\sum\\limits_{i \\in [n]} x_i = 1\\right\\} (вероятностном симплексе), а соответствующая ей дивергенция Брэгмана: V_x(y) = \\sum\\limits_{i \\in [n]} y_i \\log \\frac{y_i}{x_i} = D(y \\| x) - расстояние Кульбака - Ляйблера.\nЕще немного примеров отсюда:\n\n\n\n\n0.2.2 Свойства\n\nАксиома тождества V_x(x) = 0\nСовместимость с Евклидовой нормой: V_x(y) \\geq \\frac{1}{2}\\|x-y\\|^2 \\geq 0\n(Не)равенство треугольника: \\langle -\\nabla V_x(y), y-z\\rangle = V_x(z) - V_y(z) - V_x(y)\n\nПервые два свойства очевидны из определения. Докажем третье:\n\n\\begin{align*}\n\\langle -\\nabla V_x(y), y-z\\rangle &= \\langle \\nabla \\phi(x) - \\nabla \\phi(y) , y-z\\rangle =\\\\\n& = (\\phi(z) - \\phi(x) - \\langle \\nabla \\phi(x), z -x \\rangle) \\\\\n& - (\\phi(z) - \\phi(y) - \\langle \\nabla \\phi(y), z - y \\rangle) \\\\\n& - (\\phi(y) - \\phi(x) - \\langle \\nabla \\phi(x), y - x \\rangle) \\\\\n& = V_x(z) - V_y(z) - V_x(y)\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#алгоритм-зеркального-спуска-mirror-descent",
    "href": "docs/methods/fom/Mirror_descent.html#алгоритм-зеркального-спуска-mirror-descent",
    "title": "1 Возвращение к истокам",
    "section": "1.1 Алгоритм зеркального спуска (mirror descent):",
    "text": "1.1 Алгоритм зеркального спуска (mirror descent):\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right)\n\nИнтересные фишки: * Такая же скорость сходимости, как и для метода проекции субградиента. * Работает в существенно более широком классе практических задач"
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#онлайн-версия",
    "href": "docs/methods/fom/Mirror_descent.html#онлайн-версия",
    "title": "1 Возвращение к истокам",
    "section": "1.2 Онлайн версия",
    "text": "1.2 Онлайн версия\nСовершенно ясно, что в наших оценках на каждом шаге может быть новая функция f_k(x) на заданном классе. Поэтому, аналогичные оценки получаются и для онлайн постановки:\n\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq \\sqrt{2 M G^2 T}\n\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq \\sqrt{\\dfrac{2 M G^2}{T}}"
  },
  {
    "objectID": "docs/methods/fom/Mirror_descent.html#еще-одна-интерпретация",
    "href": "docs/methods/fom/Mirror_descent.html#еще-одна-интерпретация",
    "title": "1 Возвращение к истокам",
    "section": "1.3 Еще одна интерпретация",
    "text": "1.3 Еще одна интерпретация\nДавайте покажем, что полученный алгоритм имеет еще одну очень интуитивную интерпретацию:\n\ny_{k} = \\nabla \\phi(x_k) Отображение в сопряженное пространство с помощью функции \\nabla \\phi(x)\ny_{k+1} = y_k - \\alpha_k \\nabla f_k(x_k) Градиентный шаг в сопряженном пространстве\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S}V_{\\nabla \\phi^*(y_{k+1})}(x) Обратное отображение с помощью функции \\nabla \\phi^*(x) и проекция на бюджетное множество \n\nДля доказательства эквивалентности таких записей, следует сначала доказать факт того, что:\n\n\\left( \\nabla \\phi(x) \\right)^{-1} =  \\nabla \\phi^*(y)\n\nДля этого пусть y = \\nabla \\phi(x). Заметим, что для сопряженной функции справедливо неравенство Фенхеля - Юнга: \\phi^*(y) + \\phi(x) \\geq xy, в случае, если \\phi(x) - дифференцируема, такое преобразование называется преобразованием Лежандра и выполняется равенство: \\phi^*(y) + \\phi(x) = xy. Дифференцируя равенство по y, получаем \\nabla \\phi^*(y) = x. Таким образом,\n\n\\nabla\\phi^*(y) = \\nabla\\phi^*(\\nabla \\phi(x)) = x, \\qquad \\nabla\\phi(x) = \\nabla\\phi(\\nabla \\phi^*(y)) = y\n\nДоказательство:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{\\nabla \\phi^*(y_{k+1})}(x) \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x) - \\phi(\\nabla \\phi^*(y_{k+1})) - \\left\\langle \\nabla \\phi (\\nabla \\phi^*(y_{k+1})),x - \\nabla \\phi^*(y_{k+1})\\right\\rangle\\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ \\phi(x)  - \\left\\langle y_{k+1},x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x)  - \\left\\langle \\nabla \\phi(x_k) - \\alpha_k g_k,x \\right\\rangle \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{  \\phi(x) - \\phi(x_k) - \\left\\langle \\nabla \\phi(x_k),x \\right\\rangle + \\left\\langle \\alpha_k g_k,x \\right\\rangle  \\right\\} = \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left\\{ V_{x_k}(x) + \\left\\langle \\alpha_k g_k,x \\right\\rangle \\right\\}\n\\end{align*}\n\nВ последней строчке мы пришли к той формулировке, которую писали раньше. Заметим так же, еще одну интересную концепцию:\n\n\\begin{align*}\nx_{k+1} &= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right) \\\\\n&= \\text{arg}\\min\\limits_{x \\in S} \\left(f(x_k)+  \\langle g_k, x \\rangle + \\frac{1}{\\alpha_k}V_{x_k}(x) \\right)\n\\end{align*}\n\nЗдесь левая часть минимизируемого выражения представляет собой аппроксимацию первого порядка, а правая часть представляет собой проекционный член."
  },
  {
    "objectID": "docs/methods/fom/GD.html",
    "href": "docs/methods/fom/GD.html",
    "title": "1 Summary",
    "section": "",
    "text": "A classical problem of function minimization is considered.\n\n\\tag{GD}\nx_{k+1} = x_k - \\eta_k\\nabla f(x_k)\n\n\nThe bottleneck (for almost all gradient methods) is choosing step-size, which can lead to the dramatic difference in method’s behavior.\nOne of the theoretical suggestions: choosing stepsize inversly proportional to the gradient Lipschitz constant \\eta_k = \\dfrac{1}{L}.\nIn huge-scale applications the cost of iteration is usually defined by the cost of gradient calculation (at least \\mathcal{O}(p)).\nIf function has Lipschitz-continious gradient, then method could be rewritten as follows:\n\n \\begin{align*}x_{k+1} &= x_{k}-\\dfrac{1}{L} \\nabla f\\left(x_{k}\\right)= \\\\\n&= \\arg \\min\\limits_{x \\in \\mathbb{R}^{n}}\\left\\{f\\left(x_{k}\\right)+\\left\\langle\\nabla f\\left(x_{k}\\right), x-x_{k}\\right\\rangle+\\frac{L}{2}\\left\\|x-x_{k}\\right\\|_{2}^{2}\\right\\} \\end{align*}"
  },
  {
    "objectID": "docs/methods/fom/GD.html#direction-of-local-steepest-descent",
    "href": "docs/methods/fom/GD.html#direction-of-local-steepest-descent",
    "title": "1 Summary",
    "section": "2.1 Direction of local steepest descent",
    "text": "2.1 Direction of local steepest descent\nLet’s consider a linear approximation of the differentiable function f along some direction h, \\|h\\|_2 = 1:\n\nf(x + \\eta h) = f(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta)\n\nWe want h to be a decreasing direction:\n\nf(x + \\eta h) &lt; f(x)\n\n\nf(x) + \\eta \\langle f'(x), h \\rangle + o(\\eta) &lt; f(x)\n\nand going to the limit at \\eta \\rightarrow 0:\n\n\\langle f'(x), h \\rangle \\leq 0\n\nAlso from Cauchy–Bunyakovsky–Schwarz inequality:\n\n|\\langle f'(x), h \\rangle | \\leq \\| f'(x) \\|_2 \\| h \\|_2 \\;\\;\\;\\to\\;\\;\\; \\langle f'(x), h \\rangle \\geq -\\| f'(x) \\|_2 \\| h \\|_2 = -\\| f'(x) \\|_2\n\nThus, the direction of the antigradient\n\nh = -\\dfrac{f'(x)}{\\|f'(x)\\|_2}\n\ngives the direction of the steepest local decreasing of the function f.\nThe result of this method is\n\nx_{k+1} = x_k - \\eta f'(x_k)"
  },
  {
    "objectID": "docs/methods/fom/GD.html#gradient-flow-ode",
    "href": "docs/methods/fom/GD.html#gradient-flow-ode",
    "title": "1 Summary",
    "section": "2.2 Gradient flow ODE",
    "text": "2.2 Gradient flow ODE\nLet’s consider the following ODE, which is referred as Gradient Flow equation.\n\n\\tag{GF}\n\\frac{dx}{dt} = -f'(x(t))\n\nand discretize it on a uniform grid with \\eta step:\n\n\\frac{x_{k+1} - x_k}{\\eta} = -f'(x_k),\n\nwhere x_k \\equiv x(t_k) and \\eta = t_{k+1} - t_k - is the grid step.\nFrom here we get the expression for x_{k+1}\n\nx_{k+1} = x_k - \\eta f'(x_k),\n\nwhich is exactly gradient descent."
  },
  {
    "objectID": "docs/methods/fom/GD.html#necessary-local-minimum-condition",
    "href": "docs/methods/fom/GD.html#necessary-local-minimum-condition",
    "title": "1 Summary",
    "section": "2.3 Necessary local minimum condition",
    "text": "2.3 Necessary local minimum condition\n\n\\begin{align*}\n& f'(x) = 0\\\\\n& -\\eta f'(x) = 0\\\\\n& x - \\eta f'(x) = x\\\\\n& x_k - \\eta f'(x_k) = x_{k+1}\n\\end{align*}\n\nThis is, surely, not a proof at all, but some kind of intuitive explanation."
  },
  {
    "objectID": "docs/methods/fom/GD.html#minimizer-of-lipschitz-parabola",
    "href": "docs/methods/fom/GD.html#minimizer-of-lipschitz-parabola",
    "title": "1 Summary",
    "section": "2.4 Minimizer of Lipschitz parabola",
    "text": "2.4 Minimizer of Lipschitz parabola\nSome general highlights about Lipschitz properties are needed for explanation. If a function f: \\mathbb{R}^n \\to \\mathbb{R} is continuously differentiable and its gradient satisfies Lipschitz conditions with constant L, then \\forall x,y \\in \\mathbb{R}^n:\n\n|f(y) - f(x) - \\langle \\nabla f(x), y-x \\rangle| \\leq \\frac{L}{2} \\|y-x\\|^2,\n\nwhich geometrically means, that if we’ll fix some point x_0 \\in \\mathbb{R}^n and define two parabolas:\n\n\\phi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2} \\|x-x_0\\|^2,\n\n\n\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2} \\|x-x_0\\|^2.\n\nThen\n\n\\phi_1(x) \\leq f(x) \\leq \\phi_2(x) \\quad \\forall x \\in \\mathbb{R}^n.\n\nNow, if we have global upper bound on the function, in a form of parabola, we can try to go directly to its minimum.\n\n\\begin{align*}\n& \\nabla \\phi_2(x) = 0 \\\\\n& \\nabla f(x_0) + L (x^* - x_0) = 0 \\\\\n& x^* = x_0 - \\frac{1}{L}\\nabla f(x_0) \\\\\n& x_{k+1} = x_k - \\frac{1}{L} \\nabla f(x_k)\n\\end{align*}\n\n\nThis way leads to the \\frac{1}{L} stepsize choosing. However, often the L constant is not known.\nBut if the function is twice continuously differentiable and its gradient has Lipschitz constant L, we can derive a way to estimate this constant \\forall x \\in \\mathbb{R}^n:\n\n\\|\\nabla^2 f(x) \\| \\leq L\n\nor\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n"
  },
  {
    "objectID": "docs/methods/fom/GD.html#fixed-sequence",
    "href": "docs/methods/fom/GD.html#fixed-sequence",
    "title": "1 Summary",
    "section": "3.1 Fixed sequence",
    "text": "3.1 Fixed sequence\n\n\\eta_k = \\dfrac{1}{\\sqrt{k+1}}\n\nThe latter 2 strategies are the simplest in terms of implementation and analytical analysis. It is clear that this approach does not often work very well in practice (the function geometry is not known in advance)."
  },
  {
    "objectID": "docs/methods/fom/GD.html#exact-line-search-aka-steepest-descent",
    "href": "docs/methods/fom/GD.html#exact-line-search-aka-steepest-descent",
    "title": "1 Summary",
    "section": "3.2 Exact line search aka steepest descent",
    "text": "3.2 Exact line search aka steepest descent\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_{k+1}) = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nMore theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.\nInteresting theoretical property of this method is that each following iteration is orthogonal to the previous one:\n\n\\eta_k = \\text{arg}\\min_{\\eta \\in \\mathbb{R^+}} f(x_k - \\eta \\nabla f(x_k))\n\nOptimality conditions:\n\n\\nabla f(x_{k+1})^\\top \\nabla f(x_k) = 0"
  },
  {
    "objectID": "docs/methods/fom/GD.html#goldstein-armijo",
    "href": "docs/methods/fom/GD.html#goldstein-armijo",
    "title": "1 Summary",
    "section": "3.3 Goldstein-Armijo",
    "text": "3.3 Goldstein-Armijo"
  },
  {
    "objectID": "docs/methods/fom/GD.html#convex-case",
    "href": "docs/methods/fom/GD.html#convex-case",
    "title": "1 Summary",
    "section": "4.1 Convex case",
    "text": "4.1 Convex case\n\n4.1.1 Lipischitz continuity of the gradient\nAssume that f: \\mathbb{R}^n \\to \\mathbb{R} is convex and differentiable, and additionally \n\\|\\nabla f(x) − \\nabla f(y) \\| \\leq L \\|x − y \\| \\; \\forall x, y \\in \\mathbb{R}^n\n\ni.e. , \\nabla f is Lipschitz continuous with constant L &gt; 0.\nSince \\nabla f Lipschitz with constant L, which means \\nabla^2 f \\preceq LI, we have \\forall x, y, z:\n\n(x − y)^\\top(\\nabla^2 f(z) − LI)(x − y) \\leq 0\n\n\n(x − y)^\\top\\nabla^2 f(z)(x − y) \\leq L \\|x-y\\|^2\n\nNow we’ll consider second order Taylor approximation of f(y) and Taylor’s Remainder Theorem (we assum, that the function f is continuously differentiable), we have \\forall x, y, \\exists z ∈ [x, y]:\n\n\\begin{align*}\nf(y) &= f(x) + \\nabla f(x)^\\top(y − x) + \\frac{1}{2}(x − y)^\\top \\nabla^2 f(z)(x − y) \\\\\n& \\leq f(x) + \\nabla f(x)^\\top(y − x) + \\frac{L}{2} \\|x-y\\|^2\n\\end{align*}\n\nFor the gradient descent we have x = x_k, y = x_{k+1}, x_{k+1} = x_k - \\eta_k\\nabla f(x_k) :\n\n\\begin{align*}\nf(x_{k+1}) &\\leq  f(x_k) + \\nabla f(x_k)^\\top(-\\eta_k\\nabla f(x_k)) + \\frac{L}{2} \\| \\eta_k\\nabla f(x_k) \\|^2  \\\\\n& \\leq f(x_k) - \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\|\\nabla f(x_k)\\|^2\n\\end{align*}\n\n\n\n4.1.2 Optimal constant stepsize\nNow, if we’ll consider constant stepsize strategy and will maximize \\left( 1 - \\dfrac{L\\eta}{2}\\right)\\eta \\to \\max\\limits_{\\eta}, we’ll get \\eta = \\dfrac{1}{L}.\n\nf(x_{k+1}) \\leq f(x_k) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2\n\n\n\n4.1.3 Convexity\n\nf(x_{k}) \\leq f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*)\n\nThat’s why we have:\n\n\\begin{align*}\nf(x_{k+1}) & \\leq  f(x^*) + \\nabla f(x_k)^\\top (x_k − x^*) -  \\dfrac{1}{2L}\\|\\nabla f(x_k)\\|^2 \\\\\n& = f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_k − x^* − \\dfrac{1}{L}\\nabla f(x_k)\\|^2\\right) \\\\\n& =  f(x^*) + \\dfrac{L}{2}\\left(\\|x_k − x^*\\|^2 − \\|x_{k+1} − x^*\\|^2\\right)\n\\end{align*}\n\nThus, summing over all iterations, we have:\n\n\\begin{align*}\n\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) &\\leq \\dfrac{L}{2} \\left(\\|x_0 − x^*\\|^2 − \\|x_{k} − x^*\\|^2\\right) \\\\\n& \\leq  \\dfrac{L}{2} \\|x_0 − x^*\\|^2 =  \\dfrac{LR^2}{2},\n\\end{align*}\n\nwhere R = \\|x_0 - x^*\\|. And due to function monotonicity:\n\nf(x_k) - f(x^*) \\leq \\dfrac{1}{k}\\sum\\limits_{i=1}^k (f(x_i) - f(x^*)) \\leq \\dfrac{LR^2}{2k} = \\dfrac{R^2}{2\\eta k}"
  },
  {
    "objectID": "docs/methods/fom/GD.html#strongly-convex-case",
    "href": "docs/methods/fom/GD.html#strongly-convex-case",
    "title": "1 Summary",
    "section": "4.2 Strongly convex case",
    "text": "4.2 Strongly convex case\nIf the function is strongly convex:\n\nf(y) \\geq f(x) + \\nabla f(x)^\\top (y − x) + \\dfrac{\\mu}{2}\\|y − x \\|^2 \\; \\forall x, y \\in \\mathbb{R}^n\n\n…\n\n\\|x_{k+1} − x^*\\|^2 \\leq (1 − \\eta \\mu)\\|x_k − x^* \\|^2\n # Bounds\n\n\n\n\n\n\n\n\n\nConditions\n\\Vert f(x_k) - f(x^*)\\Vert \\leq\nType of convergence\n\\Vert x_k - x^* \\Vert \\leq\n\n\n\n\nConvexLipschitz-continuous function(G)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{GR}{k}\nSublinear\n\n\n\nConvexLipschitz-continuous gradient (L)\n\\mathcal{O}\\left(\\dfrac{1}{k} \\right) \\; \\dfrac{LR^2}{k}\nSublinear\n\n\n\n\\mu-Strongly convexLipschitz-continuous gradient(L)\n\nLinear\n(1 - \\eta \\mu)^k R^2\n\n\n\\mu-Strongly convexLipschitz-continuous hessian(M)\n\nLocally linear R &lt; \\overline{R}\n\\dfrac{\\overline{R}R}{\\overline{R} - R} \\left( 1 - \\dfrac{2\\mu}{L+3\\mu}\\right)\n\n\n\n\nR = \\| x_0 - x^*\\|  - initial distance\n$\\overline{R} = \\dfrac{2\\mu}{M}$"
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html",
    "href": "docs/methods/fom/Projected_subgradient_descent.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:"
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#recap",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#recap",
    "title": "1 Intuition",
    "section": "",
    "text": "Suppose, we are to solve the following problem:\n\n\\tag{P}\n\\min_{x \\in S} f(x),\n\nWhen S = \\mathbb{R}^n, we have the unconstrained problem, which sometimes could be solved with (sub)gradient descent algorithm:\n\n\\tag{SD}\nx_{k+1} = x_k - \\alpha_k g_k,\n\nFor this method we have the following bounds:"
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#introduction",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#introduction",
    "title": "1 Intuition",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nВ этом разделе мы будем рассматривать работу в рамках какого-то выпуклого множества S \\in \\mathbb{R}^n, так, чтобы x_k \\in S. Запишем для начала соотношение для итераций:\n\n\\begin{align*}\n\\|x_{k+1} - x^*\\|^2 &= \\|(x_{k+1} - x_k) + (x_k - x^*)\\|^2 = \\\\\n&= \\|x_k - x_{k+1}\\|^2 + \\|x_k - x^*\\|^2 - 2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle \\\\\n2 \\langle x_k - x_{k+1} ,x_k - x^*\\rangle &=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2\n\\end{align*}\n\nЗаметим, что при работе на ограниченном множестве у нас появилась небольшая проблема: x_{k+1} может не лежать в бюджетном множестве. Сейчас мы увидим, почему это является проблемой для выписывания оценок на число итераций: если мы имеем неравенство, записанное ниже, то процесс получения оценок будет абсолютно совпадать с описанными выше процедурами (потому что в случае субградиентного метода x_k - x_{k+1} = \\alpha_k g_k).\n\n\\tag{Target}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle\n\nОднако, в нашем случае мы можем лишь получить (будет показано ниже) оценки следующего вида:\n\n\\tag{Forward Target}\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nЭто связано с тем, что x_{k+1} нам легче контролировать при построении условного метода, а значит, легче записать на него оценку. К сожалению, привычной телескопической (сворачивающейся) суммы при таком неравенстве не получится. Однако, если неравенство (Forward Target) выполняется, то из него следует следующее неравенство:\n\n\\begin{align*}\n\\tag{Forward Target Fix}\n\\langle \\alpha_k g_k, x_k - x^* \\rangle &\\leq \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\\\\n& - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nДля того, чтобы доказать его, запишем (Forward Target Fix):\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle + \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\n\\leq  \\\\ \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle + \\langle x_k - x_{k+1}, x_{k+1} - x_k \\rangle\n\\end{align*}\n\nПереписывая его еще раз, получаем:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k} - x^* \\rangle\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\|x_{k} - x_{k+1}\\|^2 - \\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle = \\\\\n&= \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2}\\left(\\|x_{k} - x_{k+1}\\|^2 + 2\\langle \\alpha_k g_k, x_{k+1} - x_k \\rangle\\right) \\leq \\\\\n&\\leq \\langle x_k - x_{k+1}, x_{k} - x^* \\rangle - \\frac{1}{2}\\|x_{k} - x_{k+1}\\|^2 -\\frac{1}{2} \\left( - \\alpha_k^2 g_k^2\\right) = \\\\\n&= \\langle x_k - x_{k+1}, x_k - x^* \\rangle - \\dfrac{1}{2}\\|x_k - x_{k+1}\\|^2 + \\dfrac{1}{2}\\alpha_k^2 g_k^2\n\\end{align*}\n\nИтак, пускай мы имеем неравенство (Forward Target) - напомню, что мы его пока не доказали. Теперь покажем, как с его помощью получить оценки на сходимость метода. Для этого запишем неравенство (Forward Target Fix):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k, x_k &- x^* \\rangle + \\|x_k - x_{k+1}\\|^2 - \\alpha_k^2 g_k^2 \\leq \\\\\n&\\leq 2\\langle x_k - x_{k+1}, x_k - x^* \\rangle \\\\\n&= \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\|x_k - x_{k+1}\\|^2 \\\\\n&\\quad \\\\\n2 \\langle \\alpha_k g_k, x_k - x^* \\rangle\n&\\leq \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k^2\n\\end{align*}\n\nЕсли внимательно посмотреть на полученный результат, то это в точности совпадает с исходной точкой доказательства для субградиентного метода в безусловном сеттинге.\nМожем сразу получить оценки:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T} \\\\\nf(\\overline{x}) - f^* &\\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nТаким образом, мы показали, что для метода проекции субградиента справедлива точно такая же оценка на число итераций, если выполняется неравенство (Forward Target) :) Давайте разбираться с ним\nНам следует доказать, что:\n\n\\langle \\alpha_k g_k, x_{k+1} - x^* \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - x^* \\rangle\n\nВ более общем случае \\forall y \\in S:\n\n\\begin{align*}\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle \\leq \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle & \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nВспомним из неравенства для проекции (равно как и условия оптимальности первого порядка), что \\forall y \\in S для некоторой гладкой выпуклой минимизируемой функции g(x) в точке оптимума x \\in S:\n\n\\langle \\nabla g(x), x - y \\rangle \\leq 0\n\nВ противном бы случае, можно было бы сделать градиентный шаг в направлении y -x и уменьшить значение функции.\nРассмотрим теперь следующую функцию g(x):\n\ng(x) = \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2, \\quad \\nabla g(x) = \\alpha_k g_k + x - x_k\n\nИ давайте теперь строить условный алгоритм как минимизацию этой функции:\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nТогда из условия оптимальности:\n\n\\begin{align*}\n\\langle \\nabla g(x_{k+1}), x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k + x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k , x_{k+1} - y \\rangle  + \\langle x_{k+1} - x_k, x_{k+1} - y \\rangle &\\leq 0 \\\\\n\\langle \\alpha_k g_k, x_{k+1} - y \\rangle - \\langle x_k - x_{k+1}, x_{k+1} - y \\rangle &\\leq 0\n\\end{align*}\n\nПолученное неравенство в точности совпадает с неравенством (Forward Target), которое нам как раз таки и следовало доказать. Таким образом, мы получаем"
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#algorithm",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#algorithm",
    "title": "1 Intuition",
    "section": "2.2 Algorithm",
    "text": "2.2 Algorithm\n\nx_{k+1} = \\text{arg}\\min\\limits_{x \\in S} \\left( \\langle \\alpha_k g_k, x \\rangle + \\dfrac{1}{2} \\| x - x_k\\|^2 \\right)\n\nИнтересные фишки: * Такая же скорость сходимости, как и для безусловного алгоритма. (Однако, стоимость каждой итерации может быть существенно больше из за необходимости решать задачу оптимизации на каждом шаге) * В частном случае S = \\mathbb{R}^n в точности совпадает с безусловным алгоритмом (убедитесь)\n\n2.2.1 Adaptive stepsize (without T)\nРазберем теперь одну из стратегий того, как избежать знания количества шагов T заранее для подбора длины шага \\alpha_k. Для этого зададим “диаметр” нашего множества D:\n\nD : \\{ \\max\\limits_{x,y \\in S} \\|x - y\\| \\leq D \\}\n\nТеперь зададим длину шага на k- ой итерации, как: \\alpha_k = \\tau \\sqrt{\\dfrac{1}{k+1}}. Константу \\tau \\geq 0 подберем чуть позже.\nДля начала легко заметить, что:\n\n\\begin{align*}\n\\sum\\limits_{k=0}^{T-1} \\alpha_k &= \\tau \\sum\\limits_{k=0}^{T-1} \\dfrac{1}{\\sqrt{k+1}} = \\tau \\left( 1 + \\sum\\limits_{k=1}^{T-1} \\dfrac{1}{\\sqrt{k+1}}\\right) \\leq \\\\\n&\\leq \\tau \\left(1 + \\int\\limits_{0}^{T-1} \\dfrac{1}{\\sqrt{x+1}} dx \\right) = \\tau (2\\sqrt{T}-1)\n\\end{align*}\n\nсм. геометрический смысл неравенства ниже:\n\nВозьмем теперь равенство для классического субградиентного метода (БМ) (или неравенство в случае метода проекции субгадиента (УМ)):\n\n\\begin{align*}\n2 \\langle \\alpha_k g_k ,x_k - x^*\\rangle\n&=  \\|x_k - x^*\\|^2 - \\|x_{k+1} - x^*\\|^2 + \\alpha_k^2 g_k ^2 \\\\\n\\sum\\limits_{k=0}^{T-1} \\langle g_k ,x_k - x^*\\rangle\n&= \\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{\\|x_k - x^*\\|^2}{2 \\alpha_k} - \\dfrac{\\|x_{k+1} - x^*\\|^2}{2 \\alpha_k} + \\dfrac{\\alpha_k}{2}g_k^2 \\right) \\\\\n&\\leq \\dfrac{\\|x_0 - x^*\\|^2}{2 \\alpha_0} - \\dfrac{\\|x_T - x^*\\|^2}{2 \\alpha_{T-1}} + \\\\\n&+ \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\|x_k - x^*\\|^2 + \\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2}g_k^2 \\leq \\\\\n& \\leq D^2 \\left( \\dfrac{1}{2 \\alpha_0} + \\dfrac{1}{2}\\sum\\limits_{k=0}^{T-1} \\left( \\dfrac{1}{\\alpha_{k} }- \\dfrac{1}{\\alpha_{k-1}} \\right) \\right) + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n& \\leq \\dfrac{D^2}{2 \\alpha_{T-1}} + G^2\\sum\\limits_{k=0}^{T-1} \\dfrac{\\alpha_k}{2} \\leq \\\\\n&\\leq \\dfrac{1}{2} \\left( \\dfrac{D^2}{\\tau}\\sqrt{T} + \\tau G^2 \\left(2\\sqrt{T} - 1\\right)\\right) \\leq \\\\\n& \\leq DG \\sqrt{2T}\n\\end{align*}\n\nГде \\tau = \\dfrac{D}{G\\sqrt{2}} - выбран путем минимизации данной оценки по \\tau.\nТаким образом, мы получили, что в случае, когда количество шагов T неизвестно заранее (весьма важное свойство), оценка ухудшается в \\sqrt{2} раз. Такие оценки называют anytime bounds.\n\n\n2.2.2 Online learning:\nPSD - Projected Subgradient Descent\n\n\\begin{align*}\n\\tag{anytime PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{2T} \\\\\n\\tag{PSD}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x \\in S} \\sum\\limits_{k = 0}^{T-1} f_k(x) \\leq DG \\sqrt{T}\n\\end{align*}"
  },
  {
    "objectID": "docs/methods/fom/Projected_subgradient_descent.html#least-squares-with-l_1-regularization",
    "href": "docs/methods/fom/Projected_subgradient_descent.html#least-squares-with-l_1-regularization",
    "title": "1 Intuition",
    "section": "3.1 Least squares with l_1 regularization",
    "text": "3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R^n}} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\n\n3.1.1 Nonnegativity\n\nS = \\{x \\in \\mathbb{R}^n \\mid x \\geq 0 \\}\n\n\n\n3.1.2 l_2 - ball\n\nS = \\{x \\in \\mathbb{R}^n \\mid \\|x - x_c\\| \\le R \\}\n\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\n\n\n3.1.3 Linear equality constraints\n\nS = \\{x \\in \\mathbb{R}^n \\mid Ax = b \\}"
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html",
    "href": "docs/methods/fom/Subgradient descent.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Рассматривается классическая задача выпуклой оптимизации:\n\n\\min_{x \\in S} f(x),\n\nПодразумевается, что f(x) - выпуклая функция на выпуклом множестве S. Для начала будем рассматривать задачу безусловной минимизации (БМ), S = \\mathbb{R}^n\nВектор g называется субградиентом функции f(x): S \\to \\mathbb{R} в точке x_0, если \\forall x \\in S:\n\nf(x)  \\geq f(x_0) +  \\langle g, x - x_0 \\rangle\n\nГрадиентный спуск предполагает, что функция f(x) является дифференцируемой в каждой точке задачи. Теперь же, мы будем предполагать лишь выпуклость.\nИтак, мы имеем оракул первого порядка:\nВход: x \\in \\mathbb R^n\nВыход: \\partial f(x) и f(x)"
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#bounds",
    "href": "docs/methods/fom/Subgradient descent.html#bounds",
    "title": "1 Introduction",
    "section": "2.1 Bounds",
    "text": "2.1 Bounds\n\n2.1.1 Vanilla version\nЗапишем как близко мы подошли к оптимуму x^* = \\text{arg}\\min\\limits_{x \\in \\mathbb{R}^n} f(x) = \\text{arg} f^* на последней итерации:\n\n\\begin{align*}\n\\| x_{k+1} - x^* \\|^2 & = \\|x_k - x^* - \\alpha_k g_k\\|^2 = \\\\\n                      & = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\\end{align*}\n\nДля субградиента: \\langle g_k, x_k - x^* \\rangle \\leq f(x_k) - f(x^*) = f(x_k) - f^*. Из написанного выше:\n\n\\begin{align*}\n2\\alpha_k \\langle g_k, x_k - x^* \\rangle =  \\| x_k - x^* \\|^2 + \\alpha_k^2 g_k^2 - \\| x_{k+1} - x^* \\|^2\n\\end{align*}\n\nПросуммируем полученное неравенство для k = 0, \\ldots, T-1\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1}2\\alpha_k \\langle g_k, x_k - x^* \\rangle &=  \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq \\| x_0 - x^* \\|^2 + \\sum\\limits_{k=0}^{T-1}\\alpha_k^2 \\|g_k^2\\| \\\\\n&\\leq R^2 + G^2\\sum\\limits_{k=0}^{T-1}\\alpha_k^2\n\\end{align*}\n\nЗдесь мы предположили R^2 = \\|x_0 - x^*\\|^2, \\qquad \\|g_k\\| \\leq G. Предполагая \\alpha_k = \\alpha (постоянный шаг), имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq \\dfrac{R^2}{2 \\alpha} + \\dfrac{\\alpha}{2}G^2 T\n\\end{align*}\n\nМинимизация правой части по \\alpha дает \\alpha^* = \\dfrac{R}{G}\\sqrt{\\dfrac{1}{T}}\n\n\\begin{align*}\n\\tag{Subgradient Bound}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq GR \\sqrt{T}\n\\end{align*}\n\nТогда (используя неравенство Йенсена и свойство субградиента f(x^*) \\geq f(x_k) + \\langle g_k, x^* - x_k \\rangle) запишем оценку на т.н. Regret, а именно:\n\n\\begin{align*}\nf(\\overline{x}) - f^* &= f \\left( \\frac{1}{T}\\sum\\limits_{k=0}^{T-1} x_k \\right) - f^* \\leq \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1} (f(x_k) - f^* )\\right) \\\\\n& \\leq  \\dfrac{1}{T} \\left( \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^* \\rangle\\right) \\\\\n& \\leq G R \\dfrac{1}{ \\sqrt{T}}\n\\end{align*}\n\nВажные моменты:\n\nПолучение оценок не для x_T, а для среднего арифметического по итерациям \\overline{x} - типичный трюк при получении оценок для методов, где есть выпуклость, но нет удобного убывания на каждой итерации. Нет гарантий успеха на каждой итерации, но есть гарантия успеха в среднем\nДля выбора оптимального шага необходимо знать (предположить) число итераций заранее. Возможный выход: инициализировать T небольшим значением, после достижения этого количества итераций удваивать T и рестартовать алгоритм. Более интеллектуальный способ: адаптивный выбор длины шага.\n\n\n\n2.1.2 Steepest subgradient descent\nПопробуем выбирать на каждой итерации длину шага более оптимально. Тогда:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 + \\alpha_k^2 \\|g_k\\|^2 - 2 \\alpha_k \\langle g_k, x_k - x^* \\rangle\n\nМинимизируя выпуклую правую часть по \\alpha_k, получаем:\n\n\\alpha_k = \\dfrac{\\langle g_k, x_k - x^*\\rangle}{\\| g_k\\|^2}\n\nОценки изменятся следующим образом:\n\n\\| x_{k+1} - x^* \\|^2  = \\| x_k - x^* \\|^2 - \\dfrac{\\langle g_k, x_k - x^*\\rangle^2}{\\| g_k\\|^2}\n\n\n\\langle g_k, x_k - x^*\\rangle^2 = \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) \\| g_k\\|^2\n\n\n\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\sum\\limits_{k=0}^{T-1}\\left( \\| x_k - x^* \\|^2 - \\| x_{k+1} - x^* \\|^2 \\right) G^2\n\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq \\left( \\| x_0 - x^* \\|^2 - \\| x_{T} - x^* \\|^2 \\right) G^2\n\n\n\\dfrac{1}{T}\\left(\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle \\right)^2 \\leq \\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle^2 \\leq R^2  G^2\n\nЗначит,\n\n\\sum\\limits_{k=0}^{T-1}\\langle g_k, x_k - x^*\\rangle  \\leq GR \\sqrt{T}\n\nЧто приводит к абсолютно такой же оценке \\mathcal{O}\\left(\\dfrac{1}{\\sqrt{T}}\\right) на невязку по значению функции. На самом деле, для такого класса функций нельзя получить результат лучше, чем \\dfrac{1}{\\sqrt{T}} или \\dfrac{1}{\\varepsilon^2} по итерациям\n\n\n2.1.3 Online learning\nРассматривается следующая игра: есть игрок и природа. На каждом из k = 0, \\ldots, T-1 шагов: * Игрок выбирает действие x_k * Природа (возможно, враждебно) выбирает выпуклую функцию f_k, сообщает игроку значение f(x_k), g_k \\in \\partial f(x_k) * Игрок вычисляет следующее действие, чтобы минимизировать регрет:\n\n\\tag{Regret}\nR_{T-1} = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x)\n\nВ такой постановке цель игрока состоит в том, чтобы выбрать стратегию, которая минимизирует разницу его действия с наилучшим выбором на каждом шаге.\nНесмотря на весьма сложную (на первый взгляд) постановку задачи, существует стратегия, при которой регрет растет как \\sqrt{T}, что означает, что усредненный регрет \\dfrac{1}{T} R_{T-1} падает, как \\dfrac{1}{\\sqrt{T}}\nЕсли мы возьмем оценку (Subgradient Bound) для субградиентного метода, полученную выше, мы имеем:\n\n\\begin{align*}\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - x^* \\rangle &\\leq G \\|x_0 - x^*\\| \\sqrt{T}\n\\end{align*}\n\nОднако, в её выводе мы нигде не использовали тот факт, что x^* = \\text{arg}\\min\\limits_{x \\in S} f(x). Более того, мы вообще не использовали никакой специфичности точки x^*. Тогда можно записать это для произвольной точки y:\n\n\\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq G \\|x_0 - y\\| \\sqrt{T}\n\nЗапишем тогда оценки для регрета, взяв y = \\text{arg}\\min\\limits_{x \\in S}\\sum\\limits_{k = 0}^{T-1} f_k(x):\n\n\\begin{align*}\nR_{T-1} &= \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\min_{x} \\sum\\limits_{k = 0}^{T-1} f_k(x) = \\sum\\limits_{k = 0}^{T-1} f_k(x_k) - \\sum\\limits_{k = 0}^{T-1} f_k(y) = \\\\\n&= \\sum\\limits_{k = 0}^{T-1} \\left( f_k(x_k) - f_k(y)\\right) \\leq \\sum\\limits_{k = 0}^{T-1} \\langle g_k, x_k - y \\rangle \\leq \\\\\n&\\leq G \\|x_0 - y\\| \\sqrt{T}\n\\end{align*}\n\nИтого мы имеем для нашей стратегии с постоянным шагом:\n\n\\overline{R_{T-1}} = \\dfrac{1}{T}R_{T-1} \\leq G \\| x_0 - x^* \\| \\dfrac{1}{\\sqrt{T}}, \\qquad \\alpha_k = \\alpha = \\dfrac{\\|x_0 - x^*\\|}{G}\\sqrt{\\dfrac{1}{T}}"
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#least-squares-with-l_1-regularization",
    "href": "docs/methods/fom/Subgradient descent.html#least-squares-with-l_1-regularization",
    "title": "1 Introduction",
    "section": "3.1 Least squares with l_1 regularization",
    "text": "3.1 Least squares with l_1 regularization\n\n\\min_{x \\in \\mathbb{R}^n} \\dfrac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n\nAlgorithm will be written as:\n\nx_{k+1} = x_k - \\alpha_k \\left( A^\\top(Ax_k - b) + \\lambda \\text{sign}(x_k)\\right)\n\nwhere signum function is taken element-wise."
  },
  {
    "objectID": "docs/methods/fom/Subgradient descent.html#support-vector-machines",
    "href": "docs/methods/fom/Subgradient descent.html#support-vector-machines",
    "title": "1 Introduction",
    "section": "3.2 Support vector machines",
    "text": "3.2 Support vector machines\nLet D = \\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^n, y_i \\in \\{\\pm 1\\}\\}\nWe need to find \\omega \\in \\mathbb{R}^n and b \\in \\mathbb{R} such that\n\n\\min_{\\omega \\in \\mathbb{R}^n, b \\in \\mathbb{R}} \\dfrac{1}{2}\\|\\omega\\|_2^2 + C\\sum\\limits_{i=1}^m max[0, 1 - y_i(\\omega^\\top x_i + b)]"
  },
  {
    "objectID": "docs/methods/line_search/parabola.html",
    "href": "docs/methods/line_search/parabola.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nSampling 3 points of a function determines unique parabola. Using this information we will go directly to its minimum. Suppose, we have 3 points x_1 &lt; x_2 &lt; x_3 such that line segment [x_1, x_3] contains minimum of a function f(x). Then, we need to solve the following system of equations:\n\nax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3\n\nNote, that this system is linear, since we need to solve it on a,b,c. Minimum of this parabola will be calculated as:\n\nu = -\\dfrac{b}{2a} = x_2 - \\dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\\right]}\n\nNote, that if f_2 &lt; f_1, f_2 &lt; f_3, than u will lie in [x_1, x_3]\n\n\n2 Algorithm\ndef parabola_search(f, x1, x2, x3, epsilon):\n    f1, f2, f3 = f(x1), f(x2), f(x3)\n    while x3 - x1 &gt; epsilon:\n        u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))\n        fu = f(u)\n\n        if x2 &lt;= u:\n            if f2 &lt;= fu:\n                x1, x2, x3 = x1, x2, u\n                f1, f2, f3 = f1, f2, fu\n            else:\n                x1, x2, x3 = x2, u, x3\n                f1, f2, f3 = f2, fu, f3\n        else:\n            if fu &lt;= f2:\n                x1, x2, x3 = x1, u, x2\n                f1, f2, f3 = f1, fu, f2\n            else:\n                x1, x2, x3 = u, x2, x3\n                f1, f2, f3 = fu, f2, f3\n    return (x1 + x3) / 2\n\n\n3 Bounds\nThe convergence of this method is superlinear, but local, which means, that you can take profit from using this method only near some neighbour of optimum."
  },
  {
    "objectID": "docs/methods/line_search/binary_search.html",
    "href": "docs/methods/line_search/binary_search.html",
    "title": "1 Idea",
    "section": "",
    "text": "1 Idea\nWe divide a segment into two equal parts and choose the one that contains the solution of the problem using the values of functions. # Algorithm\ndef binary_search(f, a, b, epsilon):\n    c = (a + b) / 2\n    while abs(b - a) &gt; epsilon:\n        y = (a + c) / 2.0\n        if f(y) &lt;= f(c):\n            b = c\n            c = y\n        else:\n            z = (b + c) / 2.0\n            if f(c) &lt;= f(z):\n                a = y\n                b = z\n            else:\n                a = c\n                c = z\n    return c\n\n\n\n2 Bounds\nThe length of the line segment on k+1-th iteration:\n\n\\Delta_{k+1} = b_{k+1} - a_{k+1} = \\dfrac{1}{2^k}(b-a)\n\nFor unimodal functions, this holds if we select the middle of a segment as an output of the iteration x_{k+1}:\n\n|x_{k+1} - x_*| \\leq \\dfrac{\\Delta_{k+1}}{2} \\leq \\dfrac{1}{2^{k+1}}(b-a) \\leq (0.5)^{k+1} \\cdot (b-a)\n\nNote, that at each iteration we ask oracle no more, than 2 times, so the number of function evaluations is N = 2 \\cdot k, which implies:\n\n|x_{k+1} - x_*| \\leq (0.5)^{\\frac{N}{2}+1} \\cdot (b-a) \\leq  (0.707)^{N}  \\frac{b-a}{2}\n\nBy marking the right side of the last inequality for \\varepsilon, we get the number of method iterations needed to achieve \\varepsilon accuracy:\n\nK = \\left\\lceil \\log_2 \\dfrac{b-a}{\\varepsilon} - 1 \\right\\rceil"
  },
  {
    "objectID": "docs/methods/line_search/line_search.html",
    "href": "docs/methods/line_search/line_search.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\nSuppose, we have a problem of minimization of a function f(x): \\mathbb{R} \\to \\mathbb{R} of scalar variable:\n\nf(x) \\to \\min_{x \\in \\mathbb{R}}\n\nSometimes, we refer to the similar problem of finding minimum on the line segment [a,b]:\n\nf(x) \\to \\min_{x \\in [a,b]}\n\nLine search is one of the simplest formal optimization problems, however, it is an important link in solving more complex tasks, so it is very important to solve it effectively. Let’s restrict the class of problems under consideration where f(x) is a unimodal function.\nFunction f(x) is called unimodal on [a, b], if there is x_* \\in [a, b], that f(x_1) &gt; f(x_2) \\;\\;\\; \\forall a \\le x_1 &lt; x_2 &lt; x_* and f(x_1) &lt; f(x_2) \\;\\;\\; \\forall x_* &lt; x_1 &lt; x_2 \\leq b\n\n\n\n2 Key property of unimodal functions\nLet f(x) be unimodal function on [a, b]. Than if x_1 &lt; x_2 \\in [a, b], then: * if f(x_1) \\leq f(x_2) \\to x_* \\in [a, x_2] * if f(x_1) \\geq f(x_2) \\to x_* \\in [x_1, b] \n\n\n3 Code\nOpen In Colab{: .btn }\n\n\n4 References\n\nCMC seminars (ru)"
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "href": "docs/methods/adaptive_metrics/Natural_gradient.html",
    "title": "1 Intuition",
    "section": "",
    "text": "1 Intuition\nLet’s consider illustrative example of a simple function of 2 variables:\n\nf(x_1, x_2) = 2x_1 + \\frac{1}{3}x_2, \\quad \\nabla_x f = \\begin{pmatrix} 2\\\\ \\frac{1}{3} \\end{pmatrix}\n\nNow, let’s introduce new variables (y_1, y_2) = (2x_1, \\frac{1}{3}x_2)  or y = Bx, where B = \\begin{pmatrix} 2 & 0\\\\ 0 & \\frac{1}{3} \\end{pmatrix}. The same function, written in the new coordinates, is\n\nf(y_1, y_2) = y_1 + y_2, \\quad \\nabla_y f = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}\n\nLet’s summarize what happened: * We have a transformation of a vector space described by a coordinate transformation matrix B. * Coordinate vectors transforms as y = Bx. * However, the partial gradient of a function w.r.t. the coordinates transforms as \\frac{\\partial f}{\\partial y} = B^{-\\top} \\frac{\\partial f}{\\partial x}. * Therefore, there seems to exist one type of mathematical objects (e.g. coordinate vectors) which transform with B, and a second type of mathematical objects (e.g. the partial gradient of a function w.r.t. coordinates) which transform with B^{-\\top}.\nThese two types are called contra-variant and co-variant. This should at least tell us that indeed the so-called “gradient-vector” is somewhat different to a “normal vector”: it behaves inversely under coordinate transformations.\nNice thing here is that steepest descent direction A_x^{-1}\\nabla_x f on a sphere transforms as a covariant vector, since A_y = B^{-\\top} A_x B^{-1}:\n\n\\begin{split}\nA_y^{-1}\\nabla_y f = \\\\\n(B^{-\\top} A_x B^{-1})^{-1} B^{-\\top} \\nabla_x f = \\\\\nB A_x^{-1} B^\\top B^{-\\top} \\nabla_x f = \\\\\nB (A_x^{-1} \\nabla_x f)\n\\end{split}\n\n\n\n2 Steepest descent in distribution space\nSuppose, we have a probabilistic model represented by its likelihood p(x \\vert \\theta) . We want to maximize this likelihood function to find the most likely parameter \\theta with given observations. Equivalent formulation would be to minimize the loss function \\mathcal{L}(\\theta), which is the negative logarithm of likelihood function.\n\n\n3 Example\n\n\n4 References\n\nSome notes on gradient descent\nNatural Gradient Descent\n\n\n\n5 Code\nOpen In Colab{: .btn }"
  },
  {
    "objectID": "docs/methods/adaptive_metrics/adaptive_metric.html",
    "href": "docs/methods/adaptive_metrics/adaptive_metric.html",
    "title": "",
    "section": "",
    "text": "It is known, that antigradient -\\nabla f (x_0) is the direction of the steepest descent of the function f(x) at point x_0. However, we can introduce another concept for choosing the best direction of function decreasing.\nGiven f(x) and a point x_0. Define B_\\varepsilon(x_0) = \\{x \\in \\mathbb{R}^n : d(x, x_0) = \\varepsilon^2 \\} as the set of points with distance \\varepsilon to x_0. Here we presume the existence of a distance function d(x, x_0).\n\nx^* = \\text{arg}\\min_{x \\in B_\\varepsilon(x_0)} f(x)\n\nThen, we can define another steepest descent direction in terms of minimizer of function on a sphere:\n\ns = \\lim_{\\varepsilon \\to 0} \\frac{x^* - x_0}{\\varepsilon}\n\nLet us assume that the distance is defined locally by some metric A:\n\nd(x, x_0) = (x-x_0)^\\top A (x-x_0)\n\nLet us also consider first order Taylor approximation of a function f(x) near the point x_0:\n\n\\tag{A1}\nf(x_0 + \\delta x) \\approx f(x_0) + \\nabla f(x_0)^\\top \\delta x\n\nNow we can explicitly pose a problem of finding s, as it was stated above.\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} f(x_0 + \\delta x) \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing \\text{(A1)} it can be written as:\n\n\\begin{split}\n&\\min_{\\delta x \\in \\mathbb{R^n}} \\nabla f(x_0)^\\top \\delta x \\\\\n\\text{s.t.}\\;& \\delta x^\\top A \\delta x = \\varepsilon^2\n\\end{split}\n\nUsing Lagrange multipliers method, we can easily conclude, that the answer is:\n\n\\delta x = - \\frac{2 \\varepsilon^2}{\\nabla f (x_0)^\\top A^{-1} \\nabla f (x_0)} A^{-1} \\nabla f\n\nWhich means, that new direction of steepest descent is nothing else, but A^{-1} \\nabla f(x_0).\nIndeed, if the space is isotropic and A = I, we immediately have gradient descent formula, while Newton method uses local Hessian as a metric matrix."
  },
  {
    "objectID": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "href": "docs/methods/adaptive_metrics/Quasi_newton.html",
    "title": "1 Intuition",
    "section": "",
    "text": "1 Intuition\nFor the classic task of unconditional optimization f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n} the general scheme of iteration method is written as:\n\nx_{k+1} = x_k + \\alpha_k s_k\n\nIn the Newton method, the s_k direction (Newton’s direction) is set by the linear system solution at each step:\n\ns_k = - B_k\\nabla f(x_k), \\;\\;\\; B_k = f_{xx}^{-1}(x_k)\n\ni.e. at each iteration it is necessary to compensate hessian and gradient and resolve linear system.\nNote here that if we take a single matrix of B_k = I_n as B_k at each step, we will exactly get the gradient descent method.\nThe general scheme of quasi-Newton methods is based on the selection of the B_k matrix so that it tends in some sense at k \\to \\infty to the true value of inverted Hessian in the local optimum f_{xx}^{-1}(x_*). Let’s consider several schemes using iterative updating of B_k matrix in the following way:\n\nB_{k+1} = B_k + \\Delta B_k\n\nThen if we use Taylor’s approximation for the first order gradient, we get it:\n\n\\nabla f(x_k) - \\nabla f(x_{k+1}) \\approx f_{xx}(x_{k+1}) (x_k - x_{k+1}).\n\nNow let’s formulate our method as:\n\n\\Delta x_k = B_{k+1} \\Delta y_k, \\text{ where } \\;\\; \\Delta y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\n\nin case you set the task of finding an update \\Delta B_k:\n\n\\Delta B_k \\Delta y_k = \\Delta x_k - B_k \\Delta y_k\n\n\n\n2 Broyden method\nThe simplest option is when the amendment \\Delta B_k has a rank equal to one. Then you can look for an amendment in the form\n\n\\Delta B_k = \\mu_k q_k q_k^\\top.\n\nwhere \\mu_k is a scalar and q_k is a non-zero vector. Then mark the right side of the equation to find \\Delta B_k for \\Delta z_k:\n\n\\Delta z_k = \\Delta x_k - B_k \\Delta y_k\n\nWe get it:\n\n\\mu_k q_k q_k^\\top \\Delta y_k = \\Delta z_k\n\n\n\\left(\\mu_k \\cdot q_k^\\top \\Delta y_k\\right) q_k = \\Delta z_k\n\nA possible solution is: q_k = \\Delta z_k, \\mu_k = \\left(q_k^\\top \\Delta y_k\\right)^{-1}.\nThen an iterative amendment to Hessian’s evaluation at each iteration:\n\n\\Delta B_k = \\dfrac{(\\Delta x_k - B_k \\Delta y_k)(\\Delta x_k - B_k \\Delta y_k)^\\top}{\\langle \\Delta x_k - B_k \\Delta y_k , \\Delta y_k\\rangle}.\n\n\n\n3 Davidon–Fletcher–Powell method\n\n\\Delta B_k = \\mu_1 \\Delta x_k (\\Delta x_k)^\\top + \\mu_2 B_k \\Delta y_k (B_k \\Delta y_k)^\\top.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle}.\n\n\n\n4 Broyden–Fletcher–Goldfarb–Shanno method\n\n\\Delta B_k = Q U Q^\\top, \\quad Q = [q_1, q_2], \\quad q_1, q_2 \\in \\mathbb{R}^n, \\quad U = \\begin{pmatrix} a & c\\\\ c & b \\end{pmatrix}.\n\n\n\\Delta B_k = \\dfrac{(\\Delta x_k)(\\Delta x_k )^\\top}{\\langle \\Delta x_k , \\Delta y_k\\rangle} - \\dfrac{(B_k \\Delta y_k)( B_k \\Delta y_k)^\\top}{\\langle B_k \\Delta y_k , \\Delta y_k\\rangle} + p_k p_k^\\top.\n\n\n\n5 Code\n\nOpen In Colab\nComparison of quasi Newton methods"
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html",
    "href": "docs/theory/convex sets/Conic_sets.html",
    "title": "1 Cone",
    "section": "",
    "text": "A non-empty set S is called a cone, if:\n\n\\forall x \\in S, \\; \\theta \\ge 0 \\;\\; \\rightarrow \\;\\; \\theta x \\in S\n\n\n\n\nFigure 1: Illustration of a cone"
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#conic-combination",
    "href": "docs/theory/convex sets/Conic_sets.html#conic-combination",
    "title": "1 Cone",
    "section": "3.1 Conic combination",
    "text": "3.1 Conic combination\nLet we have x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called conic combination of x_1, x_2, \\ldots, x_k if \\theta_i \\ge 0."
  },
  {
    "objectID": "docs/theory/convex sets/Conic_sets.html#conic-hull",
    "href": "docs/theory/convex sets/Conic_sets.html#conic-hull",
    "title": "1 Cone",
    "section": "3.2 Conic hull",
    "text": "3.2 Conic hull\nThe set of all conic combinations of points in set S is called the conic hull of S:\n\n\\mathbf{cone}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\; \\theta_i \\ge 0\\right\\}\n\n\n\n\nFigure 3: Illustration of a convex hull"
  },
  {
    "objectID": "docs/theory/convex sets/index.html",
    "href": "docs/theory/convex sets/index.html",
    "title": "",
    "section": "",
    "text": "In this chapter, a variety of convex sets and related definitions are described.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html",
    "href": "docs/theory/convex sets/Convex_set.html",
    "title": "1 Definitions",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2\n\n\n\n\n\nThe set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets.\n\n\n\n\n\nLet x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0.\n\n\n\nThe set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets.\n\n\n\n\n\nThe Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex."
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#line-segment",
    "href": "docs/theory/convex sets/Convex_set.html#line-segment",
    "title": "1 Definitions",
    "section": "",
    "text": "Suppose x_1, x_2 are two points in \\mathbb{R^n}. Then the line segment between them is defined as follows:\n\nx = \\theta x_1 + (1 - \\theta)x_2, \\; \\theta \\in [0,1]\n\n\n\n\nFigure 1: Illustration of a line segment between points x_1, x_2"
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-set",
    "href": "docs/theory/convex sets/Convex_set.html#convex-set",
    "title": "1 Definitions",
    "section": "",
    "text": "The set S is called convex if for any x_1, x_2 from S the line segment between them also lies in S, i.e. \n\n\\forall \\theta \\in [0,1], \\; \\forall x_1, x_2 \\in S: \\\\ \\theta x_1 + (1- \\theta) x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn empty set and a set from a single vector are convex by definition.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAny affine set, a ray, a line segment - they all are convex sets.\n\n\n\n\n\n\n\nFigure 2: Top: examples of convex sets. Bottom: examples of non-convex sets."
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-combination",
    "href": "docs/theory/convex sets/Convex_set.html#convex-combination",
    "title": "1 Definitions",
    "section": "",
    "text": "Let x_1, x_2, \\ldots, x_k \\in S, then the point \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_k x_k is called the convex combination of points x_1, x_2, \\ldots, x_k if \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0."
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#convex-hull",
    "href": "docs/theory/convex sets/Convex_set.html#convex-hull",
    "title": "1 Definitions",
    "section": "",
    "text": "The set of all convex combinations of points from S is called the convex hull of the set S.\n\n\\mathbf{conv}(S) = \\left\\{ \\sum\\limits_{i=1}^k\\theta_i x_i \\mid x_i \\in S, \\sum\\limits_{i=1}^k\\theta_i = 1, \\; \\theta_i \\ge 0\\right\\}\n\n\nThe set \\mathbf{conv}(S) is the smallest convex set containing S.\nThe set S is convex if and only if S = \\mathbf{conv}(S).\n\nExamples:\n\n\n\nFigure 3: Top: convex hulls of the convex sets. Bottom: convex hull of the non-convex sets."
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#minkowski-addition",
    "href": "docs/theory/convex sets/Convex_set.html#minkowski-addition",
    "title": "1 Definitions",
    "section": "",
    "text": "The Minkowski sum of two sets of vectors S_1 and S_2 in Euclidean space is formed by adding each vector in S_1 to each vector in S_2:\n\nS_1+S_2=\\{\\mathbf {s_1} +\\mathbf {s_2} \\,|\\,\\mathbf {s_1} \\in S_1,\\ \\mathbf {s_2} \\in S_2\\}\n\nSimilarly, one can define a linear combination of the sets.\n\n\n\n\n\n\nExample\n\n\n\n\n\nWe will work in the \\mathbb{R}^2 space. Let’s define:\n\nS_1 := \\{x \\in \\mathbb{R}^2 : x_1^2 + x_2^2 \\leq 1\\}\n\nThis is a unit circle centered at the origin. And:\n\nS_2 := \\{x \\in \\mathbb{R}^2 : -1 \\leq x_1 \\leq 2, -3 \\leq x_2 \\leq 4\\}\n\nThis represents a rectangle. The sum of the sets S_1 and S_2 will form an enlarged rectangle S_2 with rounded corners. The resulting set will be convex."
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#by-definition",
    "href": "docs/theory/convex sets/Convex_set.html#by-definition",
    "title": "1 Definitions",
    "section": "2.1 By definition",
    "text": "2.1 By definition\n\nx_1, x_2 \\in S, \\; 0 \\le \\theta \\le 1 \\;\\; \\rightarrow \\;\\; \\theta x_1 + (1-\\theta)x_2 \\in S\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that ball in \\mathbb{R}^n (i.e. the following set \\{ \\mathbf{x} \\mid \\Vert \\mathbf{x} - \\mathbf{x}_c \\Vert \\leq r \\}) - is convex.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhich of the sets are convex:\n\nStripe, \\{x \\in \\mathbb{R}^n \\mid \\alpha \\leq a^\\top x \\leq \\beta \\}\nRectangle, \\{x \\in \\mathbb{R}^n \\mid \\alpha_i \\leq x_i \\leq \\beta_i, i = \\overline{1,n} \\}\nKleen, \\{x \\in \\mathbb{R}^n \\mid a_1^\\top x \\leq b_1, a_2^\\top x \\leq b_2 \\}\nA set of points closer to a given point than a given set that does not contain a point, \\{x \\in \\mathbb{R}^n \\mid \\Vert x - x_0\\Vert _2 \\leq \\Vert x-y\\Vert _2, \\forall y \\in S \\subseteq \\mathbb{R}^n \\}\nA set of points, which are closer to one set than another, \\{x \\in \\mathbb{R}^n \\mid \\mathbf{dist}(x,S) \\leq \\mathbf{dist}(x,T) , S,T \\subseteq \\mathbb{R}^n \\}\nA set of points, \\{x \\in \\mathbb{R}^{n} \\mid x + X \\subseteq S\\}, where S \\subseteq \\mathbb{R}^{n} is convex and X \\subseteq \\mathbb{R}^{n} is arbitrary.\nA set of points whose distance to a given point does not exceed a certain part of the distance to another given point is \\{x \\in \\mathbb{R}^n \\mid \\Vert x - a\\Vert _2 \\leq \\theta\\Vert x - b\\Vert _2, a,b \\in \\mathbb{R}^n, 0 \\leq 1 \\}"
  },
  {
    "objectID": "docs/theory/convex sets/Convex_set.html#preserving-convexity",
    "href": "docs/theory/convex sets/Convex_set.html#preserving-convexity",
    "title": "1 Definitions",
    "section": "2.2 Preserving convexity",
    "text": "2.2 Preserving convexity\n\n2.2.1 The linear combination of convex sets is convex\nLet there be 2 convex sets S_x, S_y, let the set\n\nS = \\left\\{s \\mid s = c_1 x + c_2 y, \\; x \\in S_x, \\; y \\in S_y, \\; c_1, c_2 \\in \\mathbb{R}\\right\\}\n\nTake two points from S: s_1 = c_1 x_1 + c_2 y_1, s_2 = c_1 x_2 + c_2 y_2 and prove that the segment between them \\theta s_1 + (1 - \\theta)s_2, \\theta \\in [0,1] also belongs to S\n\n\\theta s_1 + (1 - \\theta)s_2\n\n\n\\theta (c_1 x_1 + c_2 y_1) + (1 - \\theta)(c_1 x_2 + c_2 y_2)\n\n\nc_1 (\\theta x_1 + (1 - \\theta)x_2) + c_2 (\\theta y_1 + (1 - \\theta)y_2)\n\n\nc_1 x + c_2 y \\in S\n\n\n\n2.2.2 The intersection of any (!) number of convex sets is convex\nIf the desired intersection is empty or contains one point, the property is proved by definition. Otherwise, take 2 points and a segment between them. These points must lie in all intersecting sets, and since they are all convex, the segment between them lies in all sets and, therefore, in their intersection.\n\n\n2.2.3 The image of the convex set under affine mapping is convex\n\nS \\subseteq \\mathbb{R}^n \\text{ convex}\\;\\; \\rightarrow \\;\\; f(S) = \\left\\{ f(x) \\mid x \\in S \\right\\} \\text{ convex} \\;\\;\\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\nExamples of affine functions: extension, projection, transposition, set of solutions of linear matrix inequality \\left\\{ x \\mid x_1 A_1 + \\ldots + x_m A_m \\preceq B\\right\\}. Here A_i, B \\in \\mathbf{S}^p are symmetric matrices p \\times p.\nNote also that the prototype of the convex set under affine mapping is also convex.\n\nS \\subseteq \\mathbb{R}^m \\text{ convex}\\; \\rightarrow \\; f^{-1}(S) = \\left\\{ x \\in \\mathbb{R}^n \\mid f(x) \\in S \\right\\} \\text{ convex} \\;\\; \\left(f(x) = \\mathbf{A}x + \\mathbf{b}\\right)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet x \\in \\mathbb{R} is a random variable with a given probability distribution of \\mathbb{P}(x = a_i) = p_i, where i = 1, \\ldots, n, and a_1 &lt; \\ldots &lt; a_n. It is said that the probability vector of outcomes of p \\in \\mathbb{R}^n belongs to the probabilistic simplex, i.e. \n\nP = \\{ p \\mid \\mathbf{1}^Tp = 1, p \\succeq 0 \\} = \\{ p \\mid p_1 + \\ldots + p_n = 1, p_i \\ge 0 \\}.\n\nDetermine if the following sets of p are convex:\n\n\\mathbb{P}(x &gt; \\alpha) \\le \\beta\n\\mathbb{E} \\vert x^{201}\\vert \\le \\alpha \\mathbb{E}\\vert x \\vert\n\\mathbb{E} \\vert x^{2}\\vert \\ge \\alpha\\mathbb{V} x \\ge \\alpha\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "docs/theory/Duality.html",
    "href": "docs/theory/Duality.html",
    "title": "1 Duality",
    "section": "",
    "text": "Duality lets us associate to any constrained optimization problem a concave maximization problem, whose solutions lower bound the optimal value of the original problem. What is interesting is that there are cases, when one can solve the primal problem by first solving the dual one. Now, consider a general constrained optimization problem:\n\n\\text{ Primal: }f(x) \\to \\min\\limits_{x \\in S}  \\qquad \\text{ Dual: } g(y) \\to \\max\\limits_{y \\in \\Omega}\n\nWe’ll build g(y), that preserves the uniform bound:\n\ng(y) \\leq f(x) \\qquad \\forall x \\in S, \\forall y \\in \\Omega\n\nAs a consequence:\n\n\\max\\limits_{y \\in \\Omega} g(y) \\leq \\min\\limits_{x \\in S} f(x)  \n\nWe’ll consider one of many possible ways to construct g(y) in case, when we have a general mathematical programming problem with functional constraints:\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\\n& h_i(x) = 0, \\; i = 1,\\ldots, p\n\\end{split}\n\nAnd the Lagrangian, associated with this problem:\n\nL(x, \\lambda, \\nu) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) = f_0(x) + \\lambda^\\top f(x) + \\nu^\\top h(x)\n\nWe define the Lagrange dual function (or just dual function) g: \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} as the minimum value of the Lagrangian over x: for \\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p\n\ng(\\lambda, \\nu) = \\inf_{x\\in \\textbf{dom} f_0} L(x, \\lambda, \\nu) = \\inf_{x\\in \\textbf{dom} f_0} \\left( f_0(x) +\\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{i=1}^p\\nu_i h_i(x) \\right)\n\nWhen the Lagrangian is unbounded below in x, the dual function takes on the value −\\infty. Since the dual function is the pointwise infimum of a family of affine functions of (\\lambda, \\nu), it is concave, even when the original problem is not convex.\nThe dual function yields lower bounds on the optimal value p^* of the original problem. For any \\lambda \\succeq 0, \\nu:\n\ng(\\lambda, \\nu) \\leq p^*\n\nSuppose some \\hat{x} is a feasible point (\\hat{x} \\in S) for the original problem, i.e., f_i(\\hat{x}) \\leq 0 and h_i(\\hat{x}) = 0, \\; λ \\succeq 0. Then we have:\n\nL(\\hat{x}, \\lambda, \\nu) = f_0(\\hat{x}) + \\lambda^\\top f(\\hat{x}) + \\nu^\\top h(\\hat{x}) \\leq f_0(\\hat{x})\n\nHence\n\ng(\\lambda, \\nu) = \\inf_{x\\in \\textbf{dom} f_0} L(x, \\lambda, \\nu) \\leq L(\\hat{x}, \\lambda, \\nu)  \\leq f_0(\\hat{x})\n\nA natural question is: what is the best lower bound that can be obtained from the Lagrange dual function? This leads to the following optimization problem:\n\n\\begin{split}\n& g(\\lambda, \\nu) \\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\; \\nu \\in \\mathbb{R}^p }\\\\\n\\text{s.t. } & \\lambda \\succeq 0\n\\end{split}\n\nThe term “dual feasible”, to describe a pair (\\lambda, \\nu) with \\lambda \\succeq 0 and g(\\lambda, \\nu) &gt; −\\infty, now makes sense. It means, as the name implies, that (\\lambda, \\nu) is feasible for the dual problem. We refer to (\\lambda^*, \\nu^*) as dual optimal or optimal Lagrange multipliers if they are optimal for the above problem.\n\n\n\n\n\n\n\n\n\n\n\nPrimal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\textbf{dom} f_0} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in \\textbf{dom} f_0 \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\n\\begin{matrix} & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\nProblem\n\\begin{matrix}& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\ \\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\begin{matrix}  g(\\lambda, \\nu) &\\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p }\\\\ \\text{s.t. } & \\lambda \\succeq 0 \\end{matrix}\n\n\nOptimal\n\\begin{matrix} &x^* \\text{ if feasible},  \\\\ &p^* = f_0(x^*)\\end{matrix}\n\\begin{matrix} &\\lambda^*, \\nu^* \\text{ if } \\max \\text{ is achieved},  \\\\ &d^* = g(\\lambda^*, \\nu^*)\\end{matrix}\n\n\n\n\n\n\nIt is common to name this relation between optimals of primal and dual problems as weak duality. For problem, we have:\n\np^* \\geq d^*\n\nWhile the difference between them is often called duality gap:\n\np^* - d^* \\geq 0\n\nNote, that we always have weak duality, if we’ve formulated primal and dual problem. It means, that if we have managed to solve the dual problem (which is always convex, no matter whether the initial problem was or not), then we have some lower bound. Surprisingly, there are some notable cases, when these solutions are equal.\n\n\n\nStrong duality happens if duality gap is zero:\n\np^∗ = d^*\n\nNotice: both p^* and d^* may be + \\infty. * Several sufficient conditions known! * “Easy” necessary and sufficient conditions: unknown.\n\n\n\n\nConstruction of lower bound on solution of the direct problem.\nIt could be very complicated to solve the initial problem. But if we have the dual problem, we can take an arbitrary y \\in \\Omega and substitute it in g(y) - we’ll immediately obtain some lower bound.\nChecking for the problem’s solvability and attainability of the solution.\nFrom the inequality ${y } g(y) {x S} f_0(x) $ follows: if \\min\\limits_{x \\in S} f_0(x) = -\\infty, then \\Omega = \\varnothing and vice versa.\nSometimes it is easier to solve a dual problem than a primal one.\nIn this case, if the strong duality holds: g(y^∗) = f_0(x^∗) we lose nothing.\nObtaining a lower bound on the function’s residual.\nf_0(x) - f_0^∗ \\leq f_0(x) - g(y) for an arbitrary y \\in \\Omega (suboptimality certificate). Moreover, p^* \\in [g(y), f_0(x)], d^* \\in [g(y), f_0(x)]\nDual function is always concave\nAs a pointwise minimum of affine functions."
  },
  {
    "objectID": "docs/theory/Duality.html#summary",
    "href": "docs/theory/Duality.html#summary",
    "title": "1 Duality",
    "section": "",
    "text": "Primal\nDual\n\n\n\n\nFunction\nf_0(x)\ng(\\lambda, \\nu) = \\min\\limits_{x \\in \\textbf{dom} f_0} L(x, \\lambda, \\nu)\n\n\nVariables\nx \\in \\textbf{dom} f_0 \\subseteq \\mathbb{R^n}\n\\lambda \\in \\mathbb{R}^m_{+}, \\nu \\in \\mathbb{R}^p\n\n\nConstraints\n\\begin{matrix} & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\lambda_i \\geq 0, \\forall i \\in \\overline{1,m}\n\n\nProblem\n\\begin{matrix}& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n}\\\\ \\text{s.t. } & f_i(x) \\leq 0, \\; i = 1,\\ldots,m\\\\ & h_i(x) = 0, \\; i = 1,\\ldots, p \\end{matrix}\n\\begin{matrix}  g(\\lambda, \\nu) &\\to \\max\\limits_{\\lambda \\in \\mathbb{R}^m, \\nu \\in \\mathbb{R}^p }\\\\ \\text{s.t. } & \\lambda \\succeq 0 \\end{matrix}\n\n\nOptimal\n\\begin{matrix} &x^* \\text{ if feasible},  \\\\ &p^* = f_0(x^*)\\end{matrix}\n\\begin{matrix} &\\lambda^*, \\nu^* \\text{ if } \\max \\text{ is achieved},  \\\\ &d^* = g(\\lambda^*, \\nu^*)\\end{matrix}"
  },
  {
    "objectID": "docs/theory/Duality.html#weak-duality",
    "href": "docs/theory/Duality.html#weak-duality",
    "title": "1 Duality",
    "section": "",
    "text": "It is common to name this relation between optimals of primal and dual problems as weak duality. For problem, we have:\n\np^* \\geq d^*\n\nWhile the difference between them is often called duality gap:\n\np^* - d^* \\geq 0\n\nNote, that we always have weak duality, if we’ve formulated primal and dual problem. It means, that if we have managed to solve the dual problem (which is always convex, no matter whether the initial problem was or not), then we have some lower bound. Surprisingly, there are some notable cases, when these solutions are equal."
  },
  {
    "objectID": "docs/theory/Duality.html#strong-duality",
    "href": "docs/theory/Duality.html#strong-duality",
    "title": "1 Duality",
    "section": "",
    "text": "Strong duality happens if duality gap is zero:\n\np^∗ = d^*\n\nNotice: both p^* and d^* may be + \\infty. * Several sufficient conditions known! * “Easy” necessary and sufficient conditions: unknown."
  },
  {
    "objectID": "docs/theory/Duality.html#useful-features",
    "href": "docs/theory/Duality.html#useful-features",
    "title": "1 Duality",
    "section": "",
    "text": "Construction of lower bound on solution of the direct problem.\nIt could be very complicated to solve the initial problem. But if we have the dual problem, we can take an arbitrary y \\in \\Omega and substitute it in g(y) - we’ll immediately obtain some lower bound.\nChecking for the problem’s solvability and attainability of the solution.\nFrom the inequality ${y } g(y) {x S} f_0(x) $ follows: if \\min\\limits_{x \\in S} f_0(x) = -\\infty, then \\Omega = \\varnothing and vice versa.\nSometimes it is easier to solve a dual problem than a primal one.\nIn this case, if the strong duality holds: g(y^∗) = f_0(x^∗) we lose nothing.\nObtaining a lower bound on the function’s residual.\nf_0(x) - f_0^∗ \\leq f_0(x) - g(y) for an arbitrary y \\in \\Omega (suboptimality certificate). Moreover, p^* \\in [g(y), f_0(x)], d^* \\in [g(y), f_0(x)]\nDual function is always concave\nAs a pointwise minimum of affine functions."
  },
  {
    "objectID": "docs/theory/Duality.html#simple-projection-onto-simplex-with-duality",
    "href": "docs/theory/Duality.html#simple-projection-onto-simplex-with-duality",
    "title": "1 Duality",
    "section": "2.1 Simple projection onto simplex with duality",
    "text": "2.1 Simple projection onto simplex with duality\nTo find the Euclidean projection of x \\in \\mathbb{R}^n onto probability simplex \\mathcal{P} = \\{z \\in \\mathbb{R}^n \\mid z \\succeq 0, \\mathbf{1}^\\top z = 1\\}, we solve the following problem:\n\n\\begin{split}\n& \\dfrac{1}{2}\\|y - x\\|_2^2 \\to \\min\\limits_{y \\in \\mathbb{R}^{n} \\succeq 0}\\\\\n\\text{s.t. } & \\mathbf{1}^\\top y = 1\n\\end{split}\n\nHint: Consider the problem of minimizing \\dfrac{1}{2}\\|y - x\\|_2^2  subject to subject to y \\succeq 0, \\mathbf{1}^\\top y = 1. Form the partial Lagrangian\n\nL(y, \\nu) = \\dfrac{1}{2}\\|y - x\\|_2^2 +\\nu(\\mathbf{1}^\\top y - 1),\n\nleaving the constraint y \\succeq 0 implicit. Show that y = (x − \\nu \\mathbf{1})_+ minimizes L(y, \\nu) over y \\succeq 0."
  },
  {
    "objectID": "docs/theory/Duality.html#underdetermined-linear-least-squares",
    "href": "docs/theory/Duality.html#underdetermined-linear-least-squares",
    "title": "1 Duality",
    "section": "2.2 Underdetermined Linear least squares",
    "text": "2.2 Underdetermined Linear least squares\n\n2.2.1 Problem\n\n\\begin{split}\n& x^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\n\n2.2.2 Lagrangian\n\nL(x,\\nu) = x^\\top x + \\nu^\\top (Ax - b)\n\n\n\n2.2.3 Dual function\n\ng(\\nu) = \\min\\limits_{x \\in \\mathbb{R}^n} L(x, \\nu)\n\nSetting gradient to zero to find minimum 🤔:\n\n\\nabla_x L (x, \\nu) = 2x + \\nu^\\top A = 2x + A^\\top \\nu = 0 \\; \\to \\; x = -\\frac{1}{2}A^\\top \\nu\n\n\ng(\\nu) = \\frac{1}{4} \\nu^\\top A A^\\top \\nu + \\nu^\\top (-\\frac{1}{2} A A^\\top \\nu - b) = -\\frac{1}{4} \\nu^\\top A A^\\top \\nu - b^\\top \\nu\n\nHere we see lower bound property:\n\np^* \\geq -\\frac{1}{4} \\nu^\\top A A^\\top \\nu - b^\\top \\nu, \\quad \\forall \\nu\n\nLet’s solve the dual problem:\n\nd^* = b^\\top \\left( A A^\\top \\right)^{-1}b\n\nCalculate the primal optimal and check whether this problem has strong duality or not."
  },
  {
    "objectID": "docs/theory/Duality.html#lp-duality.-standard-form",
    "href": "docs/theory/Duality.html#lp-duality.-standard-form",
    "title": "1 Duality",
    "section": "2.3 LP duality. Standard form",
    "text": "2.3 LP duality. Standard form\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b \\\\\n& x \\succeq 0 \\\\\n\\end{split}"
  },
  {
    "objectID": "docs/theory/Duality.html#lp-duality.-inequality-form",
    "href": "docs/theory/Duality.html#lp-duality.-inequality-form",
    "title": "1 Duality",
    "section": "2.4 LP duality. Inequality form",
    "text": "2.4 LP duality. Inequality form\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b\n\\end{split}"
  },
  {
    "objectID": "docs/theory/Duality.html#a-nonconvex-quadratic-problem-with-strong-duality",
    "href": "docs/theory/Duality.html#a-nonconvex-quadratic-problem-with-strong-duality",
    "title": "1 Duality",
    "section": "2.5 A nonconvex quadratic problem with strong duality",
    "text": "2.5 A nonconvex quadratic problem with strong duality\nOn rare occasions strong duality obtains for a nonconvex problem. As an important example, we consider the problem of minimizing a nonconvex quadratic function over the unit ball\n\n\\begin{split}\n& x^\\top A x  + 2b^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n, A \\nsucceq 0 and b \\in \\mathbb{R}^n. Since A \\nsucceq 0, this is not a convex problem. This problem is sometimes called the trust region problem, and arises in minimizing a second-order approximation of a function over the unit ball, which is the region in which the approximation is assumed to be approximately valid.\n\n2.5.1 Lagrangian and dual function\n\nL(x, \\lambda) = x^\\top A x + 2 b^\\top x + \\lambda (x^\\top x - 1) = x^\\top( A + \\lambda I)x + 2 b^\\top x - \\lambda\n\n\ng(\\lambda) = \\begin{cases} -b^\\top(A + \\lambda I)^{\\dagger}b - \\lambda &\\text{ if } A + \\lambda I \\succeq 0 \\\\ -\\infty, &\\text{ otherwise}  \\end{cases}\n\n\n\n2.5.2 Dual problem\n\n\\begin{split}\n& -b^\\top(A + \\lambda I)^{\\dagger}b - \\lambda \\to \\max\\limits_{\\lambda \\in \\mathbb{R}}\\\\\n\\text{s.t. } & A + \\lambda I \\succeq 0\n\\end{split}\n\n\n\\begin{split}\n& -\\sum\\limits_{i=1}^n \\dfrac{(q_i^\\top b)^2}{\\lambda_i + \\lambda} - \\lambda  \\to \\max\\limits_{\\lambda \\in \\mathbb{R}}\\\\\n\\text{s.t. } & \\lambda \\geq - \\lambda_{min}(A)\n\\end{split}"
  },
  {
    "objectID": "docs/theory/Duality.html#connection-of-fenchel-and-lagrange-duality",
    "href": "docs/theory/Duality.html#connection-of-fenchel-and-lagrange-duality",
    "title": "1 Duality",
    "section": "2.6 Connection of Fenchel and Lagrange duality",
    "text": "2.6 Connection of Fenchel and Lagrange duality\n\n\\begin{split}\n& f_0(x) = \\sum_{i=1}^n f_i(x_i)\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b\n\\end{split}\n\nThe dual problem is thus\n\n\\begin{split}\n& -b \\nu - \\sum_{i=1}^n f_i^*(-\\nu a_i)  \\to \\max\\limits_{\\nu \\in \\mathbb{R}}\\\\\n\\text{s.t. } & \\lambda \\geq - \\lambda_{min}(A)\n\\end{split}\n\nwith (scalar) variable \\nu \\in \\mathbb{R}. Now suppose we have found an optimal dual variable \\nu^* (There are several simple methods for solving a convex problem with one scalar variable, such as the bisection method.). It is very easy to recover the optimal value for the primal problem."
  },
  {
    "objectID": "docs/theory/Duality.html#fenchel---rockafellar-problem",
    "href": "docs/theory/Duality.html#fenchel---rockafellar-problem",
    "title": "1 Duality",
    "section": "2.7 Fenchel - Rockafellar problem",
    "text": "2.7 Fenchel - Rockafellar problem\n\n2.7.1 Problem\nLet f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. We call Fenchel - Rockafellar problem the following minimization task:\n\nf(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\nwhere A^{-1}(G) := \\{x \\in V : Ax \\in G\\} — preimage of G. We’ll build the dual problem using variable separation. Let’s introduce new variable y = Ax. The problem could be rewritten:\n\n\\begin{split}\n& f(x) + g(y) \\to \\min\\limits_{x \\in E, \\; y \\in G }\\\\\n\\text{s.t. } & Ax = y\n\\end{split}\n\n\n\n2.7.2 Lagrangian\n\nL(x,y, \\lambda) =  f(x) + g(y) + \\lambda^\\top (Ax - y)\n\n\n\n2.7.3 Dual function\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, \\; y \\in G} L(x,y, \\lambda) \\\\\n&= \\min\\limits_{x \\in E}\\left[ f(x) + (A^*\\lambda)^\\top x \\right] + \\min\\limits_{y \\in G} \\left[g(y) - \\lambda^\\top y\\right] = \\\\\n&= -\\max\\limits_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] - \\max\\limits_{y \\in G} \\left[\\lambda^\\top y - g(y)\\right]\n\\end{split}\n\nNow, we need to remember the definition of the {% include link.html title=‘Conjugate function’%}:\n\n\\sup_{y \\in G}\\left[\\lambda^\\top y - g(y)\\right] = \\begin{cases} g^*(\\lambda), &\\text{ if } \\lambda \\in G_*\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\n\n\\sup_{x \\in E}\\left[(-A^*\\lambda)^\\top x - f(x) \\right] = \\begin{cases} f^*(-A^*\\lambda), &\\text{ if } \\lambda \\in (-A^*)^{-1}(E_*)\\\\ +\\infty, &\\text{ otherwise} \\end{cases}\n\nSo, we have:\n\n\\begin{split}\ng_d(\\lambda) &= \\min\\limits_{x \\in E, y \\in G} L(x,y, \\lambda) = \\\\\n&= \\begin{cases} -g^*(\\lambda) - f^*(-A^*\\lambda) &\\text{ if } \\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)\\\\ -\\infty, &\\text{ otherwise}  \\end{cases}\n\\end{split}\n\nwhich allows us to formulate one of the most important theorems, that connects dual problems and conjugate functions:\nFenchel - Rockafellar theorem Let f: E \\to \\mathbb{R} and g: G \\to \\mathbb{R} — function, defined on the sets E and G in Euclidian Spaces V and W respectively. Let f^*:E_* \\to \\mathbb{R}, g^*:G_* \\to \\mathbb{R} be the conjugate functions to the f and g respectively. Let A: V \\to W — linear mapping. Let p^*, d^* \\in [- \\infty, + \\infty] - optimal values of primal and dual problems:\n\np^* = f(x) + g(Ax) \\to \\min\\limits_{x \\in E \\cap A^{-1}(G)}\n\n\nd^* = f^*(-A^*\\lambda) + g^*(\\lambda) \\to \\min\\limits_{\\lambda \\in G_* \\cap (-A^*)^{-1}(E_*)},\n\nThen we have weak duality: p^* \\geq d^*. Furthermore, if the functions f and g are convex and A(\\mathbf{relint}(E)) \\cap \\mathbf{relint}(G) \\neq \\varnothing , then we have strong duality: p^* = d^*. While points x^* \\in E \\cap A^{-1}(G) and \\lambda^* \\in G_* \\cap (-A^*)^{-1}(E_*) are optimal values for primal and dual problem if and only if:\n\n\\begin{split}\n-A^*\\lambda^* &\\in \\partial f(x^*) \\\\\n\\lambda^* &\\in \\partial g(Ax^*)\n\\end{split}\n\nConvex case is especially important since if we have Fenchel - Rockafellar problem with parameters (f, g, A), than the dual problem has the form (f^*, g^*, -A^*)."
  },
  {
    "objectID": "docs/theory/Subgradient.html",
    "href": "docs/theory/Subgradient.html",
    "title": "1 Motivation",
    "section": "",
    "text": "Важным свойством непрерывной выпуклой функции f(x) является то, что в выбранной точке x_0 для всех x \\in \\text{dom } f выполнено неравенство:\n\nf(x)  \\geq f(x_0) +  \\langle g, x - x_0 \\rangle\n\nдля некоторого вектора g, то есть касательная к графику функции является глобальной оценкой снизу для функции.\n\n\nЕсли f(x) - дифференцируема, то g = \\nabla f(x_0)\nНе все непрерывные выпуклые функции дифференцируемы :)\n\nНе хочется лишаться такого вкусного свойства."
  },
  {
    "objectID": "docs/theory/Subgradient.html#moreau---rockafellar-theorem-subdifferential-of-a-linear-combination",
    "href": "docs/theory/Subgradient.html#moreau---rockafellar-theorem-subdifferential-of-a-linear-combination",
    "title": "1 Motivation",
    "section": "3.1 Moreau - Rockafellar theorem (subdifferential of a linear combination)",
    "text": "3.1 Moreau - Rockafellar theorem (subdifferential of a linear combination)\nПусть f_i(x) - выпуклые функции на выпуклых множествах S_i, \\; i = \\overline{1,n}.\nТогда, если \\bigcap\\limits_{i=1}^n \\mathbf{ri } S_i \\neq \\emptyset то функция f(x) = \\sum\\limits_{i=1}^n a_i f_i(x), \\; a_i &gt; 0 имеет субдифференциал \\partial_S f(x) на множестве S = \\bigcap\\limits_{i=1}^n S_i и\n\n\\partial_S f(x) = \\sum\\limits_{i=1}^n a_i \\partial_{S_i} f_i(x)"
  },
  {
    "objectID": "docs/theory/Subgradient.html#dubovitsky---milutin-theorem-subdifferential-of-a-point-wise-maximum",
    "href": "docs/theory/Subgradient.html#dubovitsky---milutin-theorem-subdifferential-of-a-point-wise-maximum",
    "title": "1 Motivation",
    "section": "3.2 Dubovitsky - Milutin theorem (subdifferential of a point-wise maximum)",
    "text": "3.2 Dubovitsky - Milutin theorem (subdifferential of a point-wise maximum)\nПусть f_i(x) - выпуклые функции на открытом выпуклом множестве S  \\subseteq \\mathbb{R}^n, \\; x_0 \\in S, а поточечный максимум определяется как f(x)  = \\underset{i}{\\operatorname{max}} f_i(x). Тогда:\n\n\\partial_S f(x_0) = \\mathbf{conv}\\left\\{  \\bigcup\\limits_{i \\in I(x_0)} \\partial_S f_i(x_0) \\right\\},\n\nгде I(x) = \\{ i \\in [1:m]: f_i(x) = f(x)\\}"
  },
  {
    "objectID": "docs/theory/Subgradient.html#chain-rule-for-subdifferentials",
    "href": "docs/theory/Subgradient.html#chain-rule-for-subdifferentials",
    "title": "1 Motivation",
    "section": "3.3 Chain rule for subdifferentials",
    "text": "3.3 Chain rule for subdifferentials\nПусть g_1, \\ldots, g_m - выпуклые функции на открытом выпуклом множестве S \\subseteq \\mathbb{R}^n, g = (g_1, \\ldots, g_m) - образованная из них вектор - функция, \\varphi - монотонно неубывающая выпуклая функция на открытом выпуклом множестве U \\subseteq \\mathbb{R}^m, причем g(S) \\subseteq U. Тогда субдифференциал функции f(x) = \\varphi \\left( g(x)\\right) имеет вид:\n\n\\partial f(x) = \\bigcup\\limits_{p \\in \\partial \\varphi(u)} \\left( \\sum\\limits_{i=1}^{m}p_i \\partial g_i(x) \\right),\n\nгде u = g(x)\nВ частности, если функция \\varphi дифференцируема в точке u = g(x), то формула запишется так:\n\n\\partial f(x) = \\sum\\limits_{i=1}^{m}\\dfrac{\\partial \\varphi}{\\partial u_i}(u) \\partial g_i(x)"
  },
  {
    "objectID": "docs/theory/Subgradient.html#subdifferential-calculus",
    "href": "docs/theory/Subgradient.html#subdifferential-calculus",
    "title": "1 Motivation",
    "section": "3.4 Subdifferential calculus",
    "text": "3.4 Subdifferential calculus\n\n\\partial (\\alpha f)(x) = \\alpha \\partial f(x), for \\alpha \\geq 0\n\\partial (\\sum f_i)(x) = \\sum \\partial f_i (x), f_i - выпуклые функции\n\\partial (f(Ax + b))(x) = A^T\\partial f(Ax + b) , f - выпуклая функция\nz \\in \\partial f(x) if and only if x \\in \\partial f^∗(z)."
  },
  {
    "objectID": "docs/theory/Subgradient.html#section",
    "href": "docs/theory/Subgradient.html#section",
    "title": "1 Motivation",
    "section": "4.1 1",
    "text": "4.1 1\nНайти \\partial f(x), если f(x) = |x|\nРешение:\nРешить задачу можно либо геометрически (в каждой точке числовой прямой указать угловые коэффициенты прямых, глобально подпирающих функцию снизу), либо по теореме Моро - Рокафеллара, рассмотрев f(x) как композицию выпуклых функций:\n\nf(x) = \\max\\{-x, x\\}"
  },
  {
    "objectID": "docs/theory/Subgradient.html#section-1",
    "href": "docs/theory/Subgradient.html#section-1",
    "title": "1 Motivation",
    "section": "4.2 2",
    "text": "4.2 2\nНайти \\partial f(x), если f(x) = |x - 1| + |x + 1| \nРешение:\nСовершенно аналогично применяем теорему Моро - Рокафеллара, учитывая следующее:\n\n\\partial f_1(x) = \\begin{cases} -1,  &x &lt; 1\\\\ [-1;1], \\;\\;\\;\\;\\; &x = 1 \\\\ 1,  &x &gt; 1 \\end{cases} \\qquad \\partial f_2(x) = \\begin{cases} -1,  &x &lt; -1\\\\ [-1;1], &x = -1 \\\\ 1,  &x &gt; -1  \\end{cases}\n\nТаким образом:\n\n\\partial f(x) = \\begin{cases} -2, &x &lt; -1\\\\ [-2;0], &x = -1 \\\\ 0,  &-1 &lt; x &lt; 1 \\\\ [0;2], &x = 1 \\\\ 2, &x &gt; 1 \\\\ \\end{cases}"
  },
  {
    "objectID": "docs/theory/Subgradient.html#section-2",
    "href": "docs/theory/Subgradient.html#section-2",
    "title": "1 Motivation",
    "section": "4.3 3",
    "text": "4.3 3\nНайти \\partial f(x), если f(x) = \\left[ \\max(0, f_0(x))\\right]^q. Здесь f_0(x) - выпуклая функция на открытом выпуклом множестве S, q \\geq 1.\nРешение:\nСогласно теореме о композиции (функция \\varphi (x) = x^q - дифференцируема), а g(x) = \\max(0, f_0(x)) имеем: \\partial f(x) = q(g(x))^{q-1} \\partial g(x)\nПо теореме о поточечном максимуме:\n\n\\partial g(x) = \\begin{cases} \\partial f_0(x), \\quad f_0(x) &gt; 0,\\\\ \\{0\\}, \\quad f_0(x) &lt; 0 \\\\ \\{a \\mid a = \\lambda a', \\; 0 \\le \\lambda \\le 1, \\; a' \\in \\partial f_0(x)\\}, \\;\\; f_0(x) = 0 \\end{cases}"
  },
  {
    "objectID": "docs/theory/Subgradient.html#section-3",
    "href": "docs/theory/Subgradient.html#section-3",
    "title": "1 Motivation",
    "section": "4.4 4",
    "text": "4.4 4\nНайти \\partial f(x), если f(x) = \\sin x, x \\in [\\pi/2; 2\\pi]"
  },
  {
    "objectID": "docs/theory/Subgradient.html#section-4",
    "href": "docs/theory/Subgradient.html#section-4",
    "title": "1 Motivation",
    "section": "4.5 5",
    "text": "4.5 5\nНайти \\partial f(x), если f(x) = |c_1^\\top x| + |c_2^\\top x| \nРешение: Пусть f_1(x) = |c_1^\\top x| , а f_2(x) = |c_2^\\top x| . Так как эти функции выпуклы, субдифференциал их суммы равен сумме субдифференциалов. Найдем каждый из них:\n\\partial f_1(x) = \\partial \\left( \\max \\{c_1^\\top x, -c_1^\\top x\\} \\right) = \\begin{cases} -c_1,  &c_1^\\top x &lt; 0\\\\ \\mathbf{conv}(-c_1;c_1), &c_1^\\top x = 0 \\\\ c_1,  &c_1^\\top x &gt; 0 \\end{cases} \\partial f_2(x) = \\partial \\left( \\max \\{c_2^\\top x, -c_2^\\top x\\} \\right) = \\begin{cases} -c_2,  &c_2^\\top x &lt; 0\\\\ \\mathbf{conv}(-c_2;c_2), &c_2^\\top x = 0 \\\\ c_2,  &c_2^\\top x &gt; 0 \\end{cases}\nДалее интересными представляются лишь различные взаимные расположения векторов c_1 и c_2, рассмотрение которых предлагается читателю."
  },
  {
    "objectID": "docs/theory/Subgradient.html#section-5",
    "href": "docs/theory/Subgradient.html#section-5",
    "title": "1 Motivation",
    "section": "4.6 6",
    "text": "4.6 6\nНайти \\partial f(x), если f(x) = \\| x\\|_1 \nРешение: По определению\n\n\\|x\\|_1 = |x_1| + |x_2| + \\ldots + |x_n| = s_1 x_1 + s_2 x_2 + \\ldots + s_n x_n\n\nРассмотрим эту сумму как поточечный максимум линейных функций по x: g(x) = s^\\top x, где s_i = \\{ -1, 1\\}. Каждая такая функция однозначно определяется набором коэффициентов \\{s_i\\}_{i=1}^n.\nТогда по теореме Дубовицкого - Милютина, в каждой точке \\partial f = \\mathbf{conv}\\left(\\bigcup\\limits_{i \\in I(x)} \\partial g_i(x)\\right)\nЗаметим, что \\partial g(x) = \\partial \\left( \\max \\{s^\\top x, -s^\\top x\\} \\right) = \\begin{cases} -s,  &s^\\top x &lt; 0\\\\ \\mathbf{conv}(-s;s), &s^\\top x = 0 \\\\ s,  &s^\\top x &gt; 0 \\end{cases}.\nПричем, правило выбора “активной” функции поточечного максимума в каждой точке следующее: * Если j-ая координата точки отрицательна, s_i^j = -1 * Если j-ая координата точки положительна, s_i^j = 1 * Если j-ая координата точки равна нулю, то подходят оба варианта коэффициентов и соответствующих им функций, а значит, необходимо включать субградиенты этих функций в объединение в теореме Дубовицкого - Милютина.\nВ итоге получаем ответ:\n\n\\partial f(x) = \\left\\{ g \\; : \\; \\|g\\|_\\infty \\leq 1, \\quad g^\\top x = \\|x\\|_1 \\right\\}"
  },
  {
    "objectID": "docs/theory/index.html",
    "href": "docs/theory/index.html",
    "title": "",
    "section": "",
    "text": "This chapter provides the information about foundation terms and notations for optimization.\n\n\n\n\n\n\n\n\nAffine set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConic set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConjugate set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConjugate function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDual norm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimality conditions. KKT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex optimization problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRates of convergence\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/theory/Dual norm.html",
    "href": "docs/theory/Dual norm.html",
    "title": "1 Dual norm",
    "section": "",
    "text": "1 Dual norm\nLet \\Vert x\\Vert  be the norm in the primal space x \\in S \\subseteq \\mathbb{R}^n, then the following expression defines dual norm:\n\n\\Vert x\\Vert _\\star = \\sup\\limits_{\\Vert y\\Vert  \\leq 1} \\langle y,x\\rangle\n\nThe intuition for the finite-dimensional space is how the linear function (element of the dual space) f_y(\\cdot) could stretch the elements of the primal space with respect to their size, i.e. \\Vert y\\Vert _* = \\sup\\limits_{x \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert x\\Vert }\n\n\n2 Properties\n\nOne can easily define the dual norm as:\n\n  \\Vert x\\Vert _* = \\sup\\limits_{y \\neq 0} \\dfrac{\\langle y,x\\rangle}{\\Vert y\\Vert }\n  \nThe dual norm is also a norm itself\nFor any x \\in E, y \\in E^*: x^\\top y \\leq \\Vert x\\Vert  \\cdot \\Vert y\\Vert _*\n\\left(\\Vert x\\Vert _p\\right)_* = \\Vert x\\Vert _q if \\dfrac{1}{p} + \\dfrac{1}{q} = 1, where p, q \\geq 1\n\n{: .example} The Euclidian norm is self dual \\left(\\Vert x\\Vert_2\\right)_\\star = \\Vert x\\Vert _2.\n\n\n3 Examples\n{: .example} &gt;Let $f(x) = x$. Prove that f^\\star(y) = \\mathbb{O}_{\\Vert y\\Vert _\\star \\leq 1} &gt;\n\n\nSolution"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html",
    "href": "docs/theory/Matrix_calculus.html",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note, that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?\n\n\n\n\n\n\n\nLet A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\n\n\n\n\nNorm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 4. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad (A.12)\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\n\n\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that \n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A) ,\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\nFigure 3: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\nOne can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\nFigure 4: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?\n\n\n\n\n\n\n\nThe determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det} AB = (\\text{det} A)(\\text{det} B);\n\\text{det} A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#vectors-and-matrices",
    "href": "docs/theory/Matrix_calculus.html#vectors-and-matrices",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length n is denoted by \\mathbb{R}^n, while the space of real-valued m \\times n matrices is denoted by \\mathbb{R}^{m \\times n}. That’s it: 1\n\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1} Similarly, if A \\in \\mathbb{R}^{m \\times n} we denote transposition as A^T \\in \\mathbb{R}^{n \\times m}: \nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n We will write x \\geq 0 and x \\neq 0 to indicate componentwise relationships\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\nA matrix is symmetric if A = A^T. It is denoted as A \\in \\mathbb{S}^n (set of square symmetric matrices of dimension n). Note, that only a square matrix could be symmetric by definition.\nA matrix A \\in \\mathbb{S}^n is called positive (negative) definite if for all x \\neq 0 : x^T Ax &gt; (&lt;) 0. We denote this as A \\succ (\\prec) 0. The set of such matrices is denoted as \\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\nA matrix A \\in \\mathbb{S}^n is called positive (negative) semidefinite if for all x : x^T Ax \\geq (\\leq) 0. We denote this as A \\succeq (\\preceq) 0. The set of such matrices is denoted as \\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#matrix-and-vector-product",
    "href": "docs/theory/Matrix_calculus.html#matrix-and-vector-product",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "Let A be a matrix of size m \\times n, and B be a matrix of size n \\times p, and let the product AB be: \nC = AB\n then C is a m \\times p matrix, with element (i, j) given by: \nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\nThis operation in a naive form requires \\mathcal{O}(n^3) arithmetical operations, where n is usually assumed as the largest dimension of matrices.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\mathcal{O}(n^3)? How about \\mathcal{O}(n^2), \\mathcal{O}(n)?\n\n\n\n\nLet A be a matrix of shape m \\times n, and x be n \\times 1 vector, then the i-th component of the product: \nz = Ax\n is given by: \nz_i = \\sum_{k=1}^n a_{ik}x_k\n\nRemember, that:\n\nC = AB \\quad C^T = B^T A^T\nAB \\neq BA\ne^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\ne^{A+B} \\neq e^{A} e^{B} (but if A and B are commuting matrices, which means that AB = BA, e^{A+B} = e^{A} e^{B})\n\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#norms-and-scalar-products",
    "href": "docs/theory/Matrix_calculus.html#norms-and-scalar-products",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "Norm is a qualitative measure of the smallness of a vector and is typically denoted as \\Vert x \\Vert.\nThe norm should satisfy certain properties:\n\n\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert, \\alpha \\in \\mathbb{R}\n\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert (triangle inequality)\nIf \\Vert x \\Vert = 0 then x = 0\n\nThe distance between two vectors is then defined as \nd(x, y) = \\Vert x - y \\Vert.\n The most well-known and widely used norm is Euclidean norm: \n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus.\nEuclidean norm, or 2-norm, is a subclass of an important class of p-norms:\n\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n L_1 norm (or Manhattan distance) which is defined as the sum of modules of the elements of x:\n\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\nL_1 norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here: 👨‍💻\n\n\n\nFigure 2: Balls in different norms on a plane\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n Spectral norm, \\Vert A \\Vert_2 is one of the most used matrix norms (along with the Frobenius norm).\n\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\nwhere \\sigma_1(A) is the largest singular value of the matrix A.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it true, that all matrix norms satisfy the submultiplicativity property: \\Vert AB \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert? Hint: consider Chebyshev matrix norm \\Vert A \\Vert_C = \\max\\limits_{i,j} \\vert a_{ij}\\vert.\n\n\n\n\nThe standard scalar (inner) product between vectors x and y from \\mathbb{R}^n is given by \n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\nHere x_i and y_i are the scalar i-th components of corresponding vectors.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the norm \\Vert \\cdot \\Vert and scalar product \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle and \\langle x, yB\\rangle = \\langle xB^T, y\\rangle\n\n\n\n\nThe standard scalar (inner) product between matrices X and Y from \\mathbb{R}^{m \\times n} is given by\n\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\Vert \\cdot \\Vert_F and scalar product between matrices \\langle \\cdot, \\cdot \\rangle?\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nSimplify the following expression: \n\\sum\\limits_{i=1}^n \\langle S^{-1} a_i, a_i \\rangle,\n where S = \\sum\\limits_{i=1}^n a_ia_i^T, a_i \\in \\mathbb{R}^n, \\det(S) \\neq 0\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nLet A be the matrix of columns vector a_i, therefore matrix A^T contains rows a_i^T\nNote, that, S = A A^T - it is the skeleton decomposition from vectors a_i. Also note, that A is not symmetric, while S, clearly, is.\nThe target sum is \\sum\\limits_{i=1}^n a_i^T S^{-1} a_i.\nThe most important part of this exercise lies here: we’ll present this sum as the trace of some matrix M to use trace cyclic property. \n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i = \\sum\\limits_{i=1}^n m_{ii},\n where m_{ii} - i-th diagonal element of some matrix M.\nNote, that M = A^T \\left( S^{-1} A \\right) is the product of 2 matrices, because i-th diagonal element of M is the scalar product of i-th row of the first matrix A^T and i-th column of the second matrix S^{-1} A. i-th row of matrix A^T, by definition, is a_i^T, while i-th column of the matrix S^{-1} A is clearly S^&gt;{-1} a_i.\nIndeed, m_{ii} = a_i^T S^{-1} a_i, then we can finish the exercise: \n\\begin{split}\n\\sum\\limits_{i=1}^n a_i^T S^{-1} a_i &= \\sum\\limits_{i=1}^n m_{ii} = \\text{tr} M \\\\\n&= \\text{tr} \\left( A^T S^{-1} A\\right) =  \\text{tr} \\left( AA^T S^{-1} \\right) \\\\\n&=  \\text{tr } \\left( SS^{-1} \\right) =  \\text{tr} \\left( I\\right) = n\n\\end{split}"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#eigenvalues-eigenvectors-and-the-singular-value-decomposition",
    "href": "docs/theory/Matrix_calculus.html#eigenvalues-eigenvectors-and-the-singular-value-decomposition",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "A scalar value \\lambda is an eigenvalue of the n \\times n matrix A if there is a nonzero vector q such that \nAq = \\lambda q.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The eigenvalues of this matrix can be found by solving the characteristic equation: \n\\text{det}(A - \\lambda I) = 0\n For this matrix, the eigenvalues are \\lambda_1 = 1 and \\lambda_2 = 4. These eigenvalues tell us about the scaling factors of the matrix along its principal axes.\n\n\n\n\nThe vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nA \\succeq 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq 0\n \nA \\succ 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } &gt; 0\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\nWe will just prove the first point here. The second one can be proved analogously.\n\n\\rightarrow Suppose some eigenvalue \\lambda is negative and let x denote its corresponding eigenvector. Then \nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n which contradicts the condition of A \\succeq 0.\n\\leftarrow For any symmetric matrix, we can pick a set of eigenvectors v_1, \\dots, v_n that form an orthogonal basis of \\mathbb{R}^n. Pick any x \\in \\mathbb{R}^n. \n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n here we have used the fact that v_i^T v_j = 0, for i \\neq j.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIf a matrix has all positive eigenvalues, what can we infer about the matrix’s definiteness?\n\n\n\n\nSuppose A \\in S_n, i.e., A is a real symmetric n \\times n matrix. Then A can be factorized as\n\nA = Q\\Lambda Q^T,\n\nwhere Q \\in \\mathbb{R}^{n \\times n} is orthogonal, i.e., satisfies Q^T Q = I, and \\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n). The (real) numbers \\lambda_i are the eigenvalues of A and are the roots of the characteristic polynomial \\text{det}(A - \\lambda I). The columns of Q form an orthonormal set of eigenvectors of A. The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of A. 2\nWe usually order the eigenvalues as \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n. We use the notation \\lambda_i(A) to refer to the i-th largest eigenvalue of A \\in S. We usually write the largest or maximum eigenvalue as \\lambda_1(A) = \\lambda_{\\text{max}}(A), and the least or minimum eigenvalue as \\lambda_n(A) = \\lambda_{\\text{min}}(A).\nThe largest and smallest eigenvalues satisfy\n\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\nand consequently \\forall x \\in \\mathbb{R}^n (Rayleigh quotient):\n\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\nThe condition number of a nonsingular matrix is defined as\n\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\nSuppose A \\in \\mathbb{R}^{m \\times n} with rank A = r. Then A can be factored as\n\nA = U \\Sigma V^T , \\quad (A.12)\n\nwhere U \\in \\mathbb{R}^{m \\times r} satisfies U^T U = I, V \\in \\mathbb{R}^{n \\times r} satisfies V^T V = I, and \\Sigma is a diagonal matrix with \\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r), such that\n\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\n\n\n\nThis factorization is called the singular value decomposition (SVD) of A. The columns of U are called left singular vectors of A, the columns of V are right singular vectors, and the numbers \\sigma_i are the singular values. The singular value decomposition can be written as\n\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\nwhere u_i \\in \\mathbb{R}^m are the left singular vectors, and v_i \\in \\mathbb{R}^n are the right singular vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nConsider a 2x2 matrix: \nB = \\begin{bmatrix}\n4 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n The singular value decomposition of this matrix can be represented as: \nB = U \\Sigma V^T.\n Where U and V are orthogonal matrices and \\Sigma is a diagonal matrix with the singular values on its diagonal. For this matrix, the singular values are 4 and 2, which are also the eigenvalues of the matrix.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet A \\in \\mathbb{R}^{m \\times n}, and let q := \\min\\{m, n\\}. Show that \n\\|A\\|_F^2 = \\sum_{i=1}^{q} \\sigma_i^2(A) ,\n where \\sigma_1(A) \\geq \\ldots \\geq \\sigma_q(A) \\geq 0 are the singular values of matrix A. Hint: use the connection between Frobenius norm and scalar product and SVD.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix A \\in \\mathbb{S}^n_{++}. What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\nThe latter expression refers to the fun fact: you can randomly choose r linearly independent columns of a matrix and any r linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n\n\n\nFigure 3: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of columns and rows in the Skeleton decomposition affect the accuracy of the matrix reconstruction?\n\n\n\n\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-r matrix with r \\ll n, m one needs to store \\mathcal{O}((n + m)r) \\ll nm elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form."
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#canonical-tensor-decomposition",
    "href": "docs/theory/Matrix_calculus.html#canonical-tensor-decomposition",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "One can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of r primitive tensors.\n\n\n\nFigure 4: Illustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the choice of rank in the Canonical tensor decomposition affect the accuracy and interpretability of the decomposed tensor?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#determinant-and-trace",
    "href": "docs/theory/Matrix_calculus.html#determinant-and-trace",
    "title": "1 Basic linear algebra background",
    "section": "",
    "text": "The determinant and trace can be expressed in terms of the eigenvalues\n\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\nThe determinant has several appealing (and revealing) properties. For instance,\n\n\\text{det} A = 0 if and only if A is singular;\n\\text{det} AB = (\\text{det} A)(\\text{det} B);\n\\text{det} A^{-1} = \\frac{1}{\\text{det} \\ A}.\n\nDon’t forget about the cyclic property of a trace for arbitrary matrices A, B, C, D (assuming, that all dimensions are consistent):\n\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the matrix:\n\nC = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n The determinant is \\text{det}(C) = 6 - 1 = 5, and the trace is \\text{tr}(C) = 2 + 3 = 5. The determinant gives us a measure of the volume scaling factor of the matrix, while the trace provides the sum of the eigenvalues.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhat can you say about the determinant value of a positive definite matrix?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#gradient",
    "href": "docs/theory/Matrix_calculus.html#gradient",
    "title": "1 Basic linear algebra background",
    "section": "2.1 Gradient",
    "text": "2.1 Gradient\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then vector, which contains all first-order partial derivatives:\n\n\\nabla f(x) = \\dfrac{df}{dx} = \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x_1} \\\\\n    \\frac{\\partial f}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial x_n}\n\\end{pmatrix}\n\nnamed gradient of f(x). This vector indicates the direction of the steepest ascent. Thus, vector −\\nabla f(x) means the direction of the steepest descent of the function in the point. Moreover, the gradient vector is always orthogonal to the contour line in the point.\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the gradient is: \n\\nabla f(x, y) =\n\\begin{bmatrix}\n2x \\\\\n2y \\\\\n\\end{bmatrix}\n This gradient points in the direction of the steepest ascent of the function.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the magnitude of the gradient relate to the steepness of the function?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#hessian",
    "href": "docs/theory/Matrix_calculus.html#hessian",
    "title": "1 Basic linear algebra background",
    "section": "2.2 Hessian",
    "text": "2.2 Hessian\nLet f(x):\\mathbb{R}^n→\\mathbb{R}, then matrix, containing all the second order partial derivatives:\n\nf''(x) = \\dfrac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix}\n    \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\dots  & \\frac{\\partial^2 f}{\\partial x_n \\partial x_n}\n\\end{pmatrix}\n\nIn fact, Hessian could be a tensor in such a way: \\left(f(x): \\mathbb{R}^n \\to \\mathbb{R}^m \\right) is just 3d tensor, every slice is just hessian of corresponding scalar function \\left( H\\left(f_1(x)\\right), H\\left(f_2(x)\\right), \\ldots, H\\left(f_m(x)\\right)\\right).\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x, y) = x^2 + y^2, the Hessian is:\n\nH_f(x, y) = \\begin{bmatrix} 2 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the curvature of the function in different directions.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow can the Hessian matrix be used to determine the concavity or convexity of a function?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#jacobian",
    "href": "docs/theory/Matrix_calculus.html#jacobian",
    "title": "1 Basic linear algebra background",
    "section": "2.3 Jacobian",
    "text": "2.3 Jacobian\nThe extension of the gradient of multidimensional f(x):\\mathbb{R}^n\\to\\mathbb{R}^m is the following matrix:\n\nJ_f = f'(x) = \\dfrac{df}{dx^T} = \\begin{pmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots  & \\frac{\\partial f_m}{\\partial x_n} \\\\\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots  & \\frac{\\partial f_m}{\\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots  & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function\n\nf(x, y) = \\begin{bmatrix}\nx + y \\\\\nx - y \\\\\n\\end{bmatrix},\n the Jacobian is: \nJ_f(x, y) = \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n\n\n\n\n\nThis matrix provides information about the rate of change of the function with respect to its inputs.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the Jacobian matrix relate to the gradient for scalar-valued functions?\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nCan we somehow connect those three definitions above (gradient, jacobian, and hessian) using a single correct statement?"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#summary",
    "href": "docs/theory/Matrix_calculus.html#summary",
    "title": "1 Basic linear algebra background",
    "section": "2.4 Summary",
    "text": "2.4 Summary\n\nf(x) : X \\to Y; \\qquad \\frac{\\partial f(x)}{\\partial x} \\in G\n\n\n\n\n\n\n\n\n\n\nX\nY\nG\nName\n\n\n\n\n\\mathbb{R}\n\\mathbb{R}\n\\mathbb{R}\nf'(x) (derivative)\n\n\n\\mathbb{R}^n\n\\mathbb{R}\n\\mathbb{R^n}\n\\dfrac{\\partial f}{\\partial x_i} (gradient)\n\n\n\\mathbb{R}^n\n\\mathbb{R}^m\n\\mathbb{R}^{m \\times n}\n\\dfrac{\\partial f_i}{\\partial x_j} (jacobian)\n\n\n\\mathbb{R}^{m \\times n}\n\\mathbb{R}\n\\mathbb{R}^{n \\times m}\n\\dfrac{\\partial f}{\\partial x_{ij}}"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#taylor-approximations",
    "href": "docs/theory/Matrix_calculus.html#taylor-approximations",
    "title": "1 Basic linear algebra background",
    "section": "2.5 Taylor approximations",
    "text": "2.5 Taylor approximations\nTaylor approximations provide a way to approximate functions locally by polynomials. The idea is that for a smooth function, we can approximate it by its tangent (for the first order) or by its parabola (for the second order) at a point.\n\n2.5.1 First-order Taylor approximation\nThe first-order Taylor approximation, also known as the linear approximation, is centered around some point x_0. If f: \\mathbb{R}^n \\rightarrow \\mathbb{R} is a differentiable function, then its first-order Taylor approximation is given by:\n\nf_{x_0}^I(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0)\n\nWhere:\n\nf(x_0) is the value of the function at the point x_0.\n\\nabla f(x_0) is the gradient of the function at the point x_0.\n\nIt is very usual to replace the f(x) with f_{x_0}^I(x) near the point x_0 for simple analysis of some approaches.\n\n\n\nFigure 5: First order Taylor approximation near the point x_0\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFor the function f(x) = e^x around the point x_0 = 0, the first order Taylor approximation is: \nf_{x_0}^I(x) = 1 + x\n The second-order Taylor approximation is: \nf_{x_0}^{II}(x) = 1 + x + \\frac{x^2}{2}\n These approximations provide polynomial representations of the function near the point x_0.\n\n\n\n\n\n\n2.5.2 Second-order Taylor approximation\nThe second-order Taylor approximation, also known as the quadratic approximation, includes the curvature of the function. For a twice-differentiable function f: \\mathbb{R}^n \\rightarrow \\mathbb{R}, its second-order Taylor approximation centered at some point x_0 is:\n\nf_{x_0}^{II}(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0) + \\frac{1}{2} (x - x_0)^T \\nabla^2 f(x_0) (x - x_0)\n\nWhere:\n\n\\nabla^2 f(x_0) is the Hessian matrix of f at the point x_0.\n\n\n\n\nFigure 6: Second order Taylor approximation near the point x_0\n\n\nWhen using the linear approximation of the function is not sufficient one can consider replacing the f(x) with f_{x_0}^{II}(x) near the point x_0. In general, Taylor approximations give us a way to locally approximate functions. The first-order approximation is a plane tangent to the function at the point x_0, while the second-order approximation includes the curvature and is represented by a parabola. These approximations are especially useful in optimization and numerical methods because they provide a tractable way to work with complex functions.\n\n\n\n\n\n\nExample\n\n\n\n\n\nCalculate first and second order Taylor approximation of the function f(x) = \\dfrac{1}{2}x^T A x - b^T x + c\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nWhy might one choose to use a Taylor approximation instead of the original function in certain applications?\n\n\n\n\nNote, that even the second-order approximation could become inaccurate very quickly. The code for the picture below is available here: 👨‍💻\nYour browser does not support the video tag."
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#naive-approach",
    "href": "docs/theory/Matrix_calculus.html#naive-approach",
    "title": "1 Basic linear algebra background",
    "section": "3.1 Naive approach",
    "text": "3.1 Naive approach\nThe basic idea of the naive approach is to reduce matrix/vector derivatives to the well-known scalar derivatives.  One of the most important practical tricks here is to separate indices of sum (i) and partial derivatives (k). Ignoring this simple rule tends to produce mistakes."
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#differential-approach",
    "href": "docs/theory/Matrix_calculus.html#differential-approach",
    "title": "1 Basic linear algebra background",
    "section": "3.2 Differential approach",
    "text": "3.2 Differential approach\nThe guru approach implies formulating a set of simple rules, which allows you to calculate derivatives just like in a scalar case. It might be convenient to use the differential notation here. 3\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet x \\in S be an interior point of the set S, and let D : U \\rightarrow V be a linear operator. We say that the function f is differentiable at the point x with derivative D if for all sufficiently small h \\in U the following decomposition holds: \nf(x + h) = f(x) + D[h] + o(\\|h\\|)\n If for any linear operator D : U \\rightarrow V the function f is not differentiable at the point x with derivative D, then we say that f is not differentiable at the point x.\n\n\n\n\n\n3.2.1 Differentials\nAfter obtaining the differential notation of df we can retrieve the gradient using the following formula:\n\ndf(x) = \\langle \\nabla f(x), dx\\rangle\n\nThen, if we have a differential of the above form and we need to calculate the second derivative of the matrix/vector function, we treat “old” dx as the constant dx_1, then calculate d(df) = d^2f(x)\n\nd^2f(x) = \\langle \\nabla^2 f(x) dx_1, dx\\rangle = \\langle H_f(x) dx_1, dx\\rangle\n\n\n\n3.2.2 Properties\nLet A and B be the constant matrices, while X and Y are the variables (or matrix functions).\n\ndA = 0\nd(\\alpha X) = \\alpha (dX)\nd(AXB) = A(dX )B\nd(X+Y) = dX + dY\nd(X^T) = (dX)^T\nd(XY) = (dX)Y + X(dY)\nd\\langle X, Y\\rangle = \\langle dX, Y\\rangle+ \\langle X, dY\\rangle\nd\\left( \\dfrac{X}{\\phi}\\right) = \\dfrac{\\phi dX - (d\\phi) X}{\\phi^2}\nd\\left( \\det X \\right) = \\det X \\langle X^{-T}, dX \\rangle\nd\\left(\\text{tr } X \\right) = \\langle I, dX\\rangle\ndf(g(x)) = \\dfrac{df}{dg} \\cdot dg(x)\nH = (J(\\nabla f))^T\nd(X^{-1})=-X^{-1}(dX)X^{-1}\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind \\nabla^2 f(x), if f(x) = \\dfrac12 \\langle Ax, x\\rangle - \\langle b, x\\rangle + c.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(x), if f(x) = \\ln \\langle x, Ax\\rangle.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nIt is essential for A to be positive definite, because it is a logarithm argument. So, A \\in \\mathbb{S}^n_{++}Let’s find the differential first: \n\\begin{split}\ndf &= d \\left( \\ln \\langle x, Ax\\rangle \\right) = \\dfrac{d \\left( \\langle x, Ax\\rangle \\right)}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle dx, Ax\\rangle +  \\langle x, d(Ax)\\rangle}{ \\langle x, Ax\\rangle} = \\\\\n&= \\dfrac{\\langle Ax, dx\\rangle + \\langle x, Adx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle Ax, dx\\rangle + \\langle A^T x, dx\\rangle}{ \\langle x, Ax\\rangle} = \\dfrac{\\langle (A + A^T) x, dx\\rangle}{ \\langle x, Ax\\rangle}\n\\end{split}\n\nNote, that our main goal is to derive the form df = \\langle \\cdot, dx\\rangle \ndf = \\left\\langle  \\dfrac{2 A x}{ \\langle x, Ax\\rangle} , dx\\right\\rangle\n Hence, the gradient is \\nabla f(x) = \\dfrac{2 A x}{ \\langle x, Ax\\rangle}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\Vert AX - B\\Vert_F.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind df, \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nFind the gradient \\nabla f(x) and hessian \\nabla^2f(x), if f(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "docs/theory/Matrix_calculus.html#footnotes",
    "href": "docs/theory/Matrix_calculus.html#footnotes",
    "title": "1 Basic linear algebra background",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA full introduction to applied linear algebra can be found in Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares - book by Stephen Boyd & Lieven Vandenberghe, which is indicated in the source. Also, a useful refresher for linear algebra is in Appendix A of the book Numerical Optimization by Jorge Nocedal Stephen J. Wright.↩︎\nA good cheat sheet with matrix decomposition is available at the NLA course website.↩︎\nThe most comprehensive and intuitive guide about the theory of taking matrix derivatives is presented in these notes by Dmitry Kropotov team.↩︎"
  },
  {
    "objectID": "docs/exercises/conjugate_sets.html",
    "href": "docs/exercises/conjugate_sets.html",
    "title": "1 Conjugate sets",
    "section": "",
    "text": "1 Conjugate sets\n\nProve that S^* = \\left(\\overline{S}\\right)^*\nProve that \\left( \\mathbf{conv}(S) \\right)^* = S^*\nProve that if B(0,r) is a ball of radius r at some norm with the center in zero, then \\left( B(0,r) \\right)^* = B(0,1/r)\nFind a dual cone for a monotonous non-negative cone:\n\nK = \\{ x \\in \\mathbb{R}^n \\mid x_1 \\ge x_2 \\ge \\ldots \\ge x_n \\ge 0\\}\n\nFind and sketch on the plane a conjugate set to a multi-faceted cone: S = \\mathbf{cone} \\{ (-3,1), (2,3), (4,5)\\} \nDerive the definition of the cone from the definition of the conjugate set.\nName any 3 non-trivial facts about conjugate sets.\nHow to write down a set conjugate to the polyhedron?\nDraw a conjugate set by hand for simple sets. Conjugate to zero, conjugate to the halfline, to two random points, to their convex hull, etc.\nGive examples of self-conjugate sets.\nUsing a lemma about a cone conjugate, conjugate to the sum of cones and a lemma about a cone, conjugate to the intersection of closed convex cones, prove that cones\n\nK_1 = \\{x \\in \\mathbb{R}^n \\mid x = Ay, y \\ge 0, y \\in \\mathbb{R}^m, A \\in \\mathbb{R}^{n \\times}, \\}, \\;\\; K_2 = \\{p \\in \\mathbb{R}^n \\mid A^Tp \\ge 0\\}\n\nare inter conjugated.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -4, \\;\\; -2x_1 + x_2 \\ge -4\\}\n\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge -1, \\;\\; 2x_1 - x_2 \\ge 0, \\;\\; -x_1 + 2x_2 \\ge -2\\}\n\nFind conjugate cone for the cone of positive definite (semi-definite) matrices.\nFind the conjugate cone for the exponential cone:\n\nK = \\{(x, y, z) \\mid y &gt; 0, y e^{x/y} \\leq z\\}\n\nProve that’s fair for closed convex cones:\n\n(K_1 \\cap K_2)^* = K_1^* + K_2^*\n\nFind the dual cone for the following cones:\n\n$K = \\{0\\}$\n$K = \\mathbb{R}^2$\n$K = \\{(x_1, x_2) \\mid \\vert x_1\\vert \\leq x_2\\}$\n$K = \\{(x_1, x_2) \\mid x_1 + x_2 = 0\\}$\n\nFind and sketch on the plane a conjugate set to a multifaced cone:\n\n  S = \\mathbf{conv} \\left\\{ (-4,-1), (-2,-1), (-2,1)\\right\\} + \\mathbf{cone} \\left\\{ (1,0), (2,1)\\right\\}\n\nFind and sketch on the plane a conjugate set to a polyhedra:\n\nS = \\left\\{ x \\in \\mathbb{R}^2 \\mid -3x_1 + 2x_2 \\le 7, x_1 + 5x_2 \\le 9, x_1 - x_2 \\le 3, -x_2 \\le 1\\right\\}\n\nProve that if we define the conjugate set to S as follows:\n\nS^* = \\{y \\ \\in \\mathbb{R}^n \\mid \\langle y, x\\rangle \\le 1 \\;\\; \\forall x \\in S\\},\n\nthen unit ball with the zero point as the center is the only self conjugate set in \\mathbb{R}^n.\nFind the conjugate set to the ellipsoid:\n\n  S = \\left\\{ x \\in \\mathbb{R}^n \\mid \\sum\\limits_{i = 1}^n a_i^2 x_i^2 \\le \\varepsilon^2 \\right\\}\n\nLet L be the subspace of a Euclidian space X. Prove that L^* = L^\\bot, where L^\\bot - orthogonal complement to L.\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices. Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nProve, that B_p and B_{p_*} are inter-conjugate, i.e. (B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nProve, that K_p and K_{p_*} are inter-conjugate, i.e. (K_p)^* = K_{p_*}, (K_{p_*})^* = K_p, where K_p = \\left\\{ [x, \\mu] \\in \\mathbb{R}^{n+1} : \\|x\\|_p \\leq \\mu \\right\\}, \\; 1 &lt; p &lt; \\infty is the norm cone (w.r.t. p - norm) and p, p_* are conjugated, i.e. p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\nSuppose, S = S^*. Could the set S be anything, but a unit ball? If it can, provide an example of another self-conjugate set. If it couldn’t, prove it."
  },
  {
    "objectID": "docs/exercises/zom.html",
    "href": "docs/exercises/zom.html",
    "title": "",
    "section": "",
    "text": "Implement Rastrigin function f: \\mathbb{R}^d \\to \\mathbb{R} for d = 10. link\n\nf(\\mathbf{x})=10 d+\\sum_{i=1}^{d}\\left[x_{i}^{2}-10 \\cos \\left(2 \\pi x_{i}\\right)\\right]\n\n\nConsider global optimization from here.\nPlot 4 graphs for different d from {10, 100, 1000, 10000}. On each graph you are to plot f from N_{fev} for 5 methods: basinhopping, brute, differential_evolution, shgo, dual_annealing from scipy, where N_{fev} - the number of function evaluations. This information is usually available from specific_optimizer.nfev. If you will need bounds for the optimizer, use x_i \\in [-5, 5].\nNote, that it is crucial to fix seed and to use the same starting point for fair comparison.\n\nMachine learning models often have hyperparameters. To choose optimal one between them one can use GridSearch or RandomSearch. But these algorithms computationally uneffective and don’t use any sort of information about type of optimized function. To overcome this problem one can use bayesian optimization. Using this method we optimize our model by sequentially chosing points based on prior information about function.\n\nIn this task you will use optuna package for hyperparameter optimization RandomForestClassifier. Your task is to find best Random Forest model varying at least 3 hyperparameters on iris dataset. Examples can be find here or here\n!pip install optuna\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\nimport sklearn.svm\n\nimport optuna\n\niris = sklearn.datasets.load_iris()\nx, y = iris.data, iris.target\nTry to perform hyperparameter optimization in context of any metric for imbalanced classification problem with optuna and keras. Open In Colab{: .btn }\nLet’s arrange the base stations of the wireless network optimally! Suppose you have N_{obj} = 10 clusters of 10 subscribers each. Let us use a genetic algorithm to gradually search for the optimal number and location of base stations in order to minimize the cost of arranging such stations.\nBelow is one possible implementation of the genetic algorithm.\nPopulation\nThis is a list of arrays of size [N_stations x 2]. Each individual in this case is a set of station coordinates on the plane. Generation of a random\nMutation\nDefined by the function mutation(). A mutation_rate part is selected from all individuals and a random Gaussian noise is added to the mutation_rate part of its stations. An individual with a random number of stations with random coordinates is then added to the population.\nCrossing\nDefined by children_creation() and breed(). Two sets of stations are matched with a third station, from which the even stations of one parent and the odd stations of the other are taken.\nEstimation of the value of an individual\nDefined by evaluate_generation(). The total cost corresponding to a particular individual is made up of the cost of building base stations (each cost station_cost) minus the profit from each client. The profit from each client is inversely proportional to the distance to “his” base station. Each customer joins only one (closest) base station using find_nearest_station(). In addition, the profit from each subscriber is inversely proportional to the number of subscribers at a given base station (each station has the number of subscribers stations_load connected to it). Note also that, starting from a certain proximity to the subscriber to the base station, the client’s profit ceases to grow (in our algorithm, it is the same in the radius of 0.1 from the base station, then linearly decreases).\nYour task is to come up with any modifications to the proposed procedures within the genetic algorithm so that the final quality of the algorithm is better. Suggest, describe, and test ideas for improving the algorithm.\n%matplotlib notebook\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom random import shuffle, sample\nfrom copy import deepcopy\nimport random\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n\ndef generate_problem(N_obj, N_abon_per_cluster):\n    abonents = np.zeros((N_obj*N_abon_per_cluster,2))\n    for i_obj in range(N_obj):\n        center = np.random.random(2)\n        cov    = np.random.random((2,2))*0.1\n        cov    = cov @ cov.T\n        xs, ys = np.random.multivariate_normal(center, cov, N_abon_per_cluster).T\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 0] = xs\n        abonents[i_obj*N_abon_per_cluster:(i_obj+1)*N_abon_per_cluster, 1] = ys\n    return abonents\n\ndef plot_problem(abonents):\n    plt.figure(figsize=(10,6))\n    plt.plot(abonents[:,0], abonents[:,1], 'go')\n    plt.title('The village')\n#     plt.savefig('bs_problem.svg')\n    plt.show()\n\ndef random_solution(abonents, N_solutions = 100):\n    x_min, x_max = abonents[:,0].min(), abonents[:,0].max()\n    y_min, y_max = abonents[:,1].min(), abonents[:,1].max()\n    population = []\n\n    for i_sol in range(N_solutions):\n        N_stations = int(np.random.random(1)[0]*10)+1\n        stations = np.zeros((N_stations,2))\n        stations[:,0], stations[:,1] = np.random.random(N_stations)*(x_max - x_min), np.random.random(N_stations)*(y_max - y_min)\n        population.append(stations)\n    return population\n\ndef find_nearest_station(dist_matrix):\n    return np.argmin(dist_matrix, axis=1)\n\ndef pairwise_distance(abonents, stations):\n    return cdist(abonents, stations)\n\ndef evaluate_generation(abonents, population, station_cost = 1, abonent_profit_base = 1):  \n    costs = []\n    for creature in population:\n        N_stations, N_users = len(creature), len(abonents)\n        total_cost          = N_stations*station_cost\n        dist_matrix         = pairwise_distance(abonents, creature)\n        stations_assignment = find_nearest_station(dist_matrix)\n        stations_load       = np.ones(N_stations)\n        stations_load       = np.array([1/(sum(stations_assignment == i_st)+1) for i_st, st in enumerate(stations_load)])\n\n        for i_ab, abonent in enumerate(abonents):\n            dist_to_base = dist_matrix[i_ab, stations_assignment[i_ab]]\n            total_cost  -= stations_load[stations_assignment[i_ab]]*abonent_profit_base/(max(0.1, dist_to_base))\n\n        costs.append(total_cost)\n    return np.array(costs)\n\ndef mutation(population, mutation_rate = 0.3):\n    N_creatures = len(population)\n    x_min, x_max = -1, 1\n    y_min, y_max = -1, 1\n    mutated_creatures = sample(range(N_creatures), int(mutation_rate*N_creatures))\n    for i_mut in mutated_creatures:\n        N_stations = len(population[i_mut])\n        mutated_stations = sample(range(N_stations), int(mutation_rate*N_stations))\n        for i_st_mut in mutated_stations:\n            population[i_mut][i_st_mut] += np.random.normal(0, 0.01, 2)\n\n    N_new_stations = max(1, int(random.random()*mutation_rate*N_creatures))\n    for i in range(N_new_stations):\n        new_stations = np.zeros((N_new_stations,2))\n        new_stations[:,0], new_stations[:,1] = np.random.random(N_new_stations)*(x_max - x_min), np.random.random(N_new_stations)*(y_max - y_min)\n        population.append(new_stations)\n    return population\n\ndef children_creation(parent1, parent2):\n    # whoisbatya\n    batya = random.random() &gt; 0.5\n    if batya:\n        child = np.concatenate((parent1[::2], parent2[1::2]))\n    else:\n        child = np.concatenate((parent1[1::2], parent2[::2]))\n    return np.array(child)\n\ndef breed(population):\n    new_population = deepcopy(population)\n    random.shuffle(new_population)\n    N_creatures = len(population)\n    for i in range(N_creatures//2):\n        children = children_creation(population[i], population[i+1])\n        new_population.append(children)\n    return new_population\n\ndef selection(abonents, population, offsprings = 10):\n    scores = evaluate_generation(abonents, population)\n    best = np.array(scores).argsort()[:offsprings].tolist()\n    return [population[i_b] for i_b in best], population[best[0]] \n\n\ndef let_eat_bee(N_creatures, N_generations, N_obj = 10, N_abon_per_cluster = 10):\n    abonents = generate_problem(N_obj, N_abon_per_cluster)\n\n    costs_evolution = np.zeros((N_generations, N_creatures))\n    population = random_solution(abonents, N_creatures)\n    best_creatures = []\n    for generation in range(N_generations):\n        population                = mutation(population)\n        population                = breed(population)\n        population, best_creature = selection(abonents, population, N_creatures)\n        best_creatures.append(best_creature)\n\n        costs_evolution[generation, :] = evaluate_generation(abonents, population)\n\n        # Plotting\n        x_min, x_max = 0, 1\n        y_min, y_max = 0,1\n        cost_min  = [np.min(costs_evolution[i])  for i in range(generation)]\n        cost_max  = [np.max(costs_evolution[i])  for i in range(generation)]\n        cost_mean = [np.mean(costs_evolution[i]) for i in range(generation)]\n\n        fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Topology of the best solution\", \"Cost function\"))\n        fig.update_xaxes(title_text=\"x\", range = [x_min,x_max],  row=1, col=1)\n        fig.update_yaxes(title_text=\"y\", range = [y_min,y_max], row=1, col=1)\n        fig.update_yaxes(title_text=\"Total cost\", row=1, col=2)\n        fig.update_xaxes(title_text=\"Generation\", row=1, col=2)\n\n        fig.add_trace(\n            go.Scatter(x=abonents[:, 0], y=abonents[:, 1], mode='markers', name='abonents',  marker=dict(size=5)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x=best_creatures[generation][:, 0], y=best_creatures[generation][:, 1], mode='markers', name='stations', marker=dict(size=15)),\n            row=1, col=1\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_min, name='best'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_max, name='worst'),\n            row=1, col=2\n        )\n\n        fig.add_trace(\n            go.Scatter(x = list(range(generation)), y = cost_mean, name='mean'),\n            row=1, col=2\n        )\n\n        clear_output(wait=True)\n        fig.show()\n\n    fig.write_html(\"test.html\")    \n    return costs_evolution, abonents, best_creatures\n\n\ncosts_evolution, abonents, best_creatures = let_eat_bee(200, 200)"
  },
  {
    "objectID": "docs/exercises/duality.html",
    "href": "docs/exercises/duality.html",
    "title": "1 Duality",
    "section": "",
    "text": "1 Duality\n\nToy example\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property ( p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDual vs conjugate. Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x = 0 \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate.Consider the following optimization problem\n\n\\begin{split}\n& f(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& Cx = d \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nDual vs conjugate.Consider the following optimization problem\n\n\\begin{split}\n& f_0(x) = \\sum\\limits_{i=1}^n f_i(x_i) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & a^\\top x = b \\\\\n& f_i(x) - \\text{ differentiable and strictly convex}\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\n\nNew variables. Consider an unconstrained problem of the form:\n\nf_0(Ax + b) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nAnd its equivalent reformulation:\n\n\\begin{split}\n& f_0(y) \\to \\min\\limits_{y \\in \\mathbb{R}^{m} }\\\\\n\\text{s.t. } & y = Ax + b \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problems\nFind the dual functions\nWrite down the dual problems\n\nThe weak duality inequality, d^* ≤ p^* , clearly holds when d^* = -\\infty or p^* = \\infty. Show that it holds in the other two cases as well: If p^* = −\\infty, then we must have d^* = −\\infty, and also, if d^* = \\infty, then we must have p^* = \\infty.\nExpress the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nLeast Squares. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nStandard form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nTwo-way partitioning problem. Let we have the primal problem:\n\n\\begin{split}\n& x^\\top W x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x_i^2 = 1, i = 1, \\ldots, n \\\\\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\nCan you reduce this problem to the eigenvalue problem? 🐱\n\nEntropy maximization. Let we have the primal problem:\n\n\\begin{split}\n& \\sum_i x_i \\ln x_i \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& 1^\\top x = 1 \\\\\n& x \\succ 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nMinimum volume covering ellipsoid. Let we have the primal problem:\n\n\\begin{split}\n& \\ln \\text{det} X^{-1} \\to \\min\\limits_{X \\in \\mathbb{S}^{n}_{++} }\\\\\n\\text{s.t. } & a_i^\\top X a_i \\leq 1 , i = 1, \\ldots, m\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nEquality constrained norm minimization. Let we have the primal problem:\n\n\\begin{split}\n& \\|x\\| \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nInequality form LP. Let we have the primal problem:\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax \\preceq b \\\\\n& x \\succeq 0\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nNonconvex strong duality Let we have the primal problem:\n\n\\begin{split}\n& x^\\top Ax +2b^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & x^\\top x \\leq 1 \\\\\n& A \\in \\mathbb{S}^n, A \\nsucceq 0, b \\in \\mathbb{R}^n\n\\end{split}\n\n\nFind Lagrangian of the primal problem\nFind the dual function\nWrite down the dual problem\nCheck whether problem holds strong duality or not\nWrite down the solution of the dual problem\n\nA penalty method for equality constraints. We consider the problem minimize\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere f_0(x): \\mathbb{R}^n \\to\\mathbb{R}  is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\|Ax - b\\|_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [ 1,m ] \\}. First introduce new variables y_i and equality constraints y_i = b_i − a^\\top_i x. (The solution of this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [ 1,m ]. Analytic centers have geometric applications, and play an important role in barrier methods.)"
  },
  {
    "objectID": "docs/exercises/subgradient.html",
    "href": "docs/exercises/subgradient.html",
    "title": "1 Subgradient and subdifferential",
    "section": "",
    "text": "1 Subgradient and subdifferential\n\nProve, that x_0 - is the minimum point of a convex function f(x) if and only if 0 \\in \\partial f(x_0)\nFind \\partial f(x), if f(x) = \\text{ReLU}(x) = \\max \\{0, x\\}\nFind \\partial f(x), if f(x) = \\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0, \\\\\n0.01x & \\text{otherwise}.\n\\end{cases}\nFind \\partial f(x), if f(x) = \\|x\\|_p при p = 1,2, \\infty\nFind \\partial f(x), if f(x) = \\|Ax - b\\|_1^2\nFind \\partial f(x), if f(x) = e^{\\|x\\|}. Try do the task for an arbitrary norm. At least, try \\|\\cdot\\| = \\|\\cdot\\|_{\\{2,1,\\infty\\}}.\nDescribe the connection between subgradient of a scalar function f: \\mathbb{R} \\to \\mathbb{R} and global linear lower bound, which support (tangent) the graph of the function at a point.\nWhat can we say about subdifferential of a convex function in those points, where the function is differentiable?\nDoes the subgradient coincide with the gradient of a function if the function is differentiable? Under which condition it holds?\nIf the function is convex on S, whether \\partial f(x) \\neq \\emptyset  \\;\\;\\; \\forall x \\in S always holds or not?\nFind \\partial f(x), if f(x) = x^3\nFind f(x) = \\lambda_{max} (A(x)) = \\sup\\limits_{\\|y\\|_2 = 1} y^T A(x)y, где A(x) = A_0 + x_1A_1 + \\ldots + x_nA_n, all the matrices A_i \\in \\mathbb{S}^k are symmetric and defined.\nFind subdifferential of a function f(x,y) = x^2 + xy + y^2 + 3\\vert x + y − 2\\vert at points (1,0) and (1,1).\nFind subdifferential of a function f(x) = \\sin x on the set X = [0, \\frac32 \\pi].\nFind subdifferential of a function f(x) = \\vert c^{\\top}x\\vert, \\; x \\in \\mathbb{R}^n.\nFind subdifferential of a function f(x) = \\|x\\|_1, \\; x \\in \\mathbb{R}^n.\nSuppose, that if f(x) = \\|x\\|_\\infty. Prove that \n\\partial f(0) = \\textbf{conv}\\{\\pm e_1, \\ldots , \\pm e_n\\},\n where e_i is i-th canonical basis vector (column of identity matrix)."
  },
  {
    "objectID": "docs/exercises/gop.html",
    "href": "docs/exercises/gop.html",
    "title": "1 General optimization problems",
    "section": "",
    "text": "1 General optimization problems\n\nLinear Least squares Write down exact solution of the linear least squares problem:\n\n\\|Ax-b\\|^2 \\to \\min_{x \\in \\mathbb{R}^n}, A \\in \\mathbb{R}^{m \\times n}\n\nConsider three cases:\n\n$m &lt; n$\n$m = n$\n$m &gt; n$\n\nTo successfully write a test on optimization methods, a student must spend at least \\mathrm{K} kilocalories. The evening before the test, he goes to the store to buy food for dinner. There are m items in the store, the unit price of each item is p_i, i = 1, \\ldots , m. It is also known that each item’s i-th unit gives the student energy equal to k_i kilocalories. Formulate the problem of determining the contents of a minimum value basket for the successful writing of a test. Is this a convex problem? Why?\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Ax = b\n\\end{split}\n\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & a^\\top x ≤ b,\n\\end{split}\n\nwhere a \\neq 0\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & l \\preceq x \\preceq u,\n\\end{split}\n\nwhere l \\preceq u\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered as a simplest portfolio optimization problem.\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = \\alpha, \\\\\n& 0 \\preceq x \\preceq 1,\n\\end{split}\n\nwhere \\alpha is an integer between 0 and n. What happens if \\alpha is not an integer (but satisfies 0 \\leq \\alpha \\leq n)? What if we change the equality to an inequality 1^\\top x \\leq \\alpha?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0. What is the solution if the problem is not convex (A \\notin \\mathbb{S}^n_{++}) (Hint: consider eigendecomposition of the matrix: A = Q \\mathbf{diag}(\\lambda)Q^\\top = \\sum\\limits_{i=1}^n \\lambda_i q_i q_i^\\top) and different cases of \\lambda &gt;0, \\lambda=0, \\lambda&lt;0?\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& x^\\top Bx \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & x^\\top A x \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, B \\in \\mathbb{S}^n_{+}.\nConsider the equality constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{C}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nLet X \\in \\mathbb{R}^{m \\times n} with \\text{rk} X = n, \\Omega \\in \\mathbb{S}_{++}^n, and W \\in \\mathbb{R}^{k \\times n}. Find matrix G \\in \\mathbb{R}^{k \\times m}, which solves the following optimization problem:\n\nf(G) = \\text{tr} \\left(G \\Omega G^\\top \\right) \\to \\min\\limits_{GX = W}\n 1.Consider the problem of projection some point y \\in \\mathbb{R}^n,  y \\notin \\Delta^n onto the probability simplex \\Delta^n. Find 2 ways to solve the problem numerically and compare them in terms of the total computational time, memory requirements and iteration number for n = 10, 100, 1000.\n\n\\begin{split}\n& \\|x - y \\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}"
  },
  {
    "objectID": "docs/exercises/fom.html",
    "href": "docs/exercises/fom.html",
    "title": "",
    "section": "",
    "text": "A function is said to belong to the class f \\in C^{k,p}_L (Q) if it k times is continuously differentiable on Q and the pth derivative has a Lipschitz constant L.\n\n\\|\\nabla^p f(x) - \\nabla^p f(y)\\| \\leq L \\|x-y\\|, \\qquad \\forall x,y \\in Q\n\nThe most commonly used C_L^{1,1}, C_L^{2,2} on \\mathbb{R}^n. Note that:\n\np \\leq k\nIf q \\geq k, then C_L^{q,p} \\subseteq C_L^{k,p}. The higher the order of the derivative, the stronger the constraint (fewer functions belong to the class)\n\nProve that the function belongs to the class C_L^{2,1} \\subseteq C_L^{1,1} if and only if \\forall x \\in \\mathbb{R}^n:\n\n\\||\\nabla^2 f(x)\\| \\leq L\n\nProve also that the last condition can be rewritten, without generality restriction, as follows:\n\n-L I_n \\preceq \\nabla^2 f(x) \\preceq L I_n\n\nNote: by default the Euclidean norm is used for vectors and the spectral norm is used for matrices.\nПокажите, что с помощью следующих стратегий подбора шага в градиентному спуске:\n\nПостоянный шаг h_k = \\dfrac{1}{L}\nУбывающая последовательность h_k = \\dfrac{\\alpha_k}{L}, \\quad \\alpha_k \\to 0\n\nможно получить оценку убывания функции на итерации вида:\n\nf(x_k) - f(x_{k+1}) \\geq \\dfrac{\\omega}{L}\\|\\nabla f(x_k)\\|^2\n\n\\omega &gt; 0 - некоторая константа, L - константа Липщица градиента функции\nРассмотрим функцию двух переменных:\n\nf(x_1, x_2) = x_1^2 + k x_2^2,\n\nгде k - некоторый параметр. Постройте график количества итераций, необходимых для сходимости алгоритма наискорейшего спуска (до выполнения условия \\|\\nabla f(x_k)\\| \\leq \\varepsilon = 10^{-7}) в зависимости от значения k. Рассмотрите интервал k \\in [10^{-3}; 10^3] (будет удобно использовать функцию ks = np.logspace(-3,3)) и строить график по оси абсцисс в логарифмическом масштабе plt.semilogx() или plt.loglog() для двойного лог. масштаба.\nСделайте те же графики для функции:\n\nf(x) = \\ln(1 + e^{x^\\top A x}) + \\mathbf{1}^\\top x\n\nОбъясните полученную зависимость.\nДля наглядности можете пользоваться кодом отрисовки картинок:\ndef f_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    x_1, x_2 = x\n    return x_1**2 + k*x_2**2\n\ndef df_6(x, *f_params):\n    if len(f_params) == 0:\n        k = 2\n    else:\n        k = float(f_params[0])\n    return np.array([2*x[0], 2*k*x[1]])\n\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\ndef plot_3d_function(x1, x2, f, title, *f_params, minima = None, iterations = None):\n    '''\n    '''\n    low_lim_1 = x1.min()\n    low_lim_2 = x2.min()\n    up_lim_1  = x1.max()\n    up_lim_2  = x2.max()\n\n    X1,X2 = np.meshgrid(x1, x2) # grid of point\n    Z = f((X1, X2), *f_params) # evaluation of the function on the grid\n\n    # set up a figure twice as wide as it is tall\n    fig = plt.figure(figsize=(16,7))\n    fig.suptitle(title)\n\n    #===============\n    #  First subplot\n    #===============\n    # set up the axes for the first plot\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n\n    # plot a 3D surface like in the example mplot3d/surface3d_demo\n    surf = ax.plot_surface(X1, X2, Z, rstride=1, cstride=1, \n                        cmap=cm.RdBu,linewidth=0, antialiased=False)\n\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, f(minima_), 'r*', markersize=10)\n\n\n\n    #===============\n    # Second subplot\n    #===============\n    # set up the axes for the second plot\n    ax = fig.add_subplot(1, 2, 2)\n\n    # plot a 3D wireframe like in the example mplot3d/wire3d_demo\n    im = ax.imshow(Z,cmap=plt.cm.RdBu,  extent=[low_lim_1, up_lim_1, low_lim_2, up_lim_2])\n    cset = ax.contour(x1, x2,Z,linewidths=2,cmap=plt.cm.Set2)\n    ax.clabel(cset,inline=True,fmt='%1.1f',fontsize=10)\n    fig.colorbar(im)\n    ax.set_xlabel(f'$x_1$')\n    ax.set_ylabel(f'$x_2$')\n\n    if minima is not None:\n        minima_ = np.array(minima).reshape(-1, 1)\n        ax.plot(*minima_, 'r*', markersize=10)\n\n    if iterations is not None:\n        for point in iterations:\n            ax.plot(*point, 'go', markersize=3)\n        iterations = np.array(iterations).T\n        ax.quiver(iterations[0,:-1], iterations[1,:-1], iterations[0,1:]-iterations[0,:-1], iterations[1,1:]-iterations[1,:-1], scale_units='xy', angles='xy', scale=1, color='blue')\n\n    plt.show()\n\nup_lim  = 4\nlow_lim = -up_lim\nx1 = np.arange(low_lim, up_lim, 0.1)\nx2 = np.arange(low_lim, up_lim, 0.1)\nk=0.5\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0])\n\nfrom scipy.optimize import minimize_scalar\n\ndef steepest_descent(x_0, f, df, *f_params, df_eps = 1e-2, max_iter = 1000):\n    iterations = []\n    x = np.array(x_0)\n    iterations.append(x)\n    while np.linalg.norm(df(x, *f_params)) &gt; df_eps and len(iterations) &lt;= max_iter:\n        res = minimize_scalar(lambda alpha: f(x - alpha * df(x, *f_params), *f_params))\n        alpha_opt = res.x\n        x = x - alpha_opt * df(x, *f_params)\n        iterations.append(x)\n    print(f'Finished with {len(iterations)} iterations')\n    return iterations\n\nx_0 = [10,1]\nk = 30\niterations = steepest_descent(x_0, f_6, df_6, k, df_eps = 1e-9)\ntitle = f'$f(x_1, x_2) = x_1^2 + k x_2^2, k = {k}$'\n\nplot_3d_function(x1, x2, f_6, title, k, minima=[0,0], iterations = iterations)\nSolve the Hobbit Village problem. Open In Colab{: .btn }\nSolve the problem of constrained optimization using projected gradient descent Open In Colab{: .btn }"
  },
  {
    "objectID": "docs/exercises/separation.html",
    "href": "docs/exercises/separation.html",
    "title": "1 Separation",
    "section": "",
    "text": "1 Separation\n\nLet S_1, S_2 be closed convex sets such that: S_1 \\cap S_2 = \\varnothing. Is it true that \\exists p: (p,x) &lt; (p,y) \\;\\; \\forall x \\in S_1, \\forall y \\in S_2\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_1 x_2 \\ge 1, x_1 &gt; 0\\right\\}, \\;\\;\\; S_2 = \\left\\{ x \\in \\mathbb{R}^2 \\mid x_2 \\le \\frac{4}{x_1 - 1} +9\\right\\}\n\nFind a supporting hyperplane for a set of S = \\left\\{ x \\in \\mathbb{R}^2 \\mid e^{x_1} \\le x_2\\right\\} at the boundary point x_0 = (0, 1)\nFind a supporting hyperplane for the set of S = \\left\\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2\\right\\} such that separates it from the point x_0 = \\left(-\\frac{5}{4}, \\frac{5}{16}, \\frac{15}{16}\\right)\nПриведите пример двух строго, но не сильно отделимых множеств. Двух отделимых, но не собственно отделимых множеств.\nIllustrate the difference between the tangent hyperplane and the separating hyperplane by considering a convex set with a non-smooth boundary.\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid \\dfrac{x_1^2}{4} + \\dfrac{x_2^2}{9} + \\dfrac{x_3^2}{25} \\le 1 \\} at (-6/5, 12/5, 0), (0, 9/5, 4), (6/5, 0, -4) (any choice)\nDerive the equation of the supporting hyperplane of the set of \\{ x \\in \\mathbb{R}^3 \\mid x_3 \\ge x_1^2 + x_2^2 \\}, which separates it from the points (5/4, 5/16, 15/16), (4/3, 2/3, 13/12), (11/9, 11/27, 1) (any choice)\nFind a separating hyperplane between S_1 and S_2:\n\nS_1 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_n^2 \\le 1\\right\\}, \\;\\;\\; S_2 = \\left\\{ x \\in \\mathbb{R}^n \\mid x_1^2 + x_2^2 + \\ldots + x_{n-1}^2 + 1 \\le x_n \\right\\}\n\nFind a supporting hyperplane for a set S = \\left\\{ x \\in \\mathbb{R}^3 \\mid \\frac{x_1^2}{4}+\\frac{x_2^2}{8}+\\frac{x_3^2}{25} \\le 1 \\right\\} at the border point x_0 = \\left(-1, \\frac{12}{5}, \\frac{\\sqrt{3}}{2}\\right)"
  },
  {
    "objectID": "docs/exercises/matrix_calculus.html",
    "href": "docs/exercises/matrix_calculus.html",
    "title": "1 Matrix calculus",
    "section": "",
    "text": "1 Matrix calculus\n\nFind the derivatives of f(x) = Ax, \\quad \\nabla_x f(x) = ?, \\nabla_A f(x) = ?\nFind \\nabla f(x), if f(x) = c^Tx.\nFind \\nabla f(x), if f(x) = \\dfrac{1}{2}x^TAx + b^Tx + c.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = -e^{-x^Tx}.\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert ^2_2.\nFind \\nabla f(x), if f(x) = \\Vert x\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), if f(x) = \\Vert Ax\\Vert _2 , x \\in \\mathbb{R}^p \\setminus \\{0\\}.\nFind \\nabla f(x), f^{\\prime\\prime}(x), if f(x) = \\dfrac{-1}{1 + x^T x}.\nCalculate df(x) and \\nabla f(x) for the function f(x) = \\log(x^{T}\\mathrm{A}x).\nFind f^\\prime(X), if f(X) = \\det X\nNote: here under f^\\prime(X) assumes first order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X)\n\nFind f^{\\prime\\prime}(X), if f(X) = \\log \\det X\nNote: here under f^{\\prime\\prime}(X) assumes second order approximation of f(X) using Taylor series:\n\nf(X + \\Delta X) \\approx f(X) + \\mathbf{tr}(f^\\prime(X)^T \\Delta X) + \\frac{1}{2}\\mathbf{tr}(\\Delta X^T f^{\\prime\\prime}(X) \\Delta X)\n\nFind gradient and hessian of f : \\mathbb{R}^n \\to \\mathbb{R}, if:\n\nf(x) = \\log \\sum\\limits_{i=1}^m \\exp (a_i^T x + b_i), \\;\\;\\;\\; a_1, \\ldots, a_m \\in \\mathbb{R}^n; \\;\\;\\;  b_1, \\ldots, b_m  \\in \\mathbb{R}\n\nWhat is the gradient, Jacobian, Hessian? Is there any connection between those three definitions?\nCalculate: \\dfrac{\\partial }{\\partial X} \\sum \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X} \\prod \\text{eig}(X), \\;\\;\\dfrac{\\partial }{\\partial X}\\text{tr}(X), \\;\\; \\dfrac{\\partial }{\\partial X} \\text{det}(X)\nCalculate the Frobenious norm derivative: \\dfrac{\\partial}{\\partial X}\\Vert X\\Vert _F^2\nCalculate the gradient of the softmax regression \\nabla_\\theta L in binary case (K = 2) n - dimensional objects:\n\nh_\\theta(x) = \\begin{bmatrix} P(y = 1 \\vert x; \\theta) \\\\ P(y = 2 \\vert x; \\theta) \\\\ \\vdots \\\\ P(y = K \\vert x; \\theta) \\end{bmatrix} = \\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)T} x) }} \\begin{bmatrix} \\exp(\\theta^{(1)T} x ) \\\\ \\exp(\\theta^{(2)T} x ) \\\\ \\vdots \\\\ \\exp(\\theta^{(K)T} x ) \\\\ \\end{bmatrix}\n\n\nL(\\theta) = - \\left[ \\sum_{i=1}^n  (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right]\n\nFind \\nabla f(X), if f(X) = \\text{tr } AX\nFind \\nabla f(X), if f(X) = \\langle S, X\\rangle - \\log \\det X\nFind \\nabla f(X), if f(X) = \\ln \\langle Ax, x\\rangle, A \\in \\mathbb{S^n_{++}}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if\n\nf(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right)\n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{3}\\Vert x\\Vert _2^3\nCalculate \\nabla f(X), if f(X) = \\Vert AX - B\\Vert _F, X \\in \\mathbb{R}^{k \\times n}, A \\in \\mathbb{R}^{m \\times k}, B \\in \\mathbb{R}^{m \\times n}\nCalculate the derivatives of the loss function with respect to parameters \\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial b} for the single object x_i (or, n = 1) \nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\langle x, x\\rangle^{\\langle x, x\\rangle}, x \\in \\mathbb{R}^p \\setminus \\{0\\}\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{\\langle Ax, x\\rangle}{\\Vert x\\Vert _2^2}, x \\in \\mathbb{R}^p \\setminus \\{0\\}, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert ^2_F, A \\in \\mathbb{S}^n\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\Vert xx^T\\Vert _2\nFind the gradient \\nabla f(x) and hessian f^{\\prime\\prime}(x), if f(x) = \\frac1n \\sum\\limits_{i=1}^n \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i \\in \\mathbb R^n, \\; \\mu&gt;0.\nMatch functions with their gradients:\n\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}\nf(\\mathrm{X}) = \\mathrm{Tr}\\mathrm{X}^{-1}\nf(\\mathrm{X}) = \\det \\mathrm{X}\nf(\\mathrm{X}) = \\ln \\det \\mathrm{X}\n\n\n\\nabla f(\\mathrm{X}) = \\mathrm{X}^{-1}\n\\nabla f(\\mathrm{X}) = \\mathrm{I}\n\\nabla f(\\mathrm{X}) = \\det (\\mathrm{X})\\cdot (\\mathrm{X}^{-1})^{T}\n\\nabla f(\\mathrm{X}) = -\\left(\\mathrm{X}^{-2}\\right)^{T}\n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n$ f(t) = (A − tI_n), $\nwhere $A ^{n n}, S := {t : (A − tI_n) } $.\n\nFind the gradient \\nabla f(x), if f(x) = \\text{tr}\\left( AX^2BX^{-T} \\right)."
  },
  {
    "objectID": "docs/exercises/line_search.html",
    "href": "docs/exercises/line_search.html",
    "title": "",
    "section": "",
    "text": "Which function is called unimodal?\nDerive the convergence speed for a dichotomy method for a unimodal function. What type of convergence does this method have?\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0].  Consider the following modification of solution localization method, in which the interval [a,b] is divided into 2 parts in a fixed proportion of t: x_t = a + t*(b-a) (maximum twice at iteration - as in the dichotomy method). Experiment with different values of t \\in [0,1] and plot the dependence of N (t) - the number of iterations needed to achieve \\varepsilon - accuracy from the t parameter. Consider \\varepsilon = 10^{-7}. Note that with t = 0.5 this method is exactly the same as the dichotomy method.\nDescribe the idea of successive parabolic interpolation. What type of convergence does this method have?\nWrite down Armijo–Goldstein condition.\nShow that if 0 &lt; c_2 &lt; c_1 &lt; 1, there may be no step lengths that satisfy the Wolfe conditions (sufficient decrease and curvature condition).\nShow that the one-dimensional minimizer of a strongly convex quadratic function always satisfies the Goldstein conditions.\nConsider the Rosenbrock function:\n\nf(x_1, x_2) =  10(x_2 − x_1^2)^2 + (x_1 − 1)^22\n\nYou are given the starting point x_0 = (-1, 2)^\\top. Implement the gradient descent algorithm:\n\nx^{k+1} = x^k - \\alpha^k \\nabla f(x^k),\n\nwhere the stepsize is choosen at each iteration via solution of the following line search problem\n\n\\alpha^k = \\arg\\min\\limits_{\\alpha \\in \\mathbb{R}^+}{f(x^k - \\alpha \\nabla f(x^k))}.\n\nImplement any line search method in this problem and plot 2 graphs: function value from iteration number and function value from the number of function calls (calculate only the function calls, don’t include the gradient calls).\nConsider the function f(x) = (x + \\sin x) e^x, \\;\\;\\; x \\in [-20, 0] \n\nImplement golden search and binary search methods for this function.\nMinimize the function with these two methods and add Brent method from scipy. Compare 3 methods in terms of iterations, time, number of oracle calls."
  },
  {
    "objectID": "docs/benchmarks/linear_least_squares.html",
    "href": "docs/benchmarks/linear_least_squares.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\nIn a least-squares, or linear regression, problem, we have measurements  A \\in \\mathbb{R}^{m \\times n}  and  b \\in \\mathbb{R}^{m}  and seek a vector  x \\in \\mathbb{R}^{n}  such that  A x  is close to  b . Closeness is defined as the sum of the squared differences:\n\nf(x) = \\|Ax - b\\|_2^2 \\to \\min_{x \\in \\mathbb{R^n}}"
  },
  {
    "objectID": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "href": "docs/benchmarks/CNN_on_Fashion_MNIST.html",
    "title": "",
    "section": "",
    "text": "This chapter is WIP. We will make interactive graphs with benchmarx library, which allows to benchmark different optimizers in a convenient, reproducible way.\n{% include_relative CNN_fashion_mnist.html %}"
  },
  {
    "objectID": "docs/applications/knapsack_problem.html",
    "href": "docs/applications/knapsack_problem.html",
    "title": "1 Introduction",
    "section": "",
    "text": "The knapsack problem or rucksack problem is a problem in combinatorial optimization. Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\n\n\nWikipedia\n\n\n\n\nThe most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases."
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#definitions",
    "href": "docs/applications/knapsack_problem.html#definitions",
    "title": "1 Introduction",
    "section": "",
    "text": "The most common problem is the 0-1 knapsack problem, which restricts the number x_{i} of copies of each kind of item to zero or one. Given a set of n items numbered from 1 up to n, each with a weigh w_{i} and a value v_{i}, along with a maximum weight capacity W,\n\n\\begin{split}\n& \\max\\sum_{i=1}^n v_ix_i\\\\\n\\text{s.t. } & \\sum_{i=1}^n w_ix_i\\le W,\\;x_i \\in {0,1}\\\\\n\\end{split}\n\nThe decision problem form of the knapsack problem (Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases."
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#exact-solutions",
    "href": "docs/applications/knapsack_problem.html#exact-solutions",
    "title": "1 Introduction",
    "section": "2.1 Exact solutions",
    "text": "2.1 Exact solutions\n\n2.1.1 Full search\nAs for other discrete tasks, the backpack problem can be solved by completely sorting through all possible solutions. Suppose there are n items that can be packed in a backpack. It is necessary to determine the maximum value of the cargo, whose weight does not exceed W. For each item, there are 2 options: the item is either put in a backpack or not. Then enumeration of all possible options has time complexity O(2^n), which allows it to be used only for a small number of objects. With an increase in the number of objects, the task becomes unsolvable by this method in an acceptable time.\n\n\n2.1.2 Dynamic programming algorithm\nA similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume w_{1},\\,w_{2},\\,\\ldots ,\\,w_{n}, W are strictly positive integers. Define m(i,w) to be the maximum value that can be attained with weight less than or equal to w using items up to i (first i items). We can define m(i,w) recursively as follows:\n\nm(i, w) =\n\\begin{cases}\n   m(0,w)=0  \\\\\n   m(i,w)=m(i-1,w) &\\text{$w_{i}&gt;w$ } \\\\\n   m(i,w)=max(m(i-1,w), m(i-1,w-w_{i})+w_{i}) &\\text{$w_{i} \\le w$ }\n\\end{cases}\n\nThe solution can then be found by calculating m(n,W). To do this efficiently, we can use a table to store previous computations. This solution will therefore run in O(nW) time and O(nW) space."
  },
  {
    "objectID": "docs/applications/knapsack_problem.html#approximation-algorithms",
    "href": "docs/applications/knapsack_problem.html#approximation-algorithms",
    "title": "1 Introduction",
    "section": "2.2 Approximation algorithms",
    "text": "2.2 Approximation algorithms\n\n2.2.1 Greedy algorithm\nTo solve the problem by the greedy algorithm, it is necessary to sort things by their specific value (that is, the ratio of the value of an item to its weight), and put items with the highest specific value in a backpack. The running time of this algorithm is the sum of the sorting time and stacking time. The difficulty in sorting items is O(N \\ log (N)). Next, the calculation of how many items fit in a backpack for the total time O(N). Total complexity O(N \\ log (N)) if necessary sorting and O(N) if already sorted data. It should be understood that a greedy algorithm can lead to an answer arbitrarily far from optimal. For example, if one item has a weight of 1 and a cost of 2, and another has a weight of W and a cost of W, then the greedy algorithm will pick up the final cost of 2 with the optimal answer W.\n\n\n2.2.2 Probabilistic algorithm\nIt is a modification of greedy algorithm. In this algorithm decision to include an item with index j in the knapsack is taken based on th probability of \\frac{\\lambda_j}{\\sum_{i=1}^n\\lambda_i}, where \\lambda_i is the ratio of the value of an item to its weight. This algorithm is run several times and the best solution is selected. If Algorithm starts as many times as you like, then the probability of getting it as a result the work of the optimal solution tends to 1. Total complexity is equal to O(mN log(N)) operations. Calculating experiment The above algorithms were implemented in the python programming language, their source codes are available at the link below. Initial data for the task, namely the values (v_{i}) and weights (w_{i}) of items were randomly generated. The following ranges of values have been selected v_{i}\\in [0, 100] and w_{i}\\in [100, 200]. Maximum weight capacity W generated randomly from interval W \\in [0.5\\sum w_i, 0.75\\sum w_i]."
  },
  {
    "objectID": "docs/applications/pca.html",
    "href": "docs/applications/pca.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Imagine, that you have a dataset of points. Your goal is to choose orthogonal axes, that describe your data the most informative way. To be precise, we choose first axis in such a way, that maximize the variance (expressiveness) of the projected data. All the following axes have to be orthogonal to the previously chosen ones, while satisfy largest possible variance of the projections.\nLet’s take a look at the simple 2d data. We have a set of blue points on the plane. We can easily see that the projections on the first axis (red dots) have maximum variance at the final position of the animation. The second (and the last) axis should be orthogonal to the previous one.  source\nThis idea could be used in a variety of ways. For example, it might happen, that projection of complex data on the principal plane (only 2 components) bring you enough intuition for clustering. The picture below plots projection of the labeled dataset onto the first to principal components (PCs), we can clearly see, that only two vectors (these PCs) would be enough to differ Finnish people from Italian in particular dataset (celiac disease (Dubois et al. 2010))  source"
  },
  {
    "objectID": "docs/applications/pca.html#iris-dataset",
    "href": "docs/applications/pca.html#iris-dataset",
    "title": "1 Intuition",
    "section": "3.1 🌼 Iris dataset",
    "text": "3.1 🌼 Iris dataset\nConsider the classical Iris dataset  source We have the dataset matrix A \\in \\mathbb{R}^{150 \\times 4}"
  },
  {
    "objectID": "docs/applications/ellipsoid.html",
    "href": "docs/applications/ellipsoid.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\n\nLet x_1, \\ldots, x_n be the points in \\mathbb{R}^2. Given these points we need to find an ellipsoid, that contains all points with the minimum volume (in 2d case volume of an ellipsoid is just the square).\nAn invertible linear transformation applied to a unit sphere produces an ellipsoid with the square, that is \\det A^{-1} times bigger, than the unit sphere square, that’s why we parametrize the interior of ellipsoid in the following way:\n\nS = \\{x \\in \\mathbb{R}^2 \\; | \\; u = Ax + b, \\|u\\|_2^2 \\leq 1\\}\n\nSadly, the determinant is the function, which is relatively hard to minimize explicitly. However, the function \\log \\det A^{-1} = -\\log \\det A is actually convex, which provides a great opportunity to work with it. As soon as we need to cover all the points with ellipsoid of minimum volume, we pose an optimization problem on the convex function with convex restrictions:\n\n\\begin{align*}\n& \\min_{A \\in \\mathbb{R}^{2 \\times 2}, b \\in \\mathbb{R}^{2}} -\\log\\det(A)\\\\\n\\text{s.t. } & \\|Ax_i + b\\| \\leq 1, i = 1, \\ldots, n\\\\\n& A \\succ 0\n\\end{align*}\n\n\n\n\n2 Code\nOpen In Colab{: .btn }\n\n\n3 References\n\nJupyter notebook by A. Katrutsa\nhttps://cvxopt.org/examples/book/ellipsoids.html"
  },
  {
    "objectID": "docs/applications/deep_learning.html",
    "href": "docs/applications/deep_learning.html",
    "title": "1 Problem",
    "section": "",
    "text": "A lot of practical tasks nowadays are being solved using the deep learning approach, which is usually implies finding local minimum of a non-convex function, that generalizes well (enough 😉). The goal of this short text is to show you the importance of the optimization behind neural network training.\n\n\nOne of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach."
  },
  {
    "objectID": "docs/applications/deep_learning.html#cross-entropy",
    "href": "docs/applications/deep_learning.html#cross-entropy",
    "title": "1 Problem",
    "section": "",
    "text": "One of the most commonly used loss functions in classification tasks is the normalized categorical cross-entropy in K class problem:\n\nL(\\theta) = - \\dfrac{1}{n}\\sum_{i=1}^n (y_i^\\top\\log(h_\\theta(x_i)) + (1 - y_i)^\\top\\log(1 - h_\\theta(x_i))), \\qquad h_\\theta^k(x_i) = \\dfrac{e^{\\theta_k^\\top x_i}}{\\sum_{j = 1}^K e^{\\theta_j^\\top x_i}}\n\nSince in Deep Learning tasks the number of points in a dataset could be really huge, we usually use {%include link.html title=‘Stochastic gradient descent’%} based approaches as a workhorse.\nIn such algorithms one uses the estimation of a gradient at each step instead of the full gradient vector, for example, in cross-entropy we have:\n\n\\nabla_\\theta L(\\theta) = \\dfrac{1}{n} \\sum\\limits_{i=1}^n \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\n\nThe simplest approximation is statistically judged unbiased estimation of a gradient:\n\ng(\\theta) = \\dfrac{1}{b} \\sum\\limits_{i=1}^b \\left( h_\\theta(x_i) - y_i \\right) x_i^\\top\\approx \\nabla_\\theta L(\\theta)\n\nwhere we initially sample randomly only b \\ll n points and calculate sample average. It can be also considered as a noisy version of the full gradient approach."
  },
  {
    "objectID": "docs/applications/rendezvous.html",
    "href": "docs/applications/rendezvous.html",
    "title": "1 Problem",
    "section": "",
    "text": "1 Problem\n\nWe have two bodies in discrete time: the first is described by its coordinate x_i and its speed v_i, the second has coordinate z_i and speed u_i. Each body has its own dynamics, which we denote as linear systems with matrices A, B, C, D:\n\n\\begin{align*}\nx_{i+1} = Ax_i + Bu_i \\\\\nz_{i+1} = Cz_i + Dv_i\n\\end{align*}\n\nWe want these bodies to meet in future at some point T in such a way, that preserve minimum energy through the path. We will consider only kinetic energy, which is proportional to the squared speed at each point of time, that’s why optimization problem takes the following form:\n\n\\begin{align*}\n& \\min \\sum_{i=1}^T \\|u_i\\|_2^2 + \\|v_i\\|_2^2 \\\\\n\\text{s.t. } & x_{t+1} = Ax_t + Bu_t, \\; t = 1,\\ldots,T-1\\\\\n& z_{t+1} = Cz_t + Dv_t, \\; t = 1,\\ldots,T-1\\\\\n& x_T = z_T\n\\end{align*}\n\nProblem of this type arise in space engineering - just imagine, that the first body is the spaceship, while the second, say, Mars.\n\n\n2 Code\nOpen In Colab{: .btn }\n\n\n3 References\n\nJupyter notebook by A. Katrutsa"
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html",
    "href": "docs/applications/total_variation_inpainting.html",
    "title": "1 Problem",
    "section": "",
    "text": "A grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:\n\n\n\n\nFor the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):  \nIt is not that easy, right?  \nOnly 5% of all pixels are left:\n \nWhat about 1% of all pixels?"
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#grayscale-image",
    "href": "docs/applications/total_variation_inpainting.html#grayscale-image",
    "title": "1 Problem",
    "section": "",
    "text": "A grayscale image is represented as an m \\times n matrix of intensities U^{orig} (typically between the values 0 and 255). We are given all the values of corrupted picture, but some of them should be preserved as is through the recovering procedure: U^{corr}_{ij} \\; \\forall (i,j)\\in K, where K\\subset\\{1,\\ldots,m\\}×\\{1,\\ldots,n\\} is the set of indices corresponding to known pixel values. Our job is to in-paint the image by guessing the missing pixel values, i.e., those with indices not in K. The reconstructed image will be represented by U \\in \\mathbb{R}^{m \\times n}, where U matches the known pixels, i.e. U_{ij}=U^{corr}_{ij} for (i,j)\\in K.\nThe reconstruction U is found by minimizing the total variation of U, subject to matching the known pixel values. We will use the l_{2} total variation, defined as\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU_{i+1,j}-U_{ij}\\\\ U_{i,j+1}-U_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nSo, the final optimization problem will be written as follows:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n}} \\\\\n\\text{s.t. } & U_{ij} = U^{corr}_{ij}, \\; (i,j)\\in K\n\\end{split}\n\nThe crucial thing about this problem is defining set of known pixels K. There are some heuristics: for example, we could state, that each pixel with color similar (or exactly equal) to the color of text is unknown. The results for such approach are presented below:"
  },
  {
    "objectID": "docs/applications/total_variation_inpainting.html#color-image",
    "href": "docs/applications/total_variation_inpainting.html#color-image",
    "title": "1 Problem",
    "section": "",
    "text": "For the color case we consider in-painting problem in a slightly different setting: destroying some random part of all pixels. In this case the image itself is 3d tensor (we convert all others color schemes to the RGB). As it was in the grayscale case, we construct the mask K of known pixels for all color channels uniformly, based on the principle of similarity of particular 3d pixel to the vector [0, 0, 0] (black pixel). The results are quite promising - note, that we have no information about the original picture, but assumption, that corrupted pixels are black. For the color picture we just sum all tv’s on the each channel:\n\n\\begin{split}\\mathop{\\bf tv}(U) =\n\\sum_{k = 1}^{3}\\sum_{i=1}^{m-1} \\sum_{j=1}^{n-1}\n\\left\\| \\left[ \\begin{array}{c}\nU^k_{i+1,j}-U^k_{ij}\\\\ U^k_{i,j+1}-U^k_{ij} \\end{array} \\right] \\right\\|_2.\\end{split}\n\nThen, we need to write down optimization problem to be solved:\n\n\\begin{split}\n& \\mathop{\\bf tv}(U) \\to \\min\\limits_{U \\in \\mathbb{R}^{m \\times n \\times 3}} \\\\\n\\text{s.t. } & U^k_{ij} = U^{corr, k}_{ij}, \\; (i,j)\\in K, \\; k = 1,2,3\n\\end{split}\n\nResults are presented below (these computations really take time):  \nIt is not that easy, right?  \nOnly 5% of all pixels are left:\n \nWhat about 1% of all pixels?"
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html",
    "href": "docs/applications/two_way_partitioning.html",
    "title": "1 Intuition",
    "section": "",
    "text": "Suppose, we have a set of n objects, which are needed to be split into two groups. Moreover, we have information about the preferences of all possible pairs of objects to be in the same group. This information could be presented in the matrix form: W \\in \\mathbb{R}^{n \\times n}, where  \\{w_{ij}\\} is the cost of having i-th and j-th object in the same partitions. It is easy to see, that the total number of partitions is finite and equals to 2^n. So this problem can in principle be solved by simply checking the objective value of each feasible point. Since the number of feasible points grows exponentially, however, this is possible only for small problems (say, with n \\leq 30). In general (and for n larger than, say, 50) the problem is very difficult to solve.\nFor example, bruteforce solution on MacBook Air with M1 processor without any explicit parallelization will take more, than a universe lifetime for n=62.\n\nDespite the hardness of the problems, there are several ways to approach it."
  },
  {
    "objectID": "docs/applications/two_way_partitioning.html#simple-lower-bound-with-duality",
    "href": "docs/applications/two_way_partitioning.html#simple-lower-bound-with-duality",
    "title": "1 Intuition",
    "section": "2.1 Simple lower bound with duality",
    "text": "2.1 Simple lower bound with duality\nWe now derive the dual function for this problem. The Lagrangian is\n\nL(x, \\nu) = x^\\top W x + \\sum\\limits_{i=1}^n \\nu_i (x^2_i − 1) = x^\\top (W + \\text{diag}(\\nu))x − \\mathbf{1}^\\top \\nu.\n\nWe obtain the Lagrange dual function by minimizing over x:\n\n\\begin{split}\ng(\\nu) &= \\inf_{x \\in\\mathbb{R}^n} x^\\top (W + diag(\\nu))x − \\mathbf{1}^\\top \\nu = \\\\\n&= \\begin{cases}\n\\mathbf{1}^\\top \\nu,  &W + \\text{diag}(\\nu) \\succeq 0 \\\\\n-\\infty, &\\text{ otherwise} \\end{cases}\n\\end{split}\n\nThis dual function provides lower bounds on the optimal value of the difficult problem. For example, we can take any specific value of the dual variable\n\n\\nu = −\\lambda_{min}(W)\\mathbf{1},\n\nThis yields the bound on the optimal value p^*:\n\np^* \\geq g(\\nu) \\geq −\\mathbf{1}^\\top \\nu = n \\lambda_{min}(W)\n\nQuestion Can you obtain the same lower bound without knowledge of duality, but using the idea of eigenvalues?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods.\n\n\n\nDive into the wealth of resources available:\n\nTheoryMethodsExercisesApplicationsBenchmarks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\nConvex sets\n\n\n\n\nProjection\n\n\n\n\nExercises\n\n\n\n\nSeparation\n\n\n\n\nConjugate sets\n\n\n\n\nConvex functions\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\nConjugate functions\n\n\n\n\nGeneral optimization problems\n\n\n\n\nDuality\n\n\n\n\nRates of convergence\n\n\n\n\nLine search\n\n\n\n\nCVXPY library\n\n\n\n\nAutomatic differentiation\n\n\n\n\nZero order methods\n\n\n\n\nFirst order methods\n\n\n\n\nUncategorized\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\nOpen Education for All: At 💎fmin.xyz, we steadfastly believe in dismantling barriers to education. All content on this site is open-sourced and freely accessible to everyone, echoing our commitment to inclusive learning.\nDesigned for Learners at Various Levels: Whether you are a researcher, a student, or an enthusiast with some prior expertise, 💎fmin.xyz is tailored to cater to your needs, providing a vast array of content ranging from fundamental theories to advanced applications.\nStructured Learning: Every page on 💎fmin.xyz is meticulously structured as a self-sufficient unit, providing concise and comprehensive material on a specific topic. This modular design allows for flexible and personalized learning paths.\nContinuous Improvement: The structure and content of 💎fmin.xyz are dynamic. We are devoted to refining and expanding our resources iteratively, adapting to the evolving needs of our learning community and the advancements in the field.\n\n\n\n\n\nRich Content: Delve into a wide spectrum of topics from matrix calculus, convex sets, to various optimization methods and their applications. With a plethora of exercises and benchmarks, 💎fmin.xyz is a treasure trove of knowledge waiting to be explored.\nInteractive Visualizations: Engage with our extensive collection of Python visualizations designed to elucidate key mathematical concepts, enhancing your understanding and learning experience.\nEfficient Search Functionality: With a user-friendly built-in search engine, finding specific content on 💎fmin.xyz is just a click away. Navigate through our extensive repository with ease and efficiency.\n\n\n\n\n\nEasy to Use & Share: Each page is formatted as a Markdown file or Jupyter notebook, facilitating easy copying, sharing, and utilization for your purposes.\nBuilt with Quarto: 💎fmin.xyz is powered by the Quarto system, ensuring a smooth and responsive user experience.\nContribute & Collaborate: Your insights are valuable! Contributions are not just welcomed but encouraged. Check out our guides at the repository and join us in refining and expanding this educational endeavor.\n\nEmbark on a learning adventure with 💎fmin.xyz, where knowledge is open, and education is accessible. Happy learning!"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Welcome to 💎fmin.xyz, a comprehensive source designed for enthusiasts, researchers, students, and advanced school practitioners in the fields of optimization and mathematical theory! This platform is diligently crafted to serve as a reliable companion in your journey of acquiring and refining knowledge in optimization theory and methods."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Dive into the wealth of resources available:\n\nTheoryMethodsExercisesApplicationsBenchmarks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMatrix calculus\n\n\n\n\nConvex sets\n\n\n\n\nProjection\n\n\n\n\nExercises\n\n\n\n\nSeparation\n\n\n\n\nConjugate sets\n\n\n\n\nConvex functions\n\n\n\n\nSubgradient and subdifferential\n\n\n\n\nConjugate functions\n\n\n\n\nGeneral optimization problems\n\n\n\n\nDuality\n\n\n\n\nRates of convergence\n\n\n\n\nLine search\n\n\n\n\nCVXPY library\n\n\n\n\nAutomatic differentiation\n\n\n\n\nZero order methods\n\n\n\n\nFirst order methods\n\n\n\n\nUncategorized\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#our-principles",
    "href": "index.html#our-principles",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Open Education for All: At 💎fmin.xyz, we steadfastly believe in dismantling barriers to education. All content on this site is open-sourced and freely accessible to everyone, echoing our commitment to inclusive learning.\nDesigned for Learners at Various Levels: Whether you are a researcher, a student, or an enthusiast with some prior expertise, 💎fmin.xyz is tailored to cater to your needs, providing a vast array of content ranging from fundamental theories to advanced applications.\nStructured Learning: Every page on 💎fmin.xyz is meticulously structured as a self-sufficient unit, providing concise and comprehensive material on a specific topic. This modular design allows for flexible and personalized learning paths.\nContinuous Improvement: The structure and content of 💎fmin.xyz are dynamic. We are devoted to refining and expanding our resources iteratively, adapting to the evolving needs of our learning community and the advancements in the field."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Rich Content: Delve into a wide spectrum of topics from matrix calculus, convex sets, to various optimization methods and their applications. With a plethora of exercises and benchmarks, 💎fmin.xyz is a treasure trove of knowledge waiting to be explored.\nInteractive Visualizations: Engage with our extensive collection of Python visualizations designed to elucidate key mathematical concepts, enhancing your understanding and learning experience.\nEfficient Search Functionality: With a user-friendly built-in search engine, finding specific content on 💎fmin.xyz is just a click away. Navigate through our extensive repository with ease and efficiency."
  },
  {
    "objectID": "index.html#technical-insights",
    "href": "index.html#technical-insights",
    "title": "Welcome to 💎fmin.xyz!",
    "section": "",
    "text": "Easy to Use & Share: Each page is formatted as a Markdown file or Jupyter notebook, facilitating easy copying, sharing, and utilization for your purposes.\nBuilt with Quarto: 💎fmin.xyz is powered by the Quarto system, ensuring a smooth and responsive user experience.\nContribute & Collaborate: Your insights are valuable! Contributions are not just welcomed but encouraged. Check out our guides at the repository and join us in refining and expanding this educational endeavor.\n\nEmbark on a learning adventure with 💎fmin.xyz, where knowledge is open, and education is accessible. Happy learning!"
  },
  {
    "objectID": "assets/Notebooks/taylor_inaccuracy.html",
    "href": "assets/Notebooks/taylor_inaccuracy.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport jax.numpy as jnp\nfrom jax import grad, jit\n\n# Define the function\ndef f(x):\n    return (1 + x**2)**(1/2) - 1\n\n# Get the gradient and hessian of the function\ndf = jit(grad(f))\nd2f = jit(grad(df))\n\n# Define the Taylor approximation at a point x\ndef taylor_approximation(x, x0):\n    return f(x0) + df(x0) * (x - x0) + 0.5 * d2f(x0) * (x - x0)**2\n\n# Define the minimum of the Taylor approximation\ndef taylor_min(x0):\n    return x0 - df(x0) / d2f(x0)\n\n# Set up the animation\nfig, ax = plt.subplots()\nx = jnp.linspace(-4, 4, 400)\n\ndef animate(i):\n    ax.clear()\n    # Adjusting xi to oscillate between -4 and 4\n    xi = -4 + (8 / 800) * (i % 800) if i &lt; 800 else 4 - (8 / 800) * (i % 800)\n    ax.plot(x, f(x), label=\"Function $\\sqrt{1 + x^2} - 1$\")\n    ax.plot(x, taylor_approximation(x, xi), label=f\"Taylor Approximation at $x_0$\", linestyle='dashed')\n    ax.scatter([xi], [f(xi)], color='blue', label=f\"$x_0$={xi:.1f}\")\n    ax.scatter([taylor_min(xi)], [taylor_approximation(taylor_min(xi), xi)], color='orange')\n    ax.legend(loc=\"upper center\")\n    ax.set_title('Quadratic approximation becomes inaccurate')\n    ax.set_ylim([-1, 3])\n    ax.set_xlim([-4, 4])\n    ax.grid(linestyle=\":\")\n\n# Adjusted frames to have a complete oscillation\nani = animation.FuncAnimation(fig, animate, frames=range(1600), repeat=True)\n\n# Save the animation\nani.save('inaccurate_taylor.mp4', writer='ffmpeg', fps=60, dpi=200)\n\nplt.show()"
  }
]